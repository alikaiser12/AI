 4/1:
egg_count = 0

def buy_eggs():
    egg_count += 12 # purchase a dozen eggs

buy_eggs()
 4/2:
str1 = 'Functions are important programming concepts.'

def print_fn():
    str1 = 'Variable scope is an important concept.'
    print(str1)

print_fn()
 4/3:
def print_fn():
    str1 = 'Variable scope is an important concept.'
    print(str1)

print_fn(str1)
 4/4:
str1 = 'Functions are important programming concepts.'

def print_fn():
    print(str1)

print_fn(str1)
 5/1:
numbers = [
              [34, 63, 88, 71, 29],
              [90, 78, 51, 27, 45],
              [63, 37, 85, 46, 22],
              [51, 22, 34, 11, 18]
           ]

def mean(num_list):
    return sum(num_list) / len(num_list)

averages = list(map(mean, numbers))
print(averages)
 5/2:
numbers = [
              [34, 63, 88, 71, 29],
              [90, 78, 51, 27, 45],
              [63, 37, 85, 46, 22],
              [51, 22, 34, 11, 18]
           ]

def mean(num_list):
    return sum(num_list) / len(num_list)

averages = list(map(mean, numbers))
print(averages)
 5/3:
cities = ["New York City", "Los Angeles", "Chicago", "Mountain View", "Denver", "Boston"]


def is_short(name):
    return len(name) < 10

short_cities = list(filter(is_short, cities))
print(short_cities)
 5/4:
numbers = [
              [34, 63, 88, 71, 29],
              [90, 78, 51, 27, 45],
              [63, 37, 85, 46, 22],
              [51, 22, 34, 11, 18]
           ]



averages = list(map(lambda x: sum(x) / len(x), numbers))
print(averages)
 6/1:
#!/usr/bin/python

""" Complete the code in ClassifyNB.py with the sklearn
    Naive Bayes classifier to classify the terrain data.
    
    The objective of this exercise is to recreate the decision 
    boundary found in the lesson video, and make a plot that
    visually shows the decision boundary """


from prep_terrain_data import makeTerrainData
from class_vis import prettyPicture, output_image
from ClassifyNB import classify

import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()

### the training data (features_train, labels_train) have both "fast" and "slow" points mixed
### in together--separate them so we can give them different colors in the scatterplot,
### and visually identify them
grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]
bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]
grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]
bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]


# You will need to complete this function imported from the ClassifyNB script.
# Be sure to change to that code tab to complete this quiz.
clf = classify(features_train, labels_train)



### draw the decision boundary with the text points overlaid
prettyPicture(clf, features_test, labels_test)
output_image("test.png", "png", open("test.png", "rb").read())
 6/2:
#!/usr/bin/python
import random


def makeTerrainData(n_points=1000):
###############################################################################
### make the toy dataset
    random.seed(42)
    grade = [random.random() for ii in range(0,n_points)]
    bumpy = [random.random() for ii in range(0,n_points)]
    error = [random.random() for ii in range(0,n_points)]
    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]
    for ii in range(0, len(y)):
        if grade[ii]>0.8 or bumpy[ii]>0.8:
            y[ii] = 1.0

### split into train/test sets
    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]
    split = int(0.75*n_points)
    X_train = X[0:split]
    X_test  = X[split:]
    y_train = y[0:split]
    y_test  = y[split:]

    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]
    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]
    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]
    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]

#    training_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
#            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}


    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    test_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}

    return X_train, y_train, X_test, y_test
#    return training_data, test_data
 6/3:
#!/usr/bin/python

#from udacityplots import *
import warnings
warnings.filterwarnings("ignore")

import matplotlib 
matplotlib.use('agg')

import matplotlib.pyplot as plt
import pylab as pl
import numpy as np

#import numpy as np
#import matplotlib.pyplot as plt
#plt.ioff()

def prettyPicture(clf, X_test, y_test):
    x_min = 0.0; x_max = 1.0
    y_min = 0.0; y_max = 1.0

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    h = .01  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)

    # Plot also the test points
    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    plt.scatter(grade_sig, bumpy_sig, color = "b", label="fast")
    plt.scatter(grade_bkg, bumpy_bkg, color = "r", label="slow")
    plt.legend()
    plt.xlabel("bumpiness")
    plt.ylabel("grade")

    plt.savefig("test.png")
    
import base64
import json
import subprocess

def output_image(name, format, bytes):
    image_start = "BEGIN_IMAGE_f9825uweof8jw9fj4r8"
    image_end = "END_IMAGE_0238jfw08fjsiufhw8frs"
    data = {}
    data['name'] = name
    data['format'] = format
    data['bytes'] = base64.encodestring(bytes)
    print image_start+json.dumps(data)+image_end
 6/4:
def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
 6/5:
def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> Y = np.array([1, 1, 1, 2, 2, 2])
    >>> from sklearn.naive_bayes import GaussianNB
    >>> clf = GaussianNB()
    >>> clf.fit(X, Y)
    GaussianNB()
    >>> print(clf.predict([[-0.8, -1]]))
    [1]
    >>> clf_pf = GaussianNB()
    >>> clf_pf.partial_fit(X, Y, np.unique(Y))
    GaussianNB()
    >>> print(clf_pf.predict([[-0.8, -1]]))
    [1]
 6/6:
def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    import numpy as np
    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    Y = np.array([1, 1, 1, 2, 2, 2])
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X, Y)
    GaussianNB()
    print(clf.predict([[-0.8, -1]]))
    [1]
    clf_pf = GaussianNB()
    clf_pf.partial_fit(X, Y, np.unique(Y))
    GaussianNB()
    print(clf_pf.predict([[-0.8, -1]]))
    [1]
 6/7:
#!/usr/bin/python

#from udacityplots import *
import warnings
warnings.filterwarnings("ignore")

import matplotlib 
matplotlib.use('agg')

import matplotlib.pyplot as plt
import pylab as pl
import numpy as np

#import numpy as np
#import matplotlib.pyplot as plt
#plt.ioff()

def prettyPicture(clf, X_test, y_test):
    x_min = 0.0; x_max = 1.0
    y_min = 0.0; y_max = 1.0

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    h = .01  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)

    # Plot also the test points
    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    plt.scatter(grade_sig, bumpy_sig, color = "b", label="fast")
    plt.scatter(grade_bkg, bumpy_bkg, color = "r", label="slow")
    plt.legend()
    plt.xlabel("bumpiness")
    plt.ylabel("grade")

    plt.savefig("test.png")
    
import base64
import json
import subprocess

def output_image(name, format, bytes):
    image_start = "BEGIN_IMAGE_f9825uweof8jw9fj4r8"
    image_end = "END_IMAGE_0238jfw08fjsiufhw8frs"
    data = {}
    data['name'] = name
    data['format'] = format
    data['bytes'] = base64.encodestring(bytes)
    print image_start+json.dumps(data)+image_end
 6/8:
#!/usr/bin/python
import random


def makeTerrainData(n_points=1000):
###############################################################################
### make the toy dataset
    random.seed(42)
    grade = [random.random() for ii in range(0,n_points)]
    bumpy = [random.random() for ii in range(0,n_points)]
    error = [random.random() for ii in range(0,n_points)]
    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]
    for ii in range(0, len(y)):
        if grade[ii]>0.8 or bumpy[ii]>0.8:
            y[ii] = 1.0

### split into train/test sets
    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]
    split = int(0.75*n_points)
    X_train = X[0:split]
    X_test  = X[split:]
    y_train = y[0:split]
    y_test  = y[split:]

    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]
    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]
    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]
    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]

#    training_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
#            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}


    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    test_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}

    return X_train, y_train, X_test, y_test
#    return training_data, test_data
 6/9:
def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    import numpy as np
    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    Y = np.array([1, 1, 1, 2, 2, 2])
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X, Y)
    GaussianNB()
    print(clf.predict([[-0.8, -1]]))
    [1]
    clf_pf = GaussianNB()
    clf_pf.partial_fit(X, Y, np.unique(Y))
    GaussianNB()
    print(clf_pf.predict([[-0.8, -1]]))
    [1]
6/10:
#!/usr/bin/python

#from udacityplots import *
import warnings
warnings.filterwarnings("ignore")

import matplotlib 
matplotlib.use('agg')

import matplotlib.pyplot as plt
import pylab as pl
import numpy as np

#import numpy as np
#import matplotlib.pyplot as plt
#plt.ioff()

def prettyPicture(clf, X_test, y_test):
    x_min = 0.0; x_max = 1.0
    y_min = 0.0; y_max = 1.0

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    h = .01  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)

    # Plot also the test points
    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    plt.scatter(grade_sig, bumpy_sig, color = "b", label="fast")
    plt.scatter(grade_bkg, bumpy_bkg, color = "r", label="slow")
    plt.legend()
    plt.xlabel("bumpiness")
    plt.ylabel("grade")

    plt.savefig("test.png")
    
import base64
import json
import subprocess

def output_image(name, format, bytes):
    image_start = "BEGIN_IMAGE_f9825uweof8jw9fj4r8"
    image_end = "END_IMAGE_0238jfw08fjsiufhw8frs"
    data = {}
    data['name'] = name
    data['format'] = format
    data['bytes'] = base64.encodestring(bytes)
    print image_start+json.dumps(data)+image_end
6/11:
#!/usr/bin/python

#from udacityplots import *
import warnings
warnings.filterwarnings("ignore")

import matplotlib 
matplotlib.use('agg')

import matplotlib.pyplot as plt
import pylab as pl
import numpy as np

#import numpy as np
#import matplotlib.pyplot as plt
#plt.ioff()

def prettyPicture(clf, X_test, y_test):
    x_min = 0.0; x_max = 1.0
    y_min = 0.0; y_max = 1.0

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    h = .01  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)

    # Plot also the test points
    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    plt.scatter(grade_sig, bumpy_sig, color = "b", label="fast")
    plt.scatter(grade_bkg, bumpy_bkg, color = "r", label="slow")
    plt.legend()
    plt.xlabel("bumpiness")
    plt.ylabel("grade")

    plt.savefig("test.png")
    
import base64
import json
import subprocess

def output_image(name, format, bytes):
    image_start = "BEGIN_IMAGE_f9825uweof8jw9fj4r8"
    image_end = "END_IMAGE_0238jfw08fjsiufhw8frs"
    data = {}
    data['name'] = name
    data['format'] = format
    data['bytes'] = base64.encodestring(bytes)
    print image_start+json.dumps(data)+image_end
6/12:
#!/usr/bin/python

""" Complete the code in ClassifyNB.py with the sklearn
    Naive Bayes classifier to classify the terrain data.
    
    The objective of this exercise is to recreate the decision 
    boundary found in the lesson video, and make a plot that
    visually shows the decision boundary """


from prep_terrain_data import makeTerrainData
from class_vis import prettyPicture, output_image
from ClassifyNB import classify

import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()

### the training data (features_train, labels_train) have both "fast" and "slow" points mixed
### in together--separate them so we can give them different colors in the scatterplot,
### and visually identify them
grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]
bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]
grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]
bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]


# You will need to complete this function imported from the ClassifyNB script.
# Be sure to change to that code tab to complete this quiz.
clf = classify(features_train, labels_train)

def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X, Y)
    GaussianNB()
    print(clf.predict([[-0.8, -1]]))
    [1]
 



### draw the decision boundary with the text points overlaid
prettyPicture(clf, features_test, labels_test)
output_image("test.png", "png", open("test.png", "rb").read())
6/13:
#!/usr/bin/python
import random


def makeTerrainData(n_points=1000):
###############################################################################
### make the toy dataset
    random.seed(42)
    grade = [random.random() for ii in range(0,n_points)]
    bumpy = [random.random() for ii in range(0,n_points)]
    error = [random.random() for ii in range(0,n_points)]
    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]
    for ii in range(0, len(y)):
        if grade[ii]>0.8 or bumpy[ii]>0.8:
            y[ii] = 1.0

### split into train/test sets
    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]
    split = int(0.75*n_points)
    X_train = X[0:split]
    X_test  = X[split:]
    y_train = y[0:split]
    y_test  = y[split:]

    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]
    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]
    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]
    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]

#    training_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
#            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}


    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    test_data = {"fast":{"grade":grade_sig, "bumpiness":bumpy_sig}
            , "slow":{"grade":grade_bkg, "bumpiness":bumpy_bkg}}

    return X_train, y_train, X_test, y_test
#    return training_data, test_data
6/14:
def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    import numpy as np
    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    Y = np.array([1, 1, 1, 2, 2, 2])
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X, Y)
    GaussianNB()
    print(clf.predict([[-0.8, -1]]))
    [1]
    clf_pf = GaussianNB()
    clf_pf.partial_fit(X, Y, np.unique(Y))
    GaussianNB()
    print(clf_pf.predict([[-0.8, -1]]))
    [1]
6/15:
#!/usr/bin/python

#from udacityplots import *
import warnings
warnings.filterwarnings("ignore")

import matplotlib 
matplotlib.use('agg')

import matplotlib.pyplot as plt
import pylab as pl
import numpy as np

#import numpy as np
#import matplotlib.pyplot as plt
#plt.ioff()

def prettyPicture(clf, X_test, y_test):
    x_min = 0.0; x_max = 1.0
    y_min = 0.0; y_max = 1.0

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    h = .01  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)

    # Plot also the test points
    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]
    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]
    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]
    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]

    plt.scatter(grade_sig, bumpy_sig, color = "b", label="fast")
    plt.scatter(grade_bkg, bumpy_bkg, color = "r", label="slow")
    plt.legend()
    plt.xlabel("bumpiness")
    plt.ylabel("grade")

    plt.savefig("test.png")
    
import base64
import json
import subprocess

def output_image(name, format, bytes):
    image_start = "BEGIN_IMAGE_f9825uweof8jw9fj4r8"
    image_end = "END_IMAGE_0238jfw08fjsiufhw8frs"
    data = {}
    data['name'] = name
    data['format'] = format
    data['bytes'] = base64.encodestring(bytes)
    print image_start+json.dumps(data)+image_end
6/16:
#!/usr/bin/python

""" Complete the code in ClassifyNB.py with the sklearn
    Naive Bayes classifier to classify the terrain data.
    
    The objective of this exercise is to recreate the decision 
    boundary found in the lesson video, and make a plot that
    visually shows the decision boundary """


from prep_terrain_data import makeTerrainData
from class_vis import prettyPicture, output_image
from ClassifyNB import classify

import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()

### the training data (features_train, labels_train) have both "fast" and "slow" points mixed
### in together--separate them so we can give them different colors in the scatterplot,
### and visually identify them
grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]
bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]
grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]
bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]


# You will need to complete this function imported from the ClassifyNB script.
# Be sure to change to that code tab to complete this quiz.
clf = classify(features_train, labels_train)

def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    clf.fit(X, Y)
    GaussianNB()
    print(clf.predict([[-0.8, -1]]))
    [1]
 



### draw the decision boundary with the text points overlaid
prettyPicture(clf, features_test, labels_test)
output_image("test.png", "png", open("test.png", "rb").read())
 7/1:
x = 0
y = 0
def incr(x):
    y = x + 1
    return y
incr(5)
print x, y
 7/2:
x = 0
y = 0
def incr(x):
    y = x + 1
    return y
incr(5)
print (x, y)
 7/3:
numcalls = 0
def square(x):
    global numcalls
    numcalls = numcalls + 1
    return x * x
 7/4: print(numcalls)
 7/5:
print square(5)
print square(2*5)
 7/6:
square(5)
square(2*5)
 7/7:
x = 1
def f():
    return x
print x
print f()
 7/8:
x = 1
def f():
    return x
print (x)
print f()
 7/9:
x = 1
def f():
    return x
print (x)
print (f())
7/10:
print(square(5))
print(square(2*5))
7/11:
x = 1
def f():
        y = x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/12:
x = 1
def f():
        global y = x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/13:
x = 1
def f():
        y == x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/14:
x = 1
def f():
        global y == x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/15:
x = 1
def f():
        global y = x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/16:
x = 1
def f():
        y = x
        x = 2
        return x + y
print (x)
print (f())
print (x)
7/17:
x = 2
def f(a):
    x = a * a
    return x
y = f(3)
print (x, y)
7/18:
>>> def difference(x, y):
        return x - y

>>> difference(5, 2)

>>> difference(x=5, y=2)

>>> difference(5, y=2)

>>> difference(y=2, x=5)
7/19:
>>> def difference(x, y):
        return x - y

>>> difference(5, 2)
        return x - y

>>> difference(x=5, y=2)
        return x - y

>>> difference(5, y=2)
        return x - y

>>> difference(y=2, x=5)
        return x - y
7/20:
>>> def difference(x, y):
        return x - y

>>> difference(5, 2):
        return x - y

>>> difference(x=5, y=2):
        return x - y

>>> difference(5, y=2):
        return x - y

>>> difference(y=2, x=5):
        return x - y
7/21:
def difference(x, y):
        return x 
def difference(5, 2):
        return x - y

def difference(x=5, y=2):
        return x - y

def difference(5, y=2):
        return x - y

def difference(y=2, x=5):
        return x - y
7/22:
def difference(x, y):
        return x
7/23: difference(5, 2)
7/24: difference(x=5, y=2)
7/25: difference(5, y=2)
7/26: difference(y=2, x=5)
7/27:
def difference(x, y):
        return x-y
7/28: difference(5, 2)
7/29: difference(x=5, y=2)
7/30: difference(5, y=2)
7/31: difference(y=2, x=5)
7/32:
def increment(x, amount=1):
    return x + amount
7/33: increment(10)
7/34: increment(10, 5)
7/35: increment(10, amount=2)
7/36: cube = lambda x: x ** 3
7/37: fxy(cube, 2, 3)
7/38: cube(2, 3)
7/39: cube(2)
7/40: cube = lambda (x,y): (x ** 3, y**3)
7/41: cube = lambda x: x ** 3
7/42: cube(2)
7/43: fxy(cube, 2, 3)
7/44: fxy(lambda x: x ** 3, 2, 3)
7/45:
def fxy(cube, 2, 3):
    return cube
7/46:
def fxy(cube, 2):
    return cube
7/47: fxy(cube, 2, 3)
7/48: print (2 < 3 and not 3 > 1)
7/49:
x = 4
y = 5
p = x < y or x < z
print(p)
7/50:
x = 2
if x == 2:
    print(x)
else:
    x +
7/51:
x = 2
if x == 2:
    print(x)
else:
    x +=1
7/52:
x = 2
if x == 2:
    print(x)
else:
    x ++
7/53:
x = 2
if x == 2:
    print(x)
else:
    x +=5
7/54:
x = 5
if x == 2:
    print(x)
else:
    x +=5
7/55:
x = 5
if x == 2:
    print(x)
else:
    x +=5
    print(x)
7/56:
import time  
time.asctime()
7/57: python add.py 3 5
7/58:
for x in [1, 2, 3, 4]:
    print(x)

for i  in range(10):
   print(i, i*i, i*i*i)|
7/59:
for x in [1, 2, 3, 4]:
    print(x)

for i  in range(10):
   print(i, i*i, i*i*i)
7/60:
names = ["a", "b", "c"]
values = [1, 2, 3]
for name, value in zip(names, values):
    print(name, value)
7/61: a = [[2, 3], [4, 6], [6, 1]]
7/62: a.sort(key=lambda x: x[1])
7/63: a
 9/1:
def word_frequency(words):
    """Returns frequency of each word given a list of words.

        >>> word_frequency(['a', 'b', 'a'])
        {'a': 2, 'b': 1}
    """
    frequency = {}
    for w in words:
        frequency[w] = frequency.get(w, 0) + 1
    return frequency
 9/2:
def read_words(filename):
    return open(filename).read().split()
10/1:
def fxy(f, x, y):
    return f(x) + f(y)
cube = lambda x: x**3
fxy(cube, 2, 3)
11/1:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors
    
    authors and labels:
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''

# t0 = time()
# # < your clf.fit() line of code >
# print("Training Time:", round(time()-t0, 3), "s")

# t0 = time()
# # < your clf.predict() line of code >
# print("Predicting Time:", round(time()-t0, 3), "s")

##############################################################
12/1:
balance = 0

def deposit(amount):
    global balance
    balance += amount
    return balance

def withdraw(amount):
    global balance
    balance -= amount
    return balance
12/2:
def make_account():
    return {'balance': 0}

def deposit(account, amount):
    account['balance'] += amount
    return account['balance']

def withdraw(account, amount):
    account['balance'] -= amount
    return account['balance']
12/3:
a = make_account()
b = make_account()
deposit(a, 100)

deposit(b, 50)

withdraw(b, 10)

withdraw(a, 10)
12/4: deposit(b, 50)
12/5: deposit(b, 50)
12/6:

withdraw(b, 10)
12/7: withdraw(a, 10)
13/1: print(2+2+2)
14/1: print(2+2+2)
14/2: print(2+2+2)
14/3: print
14/4: print
14/5: print('Hello, World!')
14/6:
# we can add comments to code cells using a hashtag
# on windows you can comment a line with 'ctrl + /'
# on mac you can comment a line with 'cmd + /'
#
# assign a variable to a literal value
a=3 # assign variable a to literal 3
print('five times a is',5*a,'and 3 multiplied by a is',3*a)
a=2
print('five times a is',5*a,'and 3 multiplied by a is',3*a)
14/7:
# we can also inspect the type of variables
print('a is of type',type(a))
b = 2.1
print('b is of type',type(b))
c = float(a)/b
print('c is of type',type(c), 'and its value is',c)
d = a/b
print('d is of type',type(c), 'and its value is',d)
14/8:
# a list is an ordered and indexed collection of values that are changeable and allows duplicates
simple_list = ['Dan',2,3,4,'python',2.71]
print(simple_list)
print(simple_list[0])
print(simple_list[3])
print(simple_list[-1])
print(simple_list[-2])
print(len(simple_list))
14/9:
# it is easy to change entries of a list
simple_list[3] = 52
print(simple_list[3]) # index is based on 0!!!
simple_list.append('to the back')
print(simple_list)
print(len(simple_list))
simple_list.pop(4)
print(simple_list)
14/10:
# it is easy to initialize a list of a particular size with all entries the same
repeated_list = [5]*4
print(repeated_list)
14/11:
# we can even use lists as entries of a list
simple_list[1] = [1,2,3]
print(simple_list)
print(simple_list[1])
print(simple_list[1][1])
14/12:
# copying lists is a little tricky
list2 = simple_list
print(list2)
print(simple_list)
list2[1] = 0
print(list2)
print(simple_list)
14/13:
# we must use the copy method to prevent this behavior
list3 = simple_list.copy()
print(list3)
print(simple_list)
list3[0] = 'Mitchell'
print(list3)
print(simple_list)
14/14:
# a tuple is an ordered collection of values that are unchangeable and allows duplicates
simple_tuple = (12,42,11,99,2351)
print(simple_tuple)
print(simple_tuple[1])
14/15:
# it is not possible to change entries in a touple
simple_tuple[0] = 5
14/16:
# you can work around this though...
dummy = list(simple_tuple)
dummy[0] = 5
simple_tuple = tuple(dummy)
print(simple_tuple)
14/17:
# a set is an unordered collection of values that are changeable and does not allow duplicates
simple_set = {11,-2,'water',-2}
print(simple_set)
14/18: print(simple_set[1])
14/19: print('water' in simple_set)
14/20:
# you can't change values but you can add and remove entries from a set
simple_set.add(72)
print(simple_set)
simple_set.remove('water')
print(simple_set)
14/21:
# a dictionary is a collection of values that are unordered (but indexed) and changeable
simple_dict = {
    "brand": "Apple",
    "product": "iPhone",
    "model": "X"
}
print(simple_dict)
print('I bought an',simple_dict['product'],"model",simple_dict['model'],'from',simple_dict['brand'])
14/22:
simple_dict["model"] = "11 pro"
print('I bought an',simple_dict['product'],"model",simple_dict['model'],'from',simple_dict['brand'])
14/23:
# we can also add entries to the dictionary
simple_dict['color'] = 'red'
print('I bought a',simple_dict["color"],simple_dict['product'],"model",simple_dict['model'],'from',simple_dict['brand'])
14/24:
simple_list = [1,5,2,7,3,66,1923,11]
# we can access multiple values in the list using the :
# we must be careful though
print(simple_list[2:5])
# why isn't 66 in the output?
# the number to the left of the : is inclusive, but the number to the right is exclusive
print(simple_list[2:3])
# we can also use negative numbers
print(simple_list[-5:-1])
17/1:
# define a function that takes an input, squares it, adds 7, then returns the answer
def x2p7(x):
    """    this is my documentation string
    it needs to be in triple double quotes
    this function returns x^2 + 7"""
    y = x*x
    z = y+7
    return z

print('x^2 + 7 = ',x2p7(5))
print(x2p7.__doc__)
17/2:
# if we have a documentation string then we can quickly access it with shift+tab
x2p7
20/1:
# the name of the data type (class) must always start with a capital letter

class My_data_type:  # first letter capitalized
    def init_some_vals(self,val2):  # self is first argument of function
        self.first_var = 1.7        # attributes named self.attribute_name
        self.second_var = val2      # this function doesn't return anything...it just sets some attribute values
    def multiply_vals(self):        # another internal method, but this one does return a value
        return self.first_var*self.second_var
    
me = My_data_type()    # declare an object of the class My_data_type
me.init_some_vals(2.2) # don't give self as an input!!! 
print(me.first_var)    # access attribute
print(me.multiply_vals()) # run a method
20/2: 1.7*2.2
20/3: print(self.first_var)  # self is only used inside the class definition!!!
20/4:
# we can also add attributes to an object outside of the class definition

me.third_var = 231.3
print(me.third_var)

# this is generally a bad practice though
# we want each object of the same class to have the same attributes (not values of attributes)
# so that this is internally consistent
# in many languages it isn't possible to add attributes from outside the class definition for this very reason
20/5:
you = My_data_type()
you.init_some_vals(6.1)
print(you.third_var)
20/6:
# if we don't want to have to call the initialization function with can use an initalizer with __init__
# everything inside __init__ will automatically be run when an object is declared
class Pet:
    def __init__(self,animal_type,name,age,weight_lbs,color):  # initializer
        self.animal = animal_type    # assign some attribute values from the input arguments of the initializer
        self.name = name
        self.age = age
        self.weight_lbs = weight_lbs
        self.color = color
        self.weight_kg = self.calc_weight_in_kg()  # automatically run a method...still don't feed self to the call
    
    def calc_weight_in_kg(self):  # remember to give self as an input...even though we don't use it when we call
        return 0.453592*self.weight_lbs
        
    def describe_pet(self):
        print('This pet is a',self.color, self.animal,)
        print('This pet\'s name is',self.name,'and it is',self.age,'years old')
        print('This pet weighs',self.weight_lbs,'pounds, which is',round(self.weight_kg,2),'kilograms')
20/7:
# now create 2 objects of the class Pet
my_cat = Pet('cat','Mittens',3,7,'orange')
mydog = Pet('dog','Spot',9,18,'brown')
my_cat.describe_pet()
print('')  # just so there's some space between the object outputs
mydog.describe_pet()
20/8:
# we can also access the attributes of the class from outside because they were declared using self!
print(my_cat.color)
print(mydog.age)
20/9:
# I can even change the value of attributes from outside
my_cat.weight_lbs = 5
print(my_cat.weight_lbs)
print(my_cat.weight_kg)
20/10:
# we can also access methods from outside
print(my_cat.calc_weight_in_kg())
print(mydog.calc_weight_in_kg())
print(my_cat.weight_kg)
my_cat.weight_kg=my_cat.calc_weight_in_kg()
print(my_cat.weight_kg)
20/11: # notice that these 2 method calls returned different values because they are parts of different objects!
21/1:
a= 1
 
for i in range(1,6): 
     a = a* i
     print(a)
21/2:
i='20'
if i>20:
    print(1)
else: 
    print(2)
21/3:
x = 5, y = 7, z=9

result = (x**y) + (y/z) + (x-y) +(z-y) - (y*x)

print(result)
21/4:
x = 5
y = 7
z=9

result = (x**y) + (y/z) + (x-y) +(z-y) - (y*x)

print(result)
21/5: print('\')
22/1: import os
22/2: import os
22/3:
print(os.name)

my_os = os.name
22/4: os.getcwd()
22/5: print(os.getcwd())
22/6: os.listdir('.')
22/7: os.listdir('C:\\Users\\Dan\\Google Drive\\python course')
24/1:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors
    
    authors and labels:
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''

# t0 = time()
# # < your clf.fit() line of code >
# print("Training Time:", round(time()-t0, 3), "s")

# t0 = time()
# # < your clf.predict() line of code >
# print("Predicting Time:", round(time()-t0, 3), "s")

##############################################################
24/2:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(features_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
24/3:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(labels_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
24/4:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    return clf.fit(features_train, labels_train)

### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
24/5:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
return clf.fit(features_train, labels_train)

### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
24/6:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
    return clf.fit(features_train, labels_train)

### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
24/7:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess

def classify(features_train, labels_train): 
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    return clf.fit(features_train, labels_train)

### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
27/1: print(suspect, age, end= "")
27/2:
suspext = "animash"
age = 27
print(suspect, age, end= "")
27/3:
suspect = "animash"
age = 27
print(suspect, age, end= "")
27/4:
suspect = "animash"
age = 27
print(suspect, age)
27/5:
suspect = "animash"
age = 27
email = "rajat@gmail.com"
print(suspect, age)
27/6:
suspect = "animash"
age = 27
email = "rajat@gmail.com"
print(suspect, age , email)
27/7:
suspect = "animash"
age = 27
email = "rajat@gmail.com"
print(suspect, age , email, end="")
27/8:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"
print(suspect, age , email, end="")
27/9:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"
print(suspect, age , email)
27/10:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"
print(age)
print(suspect, age , email)
27/11:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"
print(age)
print(suspect, age , email, end="")
27/12:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"

print(suspect, age , email, end="")
print(age)
27/13:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"

print(suspect, age , email,sep="  @@@@@@@@@@@@@@@@  ", end="")
print(age)
27/14: name = input("Enter your age ")
27/15: print(name)
27/16: type(name)
27/17:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
print("x is negative ")
27/18:
x = 6

if x>0:
    print("x is positive")
    print("condition was checked")
print("x is negative ")
27/19:
x = -5

if x>0:
    print("x is positive")
    print("condition was checked")
print("x is negative ")
27/20:
x = 5

if x>0:
    print("x is positive")
    print("condition was checked")
print("x is negative ")
27/21:
x = 5

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:|
print("x is negative ")
27/22:
x = 5

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:|
    print("x is negative")
print("x will execute ")
27/23:
x = 5

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
print("x will execute ")
27/24:
x = -5

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
print("x will execute ")
27/25:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
print("x will execute ")
27/26:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
print("x will execute ")
27/27:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
    
else:
    print("x is neither")
print("x will execute ")
27/28:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
    
else:
    print("x is neither negative nor positive")
print("x will execute ")
27/29: climate = input("Enter climate:......")
27/30: print(climate)
27/31:
order_id_list = ["WOWRD001","WOWRD002","WOWRD003","WOWRD004"]\
print("Order ID's:", order_id_list)
27/32:
order_id_list = ["WOWRD001","WOWRD002","WOWRD003","WOWRD004"]
print("Order ID's:", order_id_list)
27/33:
Category_list = ["Phone","Wood","Hills","Pens"]
Sub_Category_list = ["Mobile","Segun","Mountain","Red"]
Quantity = [7,9,8,5]
Unit_Prices_list = [18.2,7.5,15.5,25.5]
27/34:
print("Category List:", Category_list)
print("Sub-Category List:", Sub_Category_list)
print("Quantity:", Quantity)
print("Unit Price List:", Unit_Prices_list)
27/35: import pandas as pd
27/36: superstore_data = pd.DataFrame()
27/37: type(superstore_data)
27/38: superstore_data['OrderID']= order_id_list
27/39: superstore_data['OrderID']= order_id_list
27/40: superstore_data
27/41:
superstore_data['Category List']= Category_list
superstore_data['Sub-Category List']= Sub_Category_list 
superstore_data['Quantity:']= Quantity
superstore_data['Unit Price List']= Unit_Prices_list
27/42: superstore_data
27/43: superstore_data['Total Amount']= superstore_data['Quantity:']*superstore_data['Unit Price List']
27/44: superstore_data
27/45: superstore_data.dtypes
27/46:
col_list = ["Category List","Sub-Category List", "Unit Price List"]
superstore_data[col_list]
27/47:
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
27/48:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors
    
    authors and labels:
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''

# t0 = time()
# # < your clf.fit() line of code >
# print("Training Time:", round(time()-t0, 3), "s")

# t0 = time()
# # < your clf.predict() line of code >
# print("Predicting Time:", round(time()-t0, 3), "s")

##############################################################
27/49:
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
27/50:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors
    
    authors and labels:
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''

# t0 = time()
# # < your clf.fit() line of code >
# print("Training Time:", round(time()-t0, 3), "s")

# t0 = time()
# # < your clf.predict() line of code >
# print("Predicting Time:", round(time()-t0, 3), "s")

##############################################################
27/51:
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
27/52:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"

print(suspect, age , email,sep="  @@@@@@@@@@@@@@@@  ", end="")
print(age)
27/53: name = input("Enter your age ")
27/54: print(name)
27/55: type(name)
27/56:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
    
else:
    print("x is neither negative nor positive")
print("x will execute ")
27/57: climate = input("Enter climate:......")
27/58: print(climate)
27/59:
order_id_list = ["WOWRD001","WOWRD002","WOWRD003","WOWRD004"]
print("Order ID's:", order_id_list)
27/60:
Category_list = ["Phone","Wood","Hills","Pens"]
Sub_Category_list = ["Mobile","Segun","Mountain","Red"]
Quantity = [7,9,8,5]
Unit_Prices_list = [18.2,7.5,15.5,25.5]
27/61:
print("Category List:", Category_list)
print("Sub-Category List:", Sub_Category_list)
print("Quantity:", Quantity)
print("Unit Price List:", Unit_Prices_list)
27/62: import pandas as pd
27/63: superstore_data = pd.DataFrame()
27/64: type(superstore_data)
27/65: superstore_data['OrderID']= order_id_list
27/66: superstore_data
27/67:
superstore_data['Category List']= Category_list
superstore_data['Sub-Category List']= Sub_Category_list 
superstore_data['Quantity:']= Quantity
superstore_data['Unit Price List']= Unit_Prices_list
27/68: superstore_data
27/69: superstore_data['Total Amount']= superstore_data['Quantity:']*superstore_data['Unit Price List']
27/70: superstore_data
27/71: superstore_data.dtypes
27/72: pd.read_csv("/home/alikaiser12/Desktop/texas")
27/73:
import pandas as pd 
pd.read_csv("/home/alikaiser12/Desktop/texas")
27/74:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. 

    Use a Naive Bayes Classifier to identify emails by their authors
    
    authors and labels:
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''

# t0 = time()
# # < your clf.fit() line of code >
# print("Training Time:", round(time()-t0, 3), "s")

# t0 = time()
# # < your clf.predict() line of code >
# print("Predicting Time:", round(time()-t0, 3), "s")

##############################################################
27/75:
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()


##############################################################
# Enter Your Code Here

t0 = time()
clf.predict(labels_test)
print("Training Time:", round(time()-t0, 3), "s")

t0 = time()
clf.predict(features_test)
print("Predicting Time:", round(time()-t0, 3), "s")



##############################################################

##############################################################
'''
You Will be Required to record time for Training and Predicting 
The Code Given on Udacity Website is in Python-2
The Following Code is Python-3 version of the same code
'''



##############################################################
27/76:
suspect = "animash sharma akhilesh"
age = 27
email = "rajat@gmail.com"

print(suspect, age , email,sep="  @@@@@@@@@@@@@@@@  ", end="")
print(age)
27/77: name = input("Enter your age ")
27/78: print(name)
27/79: type(name)
27/80:
x = 0

if x>0:
    print("x is positive")
    print("condition was checked")
    
elif x<0:
    print("x is negative")
    
else:
    print("x is neither negative nor positive")
print("x will execute ")
27/81: climate = input("Enter climate:......")
27/82: print(climate)
27/83:
order_id_list = ["WOWRD001","WOWRD002","WOWRD003","WOWRD004"]
print("Order ID's:", order_id_list)
27/84:
Category_list = ["Phone","Wood","Hills","Pens"]
Sub_Category_list = ["Mobile","Segun","Mountain","Red"]
Quantity = [7,9,8,5]
Unit_Prices_list = [18.2,7.5,15.5,25.5]
27/85:
print("Category List:", Category_list)
print("Sub-Category List:", Sub_Category_list)
print("Quantity:", Quantity)
print("Unit Price List:", Unit_Prices_list)
27/86: import pandas as pd
27/87: superstore_data = pd.DataFrame()
27/88: type(superstore_data)
27/89: superstore_data['OrderID']= order_id_list
27/90: superstore_data
27/91:
superstore_data['Category List']= Category_list
superstore_data['Sub-Category List']= Sub_Category_list 
superstore_data['Quantity:']= Quantity
superstore_data['Unit Price List']= Unit_Prices_list
27/92: superstore_data
27/93: superstore_data['Total Amount']= superstore_data['Quantity:']*superstore_data['Unit Price List']
27/94: superstore_data
27/95: superstore_data.dtypes
27/96:
col_list = ["Category List","Sub-Category List", "Unit Price List"]
superstore_data[col_list]
27/97:
import pandas as pd 
pd.read_csv("/home/alikaiser12/Desktop/texas")
27/98:
import pandas as pd 
sys.path.append("/home/alikaiser12/Desktop/texas")
pd.read_csv("wowSuperstore.csv")
27/99:
import pandas as pd 
sys.path.append("/home/alikaiser12/Desktop/texas")
pd.read_csv("WOW Superstore.csv")
27/100:
import pandas as pd 
sys.path.append("/home/alikaiser12/Desktop/texas")
pd.read_csv('WOW Superstore.csv')
29/1: import pandas as pd
29/2: pd.read_csv("WOW Superstore.csv")
29/3: data = pd.read_csv("WOW Superstore.csv")
29/4: data.head()
29/5: pwd
29/6: import pandas as pd
29/7: data = pd.read_csv("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes/WOW Superstore.csv")
29/8: data.head()
29/9: data.sample(10)
29/10: data.sample()
29/11: data.sample(n=7)
29/12: data.sample(n=7)
29/13: data.sample(n=7)
29/14: data.sample(n=7)
29/15: data.sample(n=7)
29/16: data.sample(n=7)
29/17: data.sample(n=7)
29/18: data.sample(n=7)
29/19: data.sample(n=7)
29/20: data.sample(n=7)
29/21: data.sample(n=7)
29/22: data.sample(n=7)
29/23: data.sample(n=7)
29/24: data.sample(n=7)
29/25: data.sample(n=7)
29/26: data.sample(n=7, random_state=5)
29/27: data.sample(n=7, random_state=5)
29/28: data.sample(n=7, random_state=5)
30/1: import pandas as pd
30/2: data = pd.read_csv("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/naive_bayes/WOW Superstore.csv")
30/3: data = pd.read_csv("WOW Superstore.csv")
30/4: data.head()
30/5: data.sample(n=7, random_state=5)
30/6: data.shape
30/7: data.size
30/8: data.info()
31/1: data.describe()
31/2: import pandas as pd
31/3: data = pd.read_csv("WOW Superstore.csv")
31/4: data.head()
31/5: data.sample(n=7, random_state=5)
31/6: data.shape
31/7: data.size
31/8: data.info()
31/9: data.describe()
31/10: data.describe().T
31/11: data.describe().T.round(2)
31/12: data['Profit'].mean()
31/13: round(data['Profit'].mean())
31/14: round(data['Profit'].mean(),1)
31/15: data['Profit'].describe().round(2)
31/16: data['Profit'].describe()
31/17: data['Profit'].describe().round(2)
31/18: data.describe(include="object")
31/19: data.describe(include="all")
31/20: data['Category'].unique()
31/21: data['Category'].nunique()
31/22: data['Category'].unique()
31/23: data['Category'].value_counts()
31/24: data['Category'].value_counts(normalize=True)
31/25: data['Category'].value_counts(normalize=True)*100
31/26: data.groupby("Category")["Order ID"].count()
31/27: data.groupby("Category")["Order ID"].count()/data.shape[0]
31/28: data.groupby("Category")["Profit"].mean()
31/29: data.groupby("Category")["Profit"].mean().sort_values(ascending = False)
31/30: data.groupby("State")["Sales"].mean().sort_values(ascending = False)
31/31: data.groupby("State")["Sales"].sum().sort_values(ascending = False)
31/32:
data_cross = pb.crosstab(data["State"],data["Category"])
data_cross
31/33:
data_cross = pd.crosstab(data["State"],data["Category"])
data_cross
31/34: import seaborn as sns
31/35: sns.heatmap(data_cross, annot="True", fmt= "0.f", cmap="PuBu")
31/36: sns.heatmap(data_cross, annot=True, fmt= "0.f", cmap="PuBu")
31/37: sns.heatmap(data_cross, annot=True, fmt= ".0f", cmap="PuBu")
31/38: sns.heatmap(data_cross, annot=True, fmt= ".1f", cmap="PuBu")
31/39:
numerical_columns = ["Sales","Quantity","Discount","Profit"]
sns.pairplot(data=data, vars=numerical_columns, diag_kind= "kde", hue="Ship Mode")
31/40:
numerical_columns = ["Sales","Quantity","Discount","Profit"]
sns.pairplot(data=data, vars=numerical_columns, diag_kind= "hist", hue="Category")
31/41: ?np.linespace
31/42: ?np.linspace
31/43:
import numpy as np
?np.linspace
31/44: np.linspace(10,20,3)
31/45:
import pandas as pd
import numpy as np
df = pd.DataFrame([[4, 9],[3,6],[7,8]], columns=['A', 'B'])
print(df)
df['A'].apply(np.sqrt)
31/46:
df = pd.DataFrame([[4, 9],[3,6],[7,8]], columns=['A', 'B'])
print(df)
df.apply(np.sqrt)
31/47: df.apply(np.sum, axis = 0)
31/48: df.apply(np.sum, axis=1)
31/49:
def sum_input(s):
    return sum(s)
df.apply(sum_input, axis=0)
32/1:
# the first step of using numpy is to tell python to use it
import numpy as np
32/2:
print(np.cos(np.pi))
print(np.sqrt(1.21))
print(np.log(np.exp(5.2)))
32/3:
# we can create numpy arrays by converting lists
# this is a vector
vec = np.array([1,2,3])
print(vec)
# we can create matrices by converting lists of lists
mat = np.array([[1,2,1],[4,5,9],[1,8,9]])
print('')
print(mat)
print('')
print(mat.T)
32/4:
# there are lots of other ways to create numpy arrays
vec2 = np.arange(0,15)
print(vec2)
print('')
vec3 = np.arange(3,21,6)
print(vec3)
32/5:

vec4 = np.linspace(0,5,10)
print(vec4)
print('')
print(vec4.reshape(5,2))
vec4_reshaped = vec4.reshape(5,2)
print(vec4_reshaped)
print(vec4)
33/1:
# the first step of using numpy is to tell python to use it
import numpy as np
33/2:
print(np.cos(np.pi))
print(np.sqrt(1.21))
print(np.log(np.exp(5.2)))
33/3:
# we can create numpy arrays by converting lists
# this is a vector
vec = np.array([1,2,3])
print(vec)
# we can create matrices by converting lists of lists
mat = np.array([[1,2,1],[4,5,9],[1,8,9]])
print('')
print(mat)
print('')
print(mat.T)
33/4:
# there are lots of other ways to create numpy arrays
vec2 = np.arange(0,15)
print(vec2)
print('')
vec3 = np.arange(3,21,6)
print(vec3)
33/5:

vec4 = np.linspace(0,5,10)
print(vec4)
print('')
print(vec4.reshape(5,2))
vec4_reshaped = vec4.reshape(5,2)
print(vec4_reshaped)
print(vec4)
33/6:
mat2 = np.zeros([5,3])
print(mat2)
mat3 = np.ones((3,5))
print('')
print(mat3)
mat4 = np.eye(5)
print('')
print(mat4)
33/7:
# we can +-*/ arrays together if they're the right size
vec5 = np.arange(1,6)
vec6 = np.arange(3,8)
print(vec5)
print(vec6)
print(vec5+vec6)
print(vec5*vec6)
print(1/vec5)
print(np.sqrt(vec6))
33/8:
# we can do matrix multiplication
print(mat)
print('')
print(vec)
print()
product = np.matmul(mat,vec)
print(product)
33/9:
print(np.linalg.solve(mat,product))
print('')
print(np.linalg.inv(mat))
33/10:
# we can find the unique values in an array
vec7 = np.array(['blue','red','orange','purple','purple','orange','Red',6])
print(vec7)
print(np.unique(vec7))
33/11:
# we can also use numpy to generate samples of a random variable
rand_mat = np.random.rand(5,5) # uniform random variable
print(rand_mat)
rand_mat2 = np.random.randn(10,5) # standard normal random variable
print('')
print(rand_mat2)
33/12:
# we can also use numpy for statistical tools on arrays
print(np.mean(rand_mat))
print(np.std(rand_mat2))
33/13:
print(np.min(rand_mat))
print(np.max(rand_mat2))
33/14: # break here for next video!
33/15:
# how do we access entries in a numpy vector
rand_vec = np.random.randn(19)
print(rand_vec)
print(rand_vec[6])
33/16:
# we can access multiple entries at once using :
print(rand_vec[4:9])
33/17:
# we can also access multiple non-consecutive entries using np.arange
print(np.arange(0,15,3))
print(rand_vec[np.arange(0,15,3)])
33/18:
# what about matrices
print(rand_mat)
print(rand_mat[1][2])
print(rand_mat[1,2])
33/19: print(rand_mat[0:2,1:3])
33/20:
# let's change some values in an array!
print(rand_vec)
rand_vec[3:5] = 4
print('')
print(rand_vec)
rand_vec[3:5] = [1,2]
print('')
print(rand_vec)
33/21:
print(rand_mat)
rand_mat[1:3,3:5] = 0
print('')
print(rand_mat)
33/22:
sub_mat = rand_mat[0:2,0:3]
print(sub_mat)
sub_mat[:] = 3
print(sub_mat)
33/23: print(rand_mat)
33/24:
sub_mat2 = rand_mat[0:2,0:3].copy()
sub_mat2[:] = 99
print(sub_mat2)
print(rand_mat)
33/25: # break here for next video
33/26:
# we can also access entries with logicals
rand_vec = np.random.randn(15)

print(rand_vec)
print(rand_vec>0)
print(rand_vec[rand_vec>0])
33/27:
print(rand_mat2)
print(rand_mat2[rand_mat2>0])
33/28:

print(rand_vec)
print('')
rand_vec[rand_vec>0.5] = -5
print(rand_vec)
33/29:
# let's save some arrays on the disk for use later!
np.save('saved_file_name',rand_mat2)
33/30: np.savez('zipped_file_name',rand_mat=rand_mat,rand_mat2=rand_mat2)
33/31:
# now let's load it
loaded_vec = np.load('saved_file_name.npy')
loaded_zip = np.load('zipped_file_name.npz')

print(loaded_vec)
print('')
print(loaded_zip)
33/32:
print(loaded_zip['rand_mat'])
print('')
print(loaded_zip['rand_mat2'])

new_array  = loaded_zip['rand_mat']
print(new_array)
33/33:
# we can also save/load as text files...but only single variables
np.savetxt('text_file_name.txt',rand_mat,delimiter=',')
rand_mat_txt = np.loadtxt('text_file_name.txt',delimiter=',')
print(rand_mat)
print('')
print(rand_mat_txt)
34/1:
import pandas as pd
import numpy as np  # numpy is not necessary for pandas, but we will use some np code in this example
# in general it's good practice to import all pacakages at the beginning
34/2:
# first let's look at series - think of this as a single column of a spreadsheet
# each entry in a series corresponds to an individual row in the spreadsheet
# we can create a series by converting a list, or numpy array

mylist = [5.4,6.1,1.7,99.8]
myarray = np.array(mylist)
34/3:
myseries1 = pd.Series(data=mylist)
print(myseries1)
myseries2 = pd.Series(data=myarray)
print(myseries2)
34/4:
# we access individual entries the same way as with lists and arrays
print(myseries1[2])
34/5:
# we can add labels to the entries of a series

mylabels = ['first','second','third','fourth']
myseries3 = pd.Series(data=mylist,index=mylabels)
print(myseries3)
34/6:
# we need not be explicit about the entries of pd.Series
myseries4 = pd.Series(mylist,mylabels)
print(myseries4)
34/7:
# we can also access entries using the index labels
print(myseries4['second'])
34/8:
# we can do math on series 
myseries5 = pd.Series([5.5,1.1,8.8,1.6],['first','third','fourth','fifth'])
print(myseries5)
print('')
print(myseries5+myseries4)
34/9:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=1,sort=False)
df1
34/10:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=0,sort=False)
df1
34/11:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=1,sort=True)
df1
34/12:
# we can create a new dataframe 
df2 = pd.DataFrame(np.random.randn(5,5))
df2
34/13:
# lets give labels to rows and columns
df3 = pd.DataFrame(np.random.randn(5,5),index=['first row','second row','third row','fourth row','fifth row'],
                   columns=['first col','second col','third col','fourth col','fifth col'])
df3
34/14:
# we can access individual series in a data frame
print(df3['second col'])
print('')
df3[['third col','first col']]
34/15:
# we can access rows of a dataframe
df3.loc['fourth row']
34/16: df3.iloc[2]
34/17: df3.loc[['fourth row','first row'],['second col','third col']]
34/18:
# we can use logical indexing for dataframes just like for numpy arrays
df3>0
34/19: print(df3[df3>0])
34/20:
# we can add columns to a dataframe
df3['sixth col'] = np.random.randn(5,1)
df3
34/21:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1,inplace=True)
34/22:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1)
34/23:
# lets give labels to rows and columns
df3 = pd.DataFrame(np.random.randn(5,5),index=['first row','second row','third row','fourth row','fifth row'],
                   columns=['first col','second col','third col','fourth col','fifth col'])
df3
34/24:
# we can access individual series in a data frame
print(df3['second col'])
print('')
df3[['third col','first col']]
34/25:
# we can access rows of a dataframe
df3.loc['fourth row']
34/26: df3.iloc[2]
34/27: df3.loc[['fourth row','first row'],['second col','third col']]
34/28:
# we can use logical indexing for dataframes just like for numpy arrays
df3>0
34/29: print(df3[df3>0])
34/30:
# we can add columns to a dataframe
df3['sixth col'] = np.random.randn(5,1)
df3
34/31:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1)
34/32: df3
34/33:
df4 = df3.drop('first col',axis=1)
df4
34/34:
df5 = df3.drop('second row',axis=0)
df5
34/35:
# we can remove a dataframe's index labels
df5.reset_index()
34/36: df5
34/37:
df5.reset_index(inplace=True)
df5
34/38:
# we can assign new names to the index
df5['new name'] = ['This','is','the','row']
df5
34/39:
df5.set_index('new name',inplace=True)
df5
35/1:
import pandas as pd
import numpy as np  # numpy is not necessary for pandas, but we will use some np code in this example
# in general it's good practice to import all pacakages at the beginning
35/2:
# first let's look at series - think of this as a single column of a spreadsheet
# each entry in a series corresponds to an individual row in the spreadsheet
# we can create a series by converting a list, or numpy array

mylist = [5.4,6.1,1.7,99.8]
myarray = np.array(mylist)
35/3:
myseries1 = pd.Series(data=mylist)
print(myseries1)
myseries2 = pd.Series(data=myarray)
print(myseries2)
35/4:
# we access individual entries the same way as with lists and arrays
print(myseries1[2])
35/5:
# we can add labels to the entries of a series

mylabels = ['first','second','third','fourth']
myseries3 = pd.Series(data=mylist,index=mylabels)
print(myseries3)
35/6:
# we need not be explicit about the entries of pd.Series
myseries4 = pd.Series(mylist,mylabels)
print(myseries4)
35/7:
# we can also access entries using the index labels
print(myseries4['second'])
35/8:
# we can do math on series 
myseries5 = pd.Series([5.5,1.1,8.8,1.6],['first','third','fourth','fifth'])
print(myseries5)
print('')
print(myseries5+myseries4)
35/9:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=1,sort=True)
df1
35/10:
# we can create a new dataframe 
df2 = pd.DataFrame(np.random.randn(5,5))
df2
35/11:
# lets give labels to rows and columns
df3 = pd.DataFrame(np.random.randn(5,5),index=['first row','second row','third row','fourth row','fifth row'],
                   columns=['first col','second col','third col','fourth col','fifth col'])
df3
35/12:
# we can access individual series in a data frame
print(df3['second col'])
print('')
df3[['third col','first col']]
35/13:
# we can access rows of a dataframe
df3.loc['fourth row']
35/14: df3.iloc[2]
35/15: df3.loc[['fourth row','first row'],['second col','third col']]
35/16:
# we can use logical indexing for dataframes just like for numpy arrays
df3>0
35/17: print(df3[df3>0])
35/18:
# we can add columns to a dataframe
df3['sixth col'] = np.random.randn(5,1)
df3
35/19:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1)
35/20: df3
35/21:
df4 = df3.drop('first col',axis=1)
df4
35/22:
df5 = df3.drop('second row',axis=0)
df5
35/23:
# we can remove a dataframe's index labels
df5.reset_index()
35/24: df5
35/25:
df5.reset_index(inplace=True)
df5
35/26:
# we can assign new names to the index
df5['new name'] = ['This','is','the','row']
df5
35/27:
df5.set_index('new name',inplace=True)
df5
35/28:


df7 = pd.DataFrame({"customer":['101','102','103','104'], 
                    'category': ['cat2','cat2','cat1','cat3'],
                    'important': ['yes','no','yes','yes'],
                    'sales': [123,52,214,663]},index=[0,1,2,3])

df8 = pd.DataFrame({"customer":['101','103','104','105'], 
                    'color': ['yellow','green','green','blue'],
                    'distance': [12,9,44,21],
                    'sales': [123,214,663,331]},index=[4,5,6,7])
35/29: pd.concat([df7,df8],axis=0,sort=False)
35/30:


df7 = pd.DataFrame({"customer":['101','102','103','104'], 
                    'category': ['cat2','cat2','cat1','cat3'],
                    'important': ['yes','no','yes','yes'],
                    'sales': [123,52,214,663]}) #,index=[0,1,2,3]

df8 = pd.DataFrame({"customer":['101','103','104','105'], 
                    'color': ['yellow','green','green','blue'],
                    'distance': [12,9,44,21],
                    'sales': [123,214,663,331]})#,index=[4,5,6,7]
35/31: pd.concat([df7,df8],axis=0,sort=False)
35/32: pd.concat([df7,df8],axis=0,sort=True)
35/33: pd.concat([df7,df8],axis=1,sort=False)
35/34:


df7 = pd.DataFrame({"customer":['101','102','103','104'], 
                    'category': ['cat2','cat2','cat1','cat3'],
                    'important': ['yes','no','yes','yes'],
                    'sales': [123,52,214,663]},index=[0,1,2,3]) 

df8 = pd.DataFrame({"customer":['101','103','104','105'], 
                    'color': ['yellow','green','green','blue'],
                    'distance': [12,9,44,21],
                    'sales': [123,214,663,331]},index=[4,5,6,7])
35/35: pd.concat([df7,df8],axis=0,sort=False)
35/36: pd.concat([df7,df8],axis=0,sort=True)
35/37: pd.concat([df7,df8],axis=1,sort=False)
35/38: pd.merge(df7,df8,how='outer',on='customer') # outer merge is union of on
35/39: pd.merge(df7,df8,how='inner',on='customer') # inner merge is intersection of on
35/40: pd.merge(df7,df8,how='right',on='customer') # left merge is just first on, but all columns ... right is second
35/41: pd.merge(df7,df8,how='left',on='customer') # left merge is just first on, but all columns ... right is second
35/42: pd.merge(df7,df8,how='right',on='customer') # left merge is just first on, but all columns ... right is second
35/43:
df9 = pd.DataFrame({'Q1': [101,102,103],
                    'Q2': [201,202,203]},
                   index=['I0','I1','I2'])

df10 = pd.DataFrame({'Q3': [301,302,303],
                    'Q4': [401,402,403]},
                   index=['I0','I2','I3'])
35/44:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='right') # outer, inner, left, and right work the same as merge
35/45:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='outer') # outer, inner, left, and right work the same as merge
35/46:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='inner') # outer, inner, left, and right work the same as merge
35/47:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='left') # outer, inner, left, and right work the same as merge
35/48:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='right') # outer, inner, left, and right work the same as merge
37/1:
import pandas as pd
import numpy as np  # numpy is not necessary for pandas, but we will use some np code in this example
# in general it's good practice to import all pacakages at the beginning
37/2:
# first let's look at series - think of this as a single column of a spreadsheet
# each entry in a series corresponds to an individual row in the spreadsheet
# we can create a series by converting a list, or numpy array

mylist = [5.4,6.1,1.7,99.8]
myarray = np.array(mylist)
37/3:
myseries1 = pd.Series(data=mylist)
print(myseries1)
myseries2 = pd.Series(data=myarray)
print(myseries2)
37/4:
# we access individual entries the same way as with lists and arrays
print(myseries1[2])
37/5:
# we can add labels to the entries of a series

mylabels = ['first','second','third','fourth']
myseries3 = pd.Series(data=mylist,index=mylabels)
print(myseries3)
37/6:
# we need not be explicit about the entries of pd.Series
myseries4 = pd.Series(mylist,mylabels)
print(myseries4)
37/7:
# we can also access entries using the index labels
print(myseries4['second'])
37/8:
# we can do math on series 
myseries5 = pd.Series([5.5,1.1,8.8,1.6],['first','third','fourth','fifth'])
print(myseries5)
print('')
print(myseries5+myseries4)
37/9:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=1,sort=True)
df1
37/10:
# we can create a new dataframe 
df2 = pd.DataFrame(np.random.randn(5,5))
df2
37/11:
# lets give labels to rows and columns
df3 = pd.DataFrame(np.random.randn(5,5),index=['first row','second row','third row','fourth row','fifth row'],
                   columns=['first col','second col','third col','fourth col','fifth col'])
df3
37/12:
# we can access individual series in a data frame
print(df3['second col'])
print('')
df3[['third col','first col']]
37/13:
# we can access rows of a dataframe
df3.loc['fourth row']
37/14: df3.iloc[2]
37/15: df3.loc[['fourth row','first row'],['second col','third col']]
37/16:
# we can use logical indexing for dataframes just like for numpy arrays
df3>0
37/17: print(df3[df3>0])
37/18:
# we can add columns to a dataframe
df3['sixth col'] = np.random.randn(5,1)
df3
37/19:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1)
37/20: df3
37/21:
df4 = df3.drop('first col',axis=1)
df4
37/22:
df5 = df3.drop('second row',axis=0)
df5
37/23:
# we can remove a dataframe's index labels
df5.reset_index()
37/24: df5
37/25:
df5.reset_index(inplace=True)
df5
37/26:
# we can assign new names to the index
df5['new name'] = ['This','is','the','row']
df5
37/27:
df5.set_index('new name',inplace=True)
df5
37/28:


df7 = pd.DataFrame({"customer":['101','102','103','104'], 
                    'category': ['cat2','cat2','cat1','cat3'],
                    'important': ['yes','no','yes','yes'],
                    'sales': [123,52,214,663]},index=[0,1,2,3]) 

df8 = pd.DataFrame({"customer":['101','103','104','105'], 
                    'color': ['yellow','green','green','blue'],
                    'distance': [12,9,44,21],
                    'sales': [123,214,663,331]},index=[4,5,6,7])
37/29: pd.concat([df7,df8],axis=0,sort=False)
37/30: pd.concat([df7,df8],axis=0,sort=True)
37/31: pd.concat([df7,df8],axis=1,sort=False)
37/32: pd.merge(df7,df8,how='outer',on='customer') # outer merge is union of on
37/33: pd.merge(df7,df8,how='inner',on='customer') # inner merge is intersection of on
37/34: pd.merge(df7,df8,how='right',on='customer') # left merge is just first on, but all columns ... right is second
37/35:
df9 = pd.DataFrame({'Q1': [101,102,103],
                    'Q2': [201,202,203]},
                   index=['I0','I1','I2'])

df10 = pd.DataFrame({'Q3': [301,302,303],
                    'Q4': [401,402,403]},
                   index=['I0','I2','I3'])
37/36:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='right') # outer, inner, left, and right work the same as merge
37/37:
# let's now go over a few more basic functialities of pandas

df8['color'].unique()
37/38: df8['color'].value_counts()
37/39: df9.mean()
37/40: df8.columns
37/41: df8
37/42:
new_df = df8[(df8['customer']!='105') & (df8['color']!='green')]
new_df
37/43:
print(df8['sales'].mean())
print(df8['distance'].min())
37/44:
def profit(s):
    return s*0.5 # 50% markup...
37/45: df8['sales'].apply(profit)
37/46: df8['color'].apply(len)
38/1:
import pandas as pd
import numpy as np  # numpy is not necessary for pandas, but we will use some np code in this example
# in general it's good practice to import all pacakages at the beginning
38/2:
# first let's look at series - think of this as a single column of a spreadsheet
# each entry in a series corresponds to an individual row in the spreadsheet
# we can create a series by converting a list, or numpy array

mylist = [5.4,6.1,1.7,99.8]
myarray = np.array(mylist)
38/3:
myseries1 = pd.Series(data=mylist)
print(myseries1)
myseries2 = pd.Series(data=myarray)
print(myseries2)
38/4:
# we access individual entries the same way as with lists and arrays
print(myseries1[2])
38/5:
# we can add labels to the entries of a series

mylabels = ['first','second','third','fourth']
myseries3 = pd.Series(data=mylist,index=mylabels)
print(myseries3)
38/6:
# we need not be explicit about the entries of pd.Series
myseries4 = pd.Series(mylist,mylabels)
print(myseries4)
38/7:
# we can also access entries using the index labels
print(myseries4['second'])
38/8:
# we can do math on series 
myseries5 = pd.Series([5.5,1.1,8.8,1.6],['first','third','fourth','fifth'])
print(myseries5)
print('')
print(myseries5+myseries4)
38/9:
# we can combine series to create a dataframe using the concat function
df1 = pd.concat([myseries4,myseries5],axis=1,sort=True)
df1
38/10:
# we can create a new dataframe 
df2 = pd.DataFrame(np.random.randn(5,5))
df2
38/11:
# lets give labels to rows and columns
df3 = pd.DataFrame(np.random.randn(5,5),index=['first row','second row','third row','fourth row','fifth row'],
                   columns=['first col','second col','third col','fourth col','fifth col'])
df3
38/12:
# we can access individual series in a data frame
print(df3['second col'])
print('')
df3[['third col','first col']]
38/13:
# we can access rows of a dataframe
df3.loc['fourth row']
38/14: df3.iloc[2]
38/15: df3.loc[['fourth row','first row'],['second col','third col']]
38/16:
# we can use logical indexing for dataframes just like for numpy arrays
df3>0
38/17: print(df3[df3>0])
38/18:
# we can add columns to a dataframe
df3['sixth col'] = np.random.randn(5,1)
df3
38/19:
# we can remove columns or rows from a dataframe
df3.drop('first col',axis=1)
38/20: df3
38/21:
df4 = df3.drop('first col',axis=1)
df4
38/22:
df5 = df3.drop('second row',axis=0)
df5
38/23:
# we can remove a dataframe's index labels
df5.reset_index()
38/24: df5
38/25:
df5.reset_index(inplace=True)
df5
38/26:
# we can assign new names to the index
df5['new name'] = ['This','is','the','row']
df5
38/27:
df5.set_index('new name',inplace=True)
df5
38/28:


df7 = pd.DataFrame({"customer":['101','102','103','104'], 
                    'category': ['cat2','cat2','cat1','cat3'],
                    'important': ['yes','no','yes','yes'],
                    'sales': [123,52,214,663]},index=[0,1,2,3]) 

df8 = pd.DataFrame({"customer":['101','103','104','105'], 
                    'color': ['yellow','green','green','blue'],
                    'distance': [12,9,44,21],
                    'sales': [123,214,663,331]},index=[4,5,6,7])
38/29: pd.concat([df7,df8],axis=0,sort=False)
38/30: pd.concat([df7,df8],axis=0,sort=True)
38/31: pd.concat([df7,df8],axis=1,sort=False)
38/32: pd.merge(df7,df8,how='outer',on='customer') # outer merge is union of on
38/33: pd.merge(df7,df8,how='inner',on='customer') # inner merge is intersection of on
38/34: pd.merge(df7,df8,how='right',on='customer') # left merge is just first on, but all columns ... right is second
38/35:
df9 = pd.DataFrame({'Q1': [101,102,103],
                    'Q2': [201,202,203]},
                   index=['I0','I1','I2'])

df10 = pd.DataFrame({'Q3': [301,302,303],
                    'Q4': [401,402,403]},
                   index=['I0','I2','I3'])
38/36:
# join behaves just like merge, 
# except instead of using the values of one of the columns 
# to combine data frames, it uses the index labels
df9.join(df10,how='right') # outer, inner, left, and right work the same as merge
38/37:
# let's now go over a few more basic functialities of pandas

df8['color'].unique()
38/38: df8['color'].value_counts()
38/39: df9.mean()
38/40: df8.columns
38/41: df8
38/42:
new_df = df8[(df8['customer']!='105') & (df8['color']!='green')]
new_df
38/43:
print(df8['sales'].mean())
print(df8['distance'].min())
38/44:
def profit(s):
    return s*0.5 # 50% markup...
38/45: df8['sales'].apply(profit)
38/46: df8['color'].apply(len)
38/47:
df11 = df8[['distance','sales']]
df11.applymap(profit)
38/48:
def col_sum(co):
    return sum(co)
df11.apply(col_sum)
38/49: df11.applymap(col_sum)
38/50:
del df8['color']
df8
38/51: df8.index
38/52:
df8.sort_values(by='distance',inplace=True)
df8
38/53: df8
38/54:
# if some series has multiple of the same value then we can group all the unique entries together
mydict = {'customer': ['Customer 1','Customer 1','Customer2','Customer2','Customer3','Customer3'], 
          'product1': [1.1,2.1,3.8,4.2,5.5,6.9],
          'product2': [8.2,9.1,11.1,5.2,44.66,983]}
df6 = pd.DataFrame(mydict,index=['Purchase 1','Purchase 2','Purchase 3','Purchase 4','Purchase 5','Purchase 6'])
df6
38/55:
grouped_data = df6.groupby('customer')
print(grouped_data)
38/56: grouped_data.std()
38/57: grouped_data.describe()
38/58: grouped_data.mean()
38/59: grouped_data.std()
38/60: df8
38/61:
# similar to numpy arrays, we can also save and load dataframes to csv files, and also Excel files

df8.to_csv('df8.csv',index=True)
38/62:
new_df8 = pd.read_csv('df8.csv',index_col=0)
new_df8
38/63:
df8.to_excel('df8.xlsx',index=False,sheet_name='first sheet')
newer_df8 = pd.read_excel('df8.xlsx',sheet_name='first sheet',index_col=1)
newer_df8
38/64: demo_array = np.arange(0,10)
38/65:
import numpy as np
demo_array
demo_array <3
demo_array[demo_array <6]
np.max(demo_array)
38/66: demo_array
38/67: demo_array <3
38/68: demo_array[demo_array <6]
38/69: np.max(demo_array)
38/70: demo_matrix = np.array(([13,35,74,48], [23,37,37,38],[73,39,93,39]))
38/71: demo_matrix[:, (1,2)]
38/72: test_array = [10, 11.5, 12, 13.5, 14,15]
38/73: test_array[2:3]
38/74: list_l =[[12,34,55,],[66,45,77],[45,77,88]]
38/75: np.array(list_l)
38/76:
import pandas as pd

score = [10, 15, 20, 25]
pd.Series(data=score, index = ['a','b','c','d'])
38/77: np.arange(0,22,6)
38/78: subset_demo_array[:]= 101
38/79: array = ([101, 101, 101, 101, 101, 101, 101])
38/80: subset_demo_array[:]= 101
38/81: subset_demo_array= ([101, 101, 101, 101, 101, 101, 101])
38/82: subset_demo_array[:]= 101
38/83:
demo_array = np.arange(10,21)

subset_demo_array = demo_array[0:7]

subset_demo_array[:]= 101

subset_demo_array
38/84:
flowers = pd.Series([2, 3, 5, 4], index=['lily', 'rose', 'daisy', 'lotus'])

flowers['daisy']
38/85:
s1 = pd.Series(['a', 'b'])

s2 = pd.Series(['c', 'd'])
38/86: pd.concat([s1, s2])
40/1:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)

def submitAccuracy():
    return acc
40/2:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/svm")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)

def submitAccuracy():
    return acc
40/3:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/svm")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, features_test)

def submitAccuracy():
    return acc
40/4:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/svm")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()



features_train = features_train[:int(len(features_train)/100)]
labels_train = labels_train[:int(len(labels_train)/100)]

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)

def submitAccuracy():
    return acc
40/5:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/tools")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()



features_train = features_train[:int(len(features_train)/100)]
labels_train = labels_train[:int(len(labels_train)/100)]

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)

def submitAccuracy():
    return acc
40/6:
def classify(features_train, labels_train):
    
    ### your code goes here--should return a trained decision tree classifer
    
    from sklearn import tree
    
    clf = tree.DecisionTreeClassifier()
    
    clf = clf.fit(features_train, labels_train)

    
    
    return clf
39/1:
def classify(features_train, labels_train):
    
    ### your code goes here--should return a trained decision tree classifer
    
    from sklearn import tree
    
    clf = tree.DecisionTreeClassifier()
    
    clf = clf.fit(features_train, labels_train)

    
    
    return clf
39/2:
def classify(features_train, labels_train, features_test, labels_test):
    """ compute the accuracy of your Naive Bayes classifier """
    ### import the sklearn module for GaussianNB
    from sklearn.naive_bayes import GaussianNB

    ### create classifier
    clf = GaussianNB()

    ### fit the classifier on the training features and labels
    clf.fit(features_train, labels_train)

    ### use the trained classifier to predict labels for the test features
    pred = clf.predict(features_test)


    ### calculate and return the accuracy on the test data
    ### this is slightly different than the example, 
    ### where we just print the accuracy
    ### you might need to import an sklearn module
    
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(pred, labels_test)
    return accuracy
39/3:
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData
from classify import classify

import matplotlib.pyplot as plt
import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()

def submitAccuracy():
    accuracy = classify(features_train, labels_train, features_test, labels_test)
    return accuracy
40/7:
#!/usr/bin/python3

""" 
    This is the code to accompany the Lesson 2 (SVM) mini-project.

    Use a SVM to identify emails from the Enron corpus by their authors:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("/home/alikaiser12/Desktop/ML-Udacity/ud120-projects/tools")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()



features_train = features_train[:int(len(features_train)/100)]
labels_train = labels_train[:int(len(labels_train)/100)]

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)

def submitAccuracy():
    return acc
40/8:
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData
from classify import classify

import matplotlib.pyplot as plt
import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()

def submitAccuracy():
    accuracy = classify(features_train, labels_train, features_test, labels_test)
    return accuracy
39/4:
import sys
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData

import matplotlib.pyplot as plt
import copy
import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)
40/9:
import sys
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData

import matplotlib.pyplot as plt
import copy
import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel="linear")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data




clf.fit (features_train, labels_train)

pred = clf.predict(features_test)




#### store your predictions in a list named pred





from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)
41/1:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###


#########################################################
41/2:
import sys
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData

from sklearn import tree

import numpy as np
import pylab as pl

features_train, labels_train, features_test, labels_test = makeTerrainData()

clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################



#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}
44/1:
import sys
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData

from sklearn import tree

import matplotlib.pyplot as plt
import numpy as np
import pylab as pl

features_train, labels_train, features_test, labels_test = makeTerrainData()



########################## DECISION TREE #################################


### your code goes here--now create 2 decision tree classifiers,
### one with min_samples_split=2 and one with min_samples_split=50
### compute the accuracies on the testing data and store
### the accuracy numbers to acc_min_samples_split_2 and
### acc_min_samples_split_50, respectively

clfa = tree.DecisionTreeClassifier()
    
clf1 = clfa.fit(features_train, labels_train)

pred1 = clf1.predict(features_test)

from sklearn.metrics import accuracy_score
acc_min_samples_split_2 = accuracy_score(pred1, labels_test)



clfb = tree.DecisionTreeClassifier()
    
clf2 = clfb.fit(features_train, labels_train)

pred2 = clf2.predict(features_test)


from sklearn.metrics import accuracy_score
acc_min_samples_split_50 = accuracy_score(pred2, labels_test)





def submitAccuracies():
  return {"acc_min_samples_split_2":round(acc_min_samples_split_2,3),
          "acc_min_samples_split_50":round(acc_min_samples_split_50,3)}
44/2:
import math
-0.5*Math.log(0.5,2) -0.5*Math.log(0.5,2)
44/3:
import math
-0.5*math.log(0.5,2) -0.5*math.log(0.5,2)
44/4:
import math
-0.5*math.log(0.25,2) -0.5*math.log(0.25,2)
44/5:
import math
-0.25*math.log(0.25,2) -0.25*math.log(0.25,2)
44/6:
import math
-0.25*math.log(0.25,2)
44/7: -2/3*math.log(2/3,2)
44/8: -2/3*math.log(2/3,2)-1/3*math.log(1/3,2)
44/9: 1-(3*0.9184)/3
44/10: -0.0816*math.log(0.0816,2)
44/11: 1-0.29500742226636106
44/12: 1-((3*0.9184)/4)
44/13:
import math
-0.5*math.log(0.5,2)
44/14:
import math
-0.5*math.log(0.5,2)
44/15:
import math
-0.5*math.log(0.5,2) -0.5*math.log(0.5,2)
46/1:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###


#########################################################
46/2:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###

features_train, labels_train, features_test, labels_test = makeTerrainData()

clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################



#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}


#########################################################
46/3:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###



clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################



#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}


#########################################################
46/4:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess
from sklearn import tree


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###



clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################



#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}


#########################################################
45/1:
import sys
from class_vis import prettyPicture
from prep_terrain_data import makeTerrainData

from sklearn import tree

import numpy as np
import pylab as pl

features_train, labels_train, features_test, labels_test = makeTerrainData()

min_samples_split=40

clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################




#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}
46/5:
#!/usr/bin/python

""" 
    This is the code to accompany the Lesson 3 (decision tree) mini-project.

    Use a Decision Tree to identify emails from the Enron corpus by author:    
    Sara has label 0
    Chris has label 1
"""
    
import sys
from time import time
sys.path.append("../tools/")
from email_preprocess import preprocess
from sklearn import tree


### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###

min_samples_split=40

clf = tree.DecisionTreeClassifier()
    
clf = clf.fit(features_train, labels_train)


#################################################################################


########################## DECISION TREE #################################



#### your code goes here

pred = clf.predict(features_test)


from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)



    
def submitAccuracies():
  return {"acc":round(acc,3)}


#########################################################
47/1: X = [[0], [1], [2], [3]]
47/2:
X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)

neigh.fit(X, y)
KNeighborsClassifier(...)

print(neigh.predict([[1.1]]))
47/3: print(neigh.predict_proba([[0.9]]))
47/4: samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
47/5: from sklearn.neighbors import NearestNeighbors
47/6: neigh = NearestNeighbors(n_neighbors=1)
47/7: neigh.fit(samples)
47/8: print(neigh.kneighbors([[1., 1., 1.]]))
47/9: X = [[0., 1., 0.], [1., 0., 1.]]
47/10: neigh.kneighbors(X, return_distance=False)
47/11:
X = [[0], [3], [1]]
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=2)

neigh.fit(X)
47/12:
A = neigh.kneighbors_graph(X)
A.toarray()
47/13:
import numpy as np
from scipy.stats import randint
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import HalvingRandomSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

rng = np.random.RandomState(0)

X, y = make_classification(n_samples=700, random_state=rng)

clf = RandomForestClassifier(n_estimators=10, random_state=rng)

param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 11),
              "min_samples_split": randint(2, 11),
              "bootstrap": [True, False],
              "criterion": ["gini", "entropy"]}

rsh = HalvingRandomSearchCV(estimator=clf, param_distributions=param_dist,
                            factor=2, random_state=rng)
rsh.fit(X, y)
rsh.best_params_
47/14:
import numpy as np
from sklearn import datasets
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.svm import SVC

rng = np.random.RandomState(42)
iris = datasets.load_iris()
random_unlabeled_points = rng.rand(iris.target.shape[0]) < 0.3
iris.target[random_unlabeled_points] = -1
svc = SVC(probability=True, gamma="auto")
self_training_model = SelfTrainingClassifier(svc)
self_training_model.fit(iris.data, iris.target)
47/15:
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True, as_frame=True)
feature_names = X.columns
knn = KNeighborsClassifier(n_neighbors=3)
sfs = SequentialFeatureSelector(knn, n_features_to_select=2)
sfs.fit(X, y)
print("Features selected by forward sequential selection: "
      f"{feature_names[sfs.get_support()].tolist()}")
49/1:
print(__doc__)


# Code source: Gal Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
# Though the following import is not directly being used, it is required
# for 3D projection to work
from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn import datasets

np.random.seed(5)

iris = datasets.load_iris()
X = iris.data
y = iris.target

estimators = [('k_means_iris_8', KMeans(n_clusters=8)),
              ('k_means_iris_3', KMeans(n_clusters=3)),
              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,
                                               init='random'))]

fignum = 1
titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']
for name, est in estimators:
    fig = plt.figure(fignum, figsize=(4, 3))
    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
    est.fit(X)
    labels = est.labels_

    ax.scatter(X[:, 3], X[:, 0], X[:, 2],
               c=labels.astype(float), edgecolor='k')

    ax.w_xaxis.set_ticklabels([])
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])
    ax.set_xlabel('Petal width')
    ax.set_ylabel('Sepal length')
    ax.set_zlabel('Petal length')
    ax.set_title(titles[fignum - 1])
    ax.dist = 12
    fignum = fignum + 1

# Plot the ground truth
fig = plt.figure(fignum, figsize=(4, 3))
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

for name, label in [('Setosa', 0),
                    ('Versicolour', 1),
                    ('Virginica', 2)]:
    ax.text3D(X[y == label, 3].mean(),
              X[y == label, 0].mean(),
              X[y == label, 2].mean() + 2, name,
              horizontalalignment='center',
              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(float)
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
ax.set_title('Ground Truth')
ax.dist = 12

fig.show()
51/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
51/2: df = pd.read_csv("Classified Data",index_col=0)
51/3: df.head()
51/4: from sklearn.preprocessing import StandardScaler
51/5: scaler = StandardScaler()
51/6: scaler.fit(df.drop('TARGET CLASS',axis=1))
51/7: scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
51/8:
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()
51/9: from sklearn.model_selection import train_test_split
51/10:
X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],
                                                    test_size=0.30)
51/11: from sklearn.neighbors import KNeighborsClassifier
51/12: knn = KNeighborsClassifier(n_neighbors=1)
51/13: knn.fit(X_train,y_train)
51/14: pred = knn.predict(X_test)
51/15: from sklearn.metrics import classification_report,confusion_matrix
51/16: print(confusion_matrix(y_test,pred))
51/17: print(classification_report(y_test,pred))
51/18:
error_rate = []

# Will take some time
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
51/19:
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
51/20:
# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1
knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=1')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
51/21:
# NOW WITH K=23
knn = KNeighborsClassifier(n_neighbors=23)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=23')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
53/1:
import pandas as pd
df = pd.read("salaries.csv")
df.head()
53/2:
import pandas as pd
df = pd.read_csv("salaries.csv")
df.head()
53/3: inputs = df.drop("salary_more_then_100k", axis="columns")
53/4: inputs
53/5:
inputs = df.drop("salary_more_then_100k", axis="columns")
target = df["salary_more_then_100k"]
53/6: inputs
53/7: target
53/8: from skleran.preprocessing import LabelEncoder
53/9: from sklearn.preprocessing import LabelEncoder
53/10:
le_company=LabelEncoder()
le_job=LabelEncoder()
le_degree=LabelEncoder()
53/11:
inputs['company_n'] = le_company.fit_transform(inputs['company'])
inputs['job_n'] = le_company.fit_transform(inputs['job'])
inputs['degree_n'] = le_company.fit_transform(inputs['degree'])
inputs.head()
53/12:
inputs_n = inputs.drop(['company','job','degree'], axis='columns')
inputs_n
53/13: from sklearn import tree
53/14: model = tree.DecisionTreeClassifier()
53/15: model.fit(inputs_n, target)
53/16: model.score(inputs_n, target)
53/17: model.predict([[2,2,1]])
53/18: model.predict([[2,0,1]])
55/1: import pandas as pd
55/2:
df = pd.read_csv("titanic.csv")
df.head()
55/3: df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
55/4: df.head()
55/5:
inputs = df.drop('Survived',axis='columns')
target = df.Survived
55/6: inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})
55/7: inputs.Age[:10]
55/8: inputs.Age = inputs.Age.fillna(inputs.Age.mean())
55/9: inputs.head()
55/10: inputs.head(10)
55/11: inputs.head(100)
55/12: inputs.head()
55/13: from sklearn.model_selection import train_test_split
55/14: X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
55/15: len(X_train)
55/16: len(X_test)
55/17:
from sklearn import tree
model = tree.DecisionTreeClassifier()
55/18: model.fit(X_train,y_train)
55/19: model.score(X_test,y_test)
57/1:
import pandas as pd
from sklearn.datasets import load_digits
digits = load_digits()
57/2: dir(digits)
57/3:
%matplotlib inline
import matplotlib.pyplot as plt
57/4:
plt.gray() 
for i in range(4):
    plt.matshow(digits.images[i])
57/5: digits.data[:5]
57/6:
df = pd.DataFrame(digits.data)
df.head()
57/7: df['target'] = digits.target
57/8: df[0:12]
57/9:
X = df.drop('target',axis='columns')
y = df.target
57/10:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
57/11:
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=20)
model.fit(X_train, y_train)
57/12: model.score(X_test, y_test)
57/13: y_predicted = model.predict(X_test)
57/14:
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_predicted)
cm
57/15:
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
57/16:
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=30)
model.fit(X_train, y_train)
57/17: model.score(X_test, y_test)
59/1:
from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
59/2:
import pandas as pd
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df.head()
59/3:
df['target'] = iris.target
df.head()
59/4:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2)
59/5:
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)
59/6: model.score(X_test,y_test)
59/7:
model = RandomForestClassifier(n_estimators=40)
model.fit(X_train, y_train)
model.score(X_test,y_test)
61/1:
import pandas as pd
import seaborn as sns   # Why sns?  It's a reference to The West Wing
import matplotlib.pyplot as plt  # seaborn is based on matplotlib
sns.set(color_codes=True) # adds a nice background to the graphs
%matplotlib inline 
# tells python to actually display the graphs
61/2: auto = pd.read_csv('Automobile.csv')
61/3: auto = pd.read_csv('Automobile.csv')
61/4: auto.head()
61/5: auto.head()
61/6: sns.histplot(auto.highway_mpg);
61/7:
# we can turn the kde off and put a tic mark along the x-axis for every data point with rug
sns.rugplot(data = auto, x = 'city_mpg');
sns.histplot(auto.highway_mpg)
61/8: sns.jointplot(data = auto, x = 'engine_size', y = 'horsepower');
61/9: sns.jointplot(data = auto, x = 'engine_size', y = 'horsepower')
61/10: sns.jointplot(data = auto, x = 'engine_size', y = 'horsepower', kind="hex");
61/11: sns.jointplot(data = auto, x= 'engine_size', y = 'horsepower', kind="kde");
61/12: sns.pairplot(auto[['normalized_losses', 'engine_size', 'horsepower']]);
62/1: sns.stripplot(data = auto, x = 'fuel_type', y='horsepower', jitter=True);
62/2:
import pandas as pd
import seaborn as sns   # Why sns?  It's a reference to The West Wing
import matplotlib.pyplot as plt  # seaborn is based on matplotlib
sns.set(color_codes=True) # adds a nice background to the graphs
%matplotlib inline 
# tells python to actually display the graphs
62/3: auto = pd.read_csv('Automobile.csv')
62/4: auto.head()
62/5: sns.histplot(auto.highway_mpg);
62/6:
# we can turn the kde off and put a tic mark along the x-axis for every data point with rug
sns.rugplot(data = auto, x = 'city_mpg');
sns.histplot(auto.highway_mpg)
62/7: sns.jointplot(data = auto, x = 'engine_size', y = 'horsepower')
62/8: sns.jointplot(data = auto, x = 'engine_size', y = 'horsepower', kind="hex");
62/9: sns.jointplot(data = auto, x= 'engine_size', y = 'horsepower', kind="kde");
62/10: sns.pairplot(auto[['normalized_losses', 'engine_size', 'horsepower']]);
62/11: sns.stripplot(data = auto, x = 'fuel_type', y='horsepower', jitter=True);
62/12: sns.stripplot(data = auto, x = 'fuel_type', y='horsepower', jitter=False);
62/13: sns.swarmplot(data = auto, x='fuel_type', y ='horsepower');
62/14: sns.boxplot(data = auto, x ='number_of_doors', y ='horsepower');
62/15: sns.boxplot(data = auto,x='number_of_doors',y='horsepower', hue=auto['fuel_type']);
62/16: sns.barplot(data = auto, x='body_style',y='horsepower', hue=auto['fuel_type']);
62/17: sns.countplot(data = auto, x='body_style',hue='fuel_type');
62/18: sns.pointplot(data = auto,x='body_style',y= 'horsepower', hue='number_of_doors');
62/19:
sns.catplot(x="fuel_type",
               y = "horsepower",
               hue="number_of_doors", 
               col="drive_wheels", 
               data=auto, 
               kind="box");
62/20:
sns.catplot(x="fuel_type",
               y = "horsepower",
               hue="number_of_doors", 
               col="drive_wheels", 
               data=auto, 
               kind="violin");
62/21:
sns.catplot(x="fuel_type",
               y = "horsepower",
               hue="number_of_doors", 
               col="drive_wheels", 
               data=auto, 
               kind="count");
62/22:
sns.catplot(x="fuel_type",
               y = "horsepower",
               hue="number_of_doors", 
               col="drive_wheels", 
               data=auto, 
               kind="count");
62/23:
sns.catplot(x="fuel_type",
           y = "horsepower",
           hue="number_of_doors", 
           col="drive_wheels", 
           data=auto, 
           kind="count");
62/24:
sns.catplot(x="fuel_type",
               #y = "horsepower",
               hue="number_of_doors", 
               col="drive_wheels", 
               data=auto, 
               kind="count");
62/25: sns.lmplot(y="horsepower", x="engine_size", data=auto);
62/26: sns.lmplot(y="horsepower", x="engine_size",hue="fuel_type", data=auto);
64/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from PIL import Image
64/2: data = pd.read_csv('Melbourne_housing_FULL.csv')
64/3: data = data['Distance']
64/4: len(data)
64/5: data.isnull().sum()
64/6: data = data.dropna() # drops all missing values in the data
64/7:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=50)
64/8:
sns.distplot(data) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
64/9: sns.distplot(data, hist=False) # adding an argument to plot only frequency polygon
64/10: sns.violinplot(data) # plots a violin plt using the seaborn package.
64/11:
plt.figure(figsize=(20,10)) # makes the plot wider
plt.hist(data, color='g') # plots a simple histogram
plt.axvline(data.mean(), color='m', linewidth=1)
plt.axvline(data.median(), color='b', linestyle='dashed', linewidth=1)
plt.axvline(data.mode()[0], color='w', linestyle='dashed', linewidth=1)
64/12: sns.distplot(data, hist_kws=dict(cumulative=True), kde_kws=dict(cumulative=True))
64/13: data1 = pd.read_csv('Melbourne_housing_FULL.csv')
64/14: data1.head()
65/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from PIL import Image
65/2: data = pd.read_csv('Melbourne_housing_FULL.csv')
65/3: data = data['Distance']
65/4: len(data)
65/5: data.isnull().sum()
66/1:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from PIL import Image
66/2: data = pd.read_csv('Melbourne_housing_FULL.csv')
66/3: data = data['Distance']
66/4: len(data)
66/5: data.isnull().sum()
66/6: data = data.dropna() # drops all missing values in the data
66/7:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=50)
66/8:
sns.distplot(data) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
66/9: sns.distplot(data, hist=False) # adding an argument to plot only frequency polygon
66/10: sns.violinplot(data) # plots a violin plt using the seaborn package.
66/11:
plt.figure(figsize=(20,10)) # makes the plot wider
plt.hist(data, color='g') # plots a simple histogram
plt.axvline(data.mean(), color='m', linewidth=1)
plt.axvline(data.median(), color='b', linestyle='dashed', linewidth=1)
plt.axvline(data.mode()[0], color='w', linestyle='dashed', linewidth=1)
66/12: sns.distplot(data, hist_kws=dict(cumulative=True), kde_kws=dict(cumulative=True))
66/13: data1 = pd.read_csv('Melbourne_housing_FULL.csv')
66/14: data1.head()
66/15: data1 = data1.dropna().reset_index(drop=True)
66/16: sns.pairplot(data1)
66/17: sns.scatterplot(data1['Price'], data1['Distance'])  # Plots the scatter plot using two variables
66/18: data1.corr()   # displays the correlation between every possible pair of attributes as a dataframe
66/19: sns.heatmap(data1.corr(), annot=True)  # plot the correlation coefficients as a heatmap
67/1:
import warnings
warnings.filterwarnings('ignore')
67/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
67/3: data=pd.read_csv('master.csv')
68/1:
import warnings
warnings.filterwarnings('ignore')
68/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
68/3: data=pd.read_csv('master.csv')
68/4: data.head()
68/5: data.describe()
68/6: data.columns
68/7: data.shape
68/8: data.dtypes.value_counts()
68/9: data.info()
68/10:
def missing_check(df):
    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values
    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  # percentage of values that are null
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])  # putting the above two together
    return missing_data # return the dataframe
missing_check(data)
68/11: data[['suicides_no','population','suicides/100k pop','gdp_per_capita ($)']].describe() #descriptive stats of continuous columns
68/12:
my_tab = pd.crosstab(index=data["age"],  # Make a crosstab
                     columns="count")                  # Name the count column
my_tab
68/13:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no']).tail(10).plot(x='country',
                                                                                                           y='suicides_no',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
68/14:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no'],
                    ascending=True).head(10).plot(x='country',y='suicides_no',kind='bar', figsize=(15,5))

plt.show()
68/15:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='age', y='suicides_no', data=data, palette='muted')  # barplot
68/16:
plt.figure(figsize=(8,4))
ax = sns.barplot(x="sex", y="suicides_no", data=data)
70/1:
import warnings
warnings.filterwarnings('ignore')
70/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
70/3: data=pd.read_csv('master.csv')
70/4: data.head()
70/5: data.describe()
70/6: data.columns
70/7: data.shape
70/8: data.dtypes.value_counts()
70/9: data.info()
70/10:
def missing_check(df):
    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values
    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  # percentage of values that are null
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])  # putting the above two together
    return missing_data # return the dataframe
missing_check(data)
70/11: data[['suicides_no','population','suicides/100k pop','gdp_per_capita ($)']].describe() #descriptive stats of continuous columns
70/12:
my_tab = pd.crosstab(index=data["age"],  # Make a crosstab
                     columns="count")                  # Name the count column
my_tab
70/13:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no']).tail(10).plot(x='country',
                                                                                                           y='suicides_no',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
70/14:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no'],
                    ascending=True).head(10).plot(x='country',y='suicides_no',kind='bar', figsize=(15,5))

plt.show()
70/15:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no'],
                    ascending=True).head(10).plot(x='country',y='suicides_no',kind='bar', figsize=(15,5))

plt.show()
70/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='age', y='suicides_no', data=data, palette='muted')  # barplot
70/17:
plt.figure(figsize=(8,4))
ax = sns.barplot(x="sex", y="suicides_no", data=data)
70/18:
plt.figure(figsize=(15,5))

ax = sns.barplot(x='generation', y='suicides_no', data=data)
70/19:
figure = plt.figure(figsize=(15,5))

ax = sns.scatterplot(x=data['population'],y='suicides_no', data=data, size = "suicides_no") # scatter plot
70/20:
figure = plt.figure(figsize=(50,15))

ax = sns.regplot(x='population',y='suicides_no', data=data ) # regression plot - scatter plot with a regression line
70/21:
#Here we plotting a line plot.
sns.lineplot(x='population',y='suicides_no', data=data.head() )
70/22:
figure = plt.figure(figsize=(15,7))

sns.scatterplot(x='gdp_per_capita ($)', y='suicides/100k pop', data=data) # scatter plot 
plt.show()
70/23:
plt.figure(figsize=(10,5))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f', center = 1 )  # heatmap
plt.show()
70/24:
plt.figure(figsize=(15,5))
sns.barplot(data=data,x='sex',y='suicides_no',hue='age')
plt.show()
70/25:
plt.figure(figsize=(15,5))
sns.barplot(data=data,x='sex',y='suicides_no',hue='generation')
plt.show()
70/26:
suic_sum_m = data['suicides_no'].groupby([data['country'],data['sex']]).sum()  # number of suicides by country and sex
suic_sum_m = suic_sum_m.reset_index().sort_values(by='suicides_no',ascending=False) # sort in descending order
most_cont_m = suic_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of suicides for 31 years.')

sns.barplot(y='country',x='suicides_no',hue='sex',data=most_cont_m,palette='Set2');

plt.ylabel('Count of suicides')
plt.tight_layout()
70/27:
plt.figure(figsize=(15,5))

sns.pointplot(x="generation", y="suicides_no", hue = 'sex',  data=data)
plt.show()
70/28:
plt.figure(figsize=(10,5))
sns.violinplot(x=data.generation, y=data['population'])
plt.show()
70/29:
data[['year','suicides_no']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
70/30:
data[['year','population']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
70/31:
data[['year','suicides/100k pop']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
71/1:
import warnings
warnings.filterwarnings('ignore')
72/1:
import warnings
warnings.filterwarnings('ignore')
72/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
72/3: data=pd.read_csv('master.csv')
72/4: data.head()
72/5: data.describe()
72/6: data.columns
72/7: data.shape
72/8: data.dtypes.value_counts()
72/9: data.info()
72/10:
def missing_check(df):
    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values
    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  # percentage of values that are null
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])  # putting the above two together
    return missing_data # return the dataframe
missing_check(data)
72/11: data[['suicides_no','population','suicides/100k pop','gdp_per_capita ($)']].describe() #descriptive stats of continuous columns
72/12:
my_tab = pd.crosstab(index=data["age"],  # Make a crosstab
                     columns="count")                  # Name the count column
my_tab
72/13:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no']).tail(10).plot(x='country',
                                                                                                           y='suicides_no',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
72/14:
data.groupby(by=['country'])['suicides_no'].sum().reset_index().sort_values(['suicides_no'],
                    ascending=True).head(10).plot(x='country',y='suicides_no',kind='bar', figsize=(15,5))

plt.show()
72/15:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='age', y='suicides_no', data=data, palette='muted')  # barplot
72/16:
plt.figure(figsize=(8,4))
ax = sns.barplot(x="sex", y="suicides_no", data=data)
72/17:
plt.figure(figsize=(15,5))

ax = sns.barplot(x='generation', y='suicides_no', data=data)
72/18:
figure = plt.figure(figsize=(15,5))

ax = sns.scatterplot(x=data['population'],y='suicides_no', data=data, size = "suicides_no") # scatter plot
72/19:
figure = plt.figure(figsize=(50,15))

ax = sns.regplot(x='population',y='suicides_no', data=data ) # regression plot - scatter plot with a regression line
72/20:
#Here we plotting a line plot.
sns.lineplot(x='population',y='suicides_no', data=data.head() )
72/21:
figure = plt.figure(figsize=(15,7))

sns.scatterplot(x='gdp_per_capita ($)', y='suicides/100k pop', data=data) # scatter plot 
plt.show()
72/22:
plt.figure(figsize=(10,5))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f', center = 1 )  # heatmap
plt.show()
72/23:
plt.figure(figsize=(15,5))
sns.barplot(data=data,x='sex',y='suicides_no',hue='age')
plt.show()
72/24:
plt.figure(figsize=(15,5))
sns.barplot(data=data,x='sex',y='suicides_no',hue='generation')
plt.show()
72/25:
suic_sum_m = data['suicides_no'].groupby([data['country'],data['sex']]).sum()  # number of suicides by country and sex
suic_sum_m = suic_sum_m.reset_index().sort_values(by='suicides_no',ascending=False) # sort in descending order
most_cont_m = suic_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of suicides for 31 years.')

sns.barplot(y='country',x='suicides_no',hue='sex',data=most_cont_m,palette='Set2');

plt.ylabel('Count of suicides')
plt.tight_layout()
72/26:
plt.figure(figsize=(15,5))

sns.pointplot(x="generation", y="suicides_no", hue = 'sex',  data=data)
plt.show()
72/27:
plt.figure(figsize=(10,5))
sns.violinplot(x=data.generation, y=data['population'])
plt.show()
72/28:
data[['year','suicides_no']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
72/29:
data[['year','population']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
72/30:
data[['year','suicides/100k pop']].groupby(['year']).sum().plot(figsize=(15,5))

plt.show()
72/31:
#Installation step 
#!pip install pandas-profiling
#or 
import sys
!{sys.executable} -m pip install pandas-profiling
72/32:
#import pandas_profiling
import pandas_profiling
72/33: df = pd.read_csv('match_data.csv')
72/34:
#Getting the pandas profiling report 
pandas_profiling.ProfileReport(df)
74/1:
total_cases = 9123

if total_cases < 4000:
    print("Low")
    
elif total_cases < 10000:
74/2:
total_cases = 9123

if total_cases < 4000:
    print("Low")
    
elif total_cases < 10000:
    print("Medium")
    
else:
    print("High")
74/3:
total_cases = 1123

if total_cases < 4000:
    print("Low")
    
elif total_cases < 10000:
    print("Medium")
    
else:
    print("High")
74/4:
total_cases = 5123

if total_cases < 4000:
    print("Low")
    
elif total_cases < 10000:
    print("Medium")
    
else:
    print("High")
74/5:
total_cases = 

if total_cases < 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/6:
total_cases = 7125

if total_cases < 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/7:
total_cases = 10000

if total_cases < 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/8:
total_cases = 10001

if total_cases < 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/9:
total_cases = 10001

if total_cases <= 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/10:
total_cases = 4000

if total_cases <= 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/11:
total_cases = 1125

if total_cases <= 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
74/12:
def cases_segment(total_cases):
    if total_cases <= 4000:
        print("Low")
    
    elif total_cases <= 10000:
        print("Medium")

    else:
        print("High")
74/13: cases_segment(1223)
74/14:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

for index,state in enumerate(states):
74/15:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

for index,state in enumerate(states):
    segment = cases_segment(total_cases[index])
    print("State  {} has cases {}" .format(state, segment))
74/16:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/17:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    segment = cases_segment(total_cases[i])
74/18:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(state, segment))
74/19:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(state[i], segment))
74/20:
states = ["West Bengal","AP","Maharastra","Karnataka"]
total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(states[i], segment))
74/21:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(states[i], segment))
74/22:
def cases_segment(total_cases):
    if total_cases <= 10000:
        print("Medium")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases <= 4000:
        print("Low")

    else:
        print("High")
74/23: cases_segment(1223)
74/24:
def cases_segment(total_cases):
    if total_cases <= 4000:
        print("Low")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases <= 10000:
        print("Medium")

    else:
        print("High")
74/25: cases_segment(1223)
74/26:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(states[i], segment))
74/27:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print(total_cases[i])
    segment = cases_segment(total_cases[i])
    print("State  {} has cases {}" .format(states[i], segment))
74/28:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    
    print("State  {} has cases {}" .format(states[i], segment))
74/29:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    
    print("State  {} has cases {}" .format(states[i], segment))
74/30:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    
    print("State  {} has {} cases " .format(states[i], segment))
74/31:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/32:
def cases_segment(total_cases):
    if total_cases <= 4000:
        print("Low")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases <= 10000:
        print("Medium")

    else:
        print("High")
74/33: cases_segment(1223)
74/34:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    
    print("State  {} has {} cases " .format(states[i], segment))
74/35:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/36:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    print(segment)
    
    print("State  {} has {} cases " .format(states[i], segment))
74/37:
def cases_segment(total_cases):
    if total_cases > 10000:
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= 4000:
        print("Medium")

    else:
        print("Low")
74/38: cases_segment(1223)
74/39:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    
    print(total_cases[i])
    
    segment = cases_segment(total_cases[i])
    
    print(segment)
    
    print("State  {} has {} cases " .format(states[i], segment))
74/40:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/41:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i], segment))
74/42:
def cases_segment(total_cases):
    
    if total_cases > 10000:
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= 4000:
        
        print("Medium")

    else:
        print("Low")
74/43: cases_segment(1223)
74/44: cases_segment(7223)
74/45: cases_segment(17223)
74/46:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
74/47:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/48:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment =cases_segment(total_cases[i])
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
74/49:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/50:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 5100,4525,2500]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
74/51:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 1500,7000,9000]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
74/52:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
74/53:
def cases_segment(total_cases):
    
    if total_cases > 10000:
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= 4000:
        
        print("Medium")

    else:
        print("Low")
74/54: cases_segment(17223)
74/55:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 1500,7000,9000]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
74/56:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 1500,7000,9000]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
    
    print("")
76/1:
total_cases = 1125

if total_cases <= 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
76/2:
def cases_segment(total_cases):
    
    if total_cases > 10000:
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= 4000:
        
        print("Medium")

    else:
        print("Low")
76/3: cases_segment(17223)
76/4:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 1500,7000,9000]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
    
    print("")
76/5:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
76/6: import pandas as pd
76/7: pwd
76/8: data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")
76/9:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.head()
76/10:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample()
76/11:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/12:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/13:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/14:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/15:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/16:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/17:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/18:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/19:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/20:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/21:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/22:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/23:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/24:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/25:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/26:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/27:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/28:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/29:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/30:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10)
76/31:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/32:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/33:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/34:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/35:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/36:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/37:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/38:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/39:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/40:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
76/41: data_covid_cases.describe()
76/42: data_covid_cases.describe().round(2)
76/43: data_covid_cases['Total Cases'].mean()
76/44: data_covid_cases['Total Cases'].median()
76/45: data_covid_cases['Total Cases','Deaths'].median()
76/46: data_covid_cases[['Total Cases','Deaths']].median()
76/47: data_covid_cases[['Total Cases','Deaths']].groupby['Total Cases'].count()
76/48: data_covid_cases['Total Cases'].groupby['Total Cases'].count()
76/49: data_covid_cases['Total Cases','Deaths'].groupby('Total Cases').count()
76/50: data_covid_cases[['Total Cases']].median()
76/51: data_covid_cases['Total Cases','Deaths'].groupby('Total Cases').count()
76/52: data_covid_cases['Total Cases'].groupby('Total Cases').count()
76/53: data_covid_cases[['Total Cases','Active']].median()
76/54: data_covid_cases[['Total Cases','Acitve']].groupby('Total Cases').count()
76/55: data_covid_cases[['Total Cases']].groupby('Total Cases').count()
76/56: data_covid_cases[['Total Cases','State/UTs']].groupby('Total Cases').count('State/UTs')
76/57: data_covid_cases.groupby('Total Cases').count()
76/58: data_covid_cases.groupby('Total Cases').count().reset_index()
76/59: data_covid_cases.groupby['Total Cases'].value_counts()
76/60: data_covid_cases['Total Cases'].value_counts()
76/61: data_covid_cases['Active Ratio'].value_counts()
76/62: data_covid_cases['Active Ratio(%)'].value_counts()
76/63: data_covid_cases['Active Ratio (%)'].value_counts()
76/64: data_covid_cases[data_covid_cases['Active Ratio (%)'] == 0.02]
76/65:
data_covid_cases['Segment"]="NA"
data_covid_cases.head()
76/66:
data_covid_cases['Segment']="NA"
data_covid_cases.head()
76/67: data_covid_cases['Total Cases'].quantile(0.25)
76/68:
def cases_segment(total_cases):
    
    if total_cases > data_covid_cases['Total Cases'].quantile(0.75):
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= data_covid_cases['Total Cases'].quantile(0.25):
        
        print("Medium")

    else:
        print("Low")
76/69: data_covid_cases['Total Cases'].quantile(0.25)
76/70: cases_segment(17000)
76/71: cases_segment(1700000)
76/72: cases_segment(170000)
76/73: data_covid_cases['Segment'] = data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
76/74:
data_covid_cases['Segment'] = data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
data_covid_cases.head()
76/75: data_covid_cases['Segment'] = data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
76/76: data_covid_cases.head()
76/77:
data_covid_cases['Segment'] = data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
data_covid_cases.sample(5)
76/78:
def cases_segment(total_cases):
    
    if total_cases > data_covid_cases['Total Cases'].quantile(0.75):
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= data_covid_cases['Total Cases'].quantile(0.25):
        
        print("Medium")

    else:
        print("Low")
76/79: data_covid_cases['Total Cases'].quantile(0.25)
76/80: cases_segment(170000)
76/81:
data_covid_cases['Segment'] = data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
data_covid_cases.sample(5)
76/82: data_covid_cases.head()
76/83:
data_covid_cases['Segment']=data_covid_cases['Total Cases'].apply(lambda x:cases_segment(x))
data_covid_cases.sample(5)
76/84:
data_covid_cases['Segment']=data_covid_cases['Total Cases'].apply(lambda x: cases_segment(x))
data_covid_cases.sample(5)
77/1:
total_cases = 1125

if total_cases <= 4000:
    print("Low")
    
elif total_cases <= 10000:
    print("Medium")
    
else:
    print("High")
77/2:
def cases_segment(total_cases):
    
    if total_cases > 10000:
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= 4000:
        
        print("Medium")

    else:
        print("Low")
77/3: cases_segment(17223)
77/4:
states = ["West Bengal","AP","Maharastra","Karnataka"]

total_cases = [12000, 1500,7000,9000]

# for index,state in enumerate(states):
#     segment = cases_segment(total_cases[index])
#     print("State  {} has cases {}" .format(state, segment))


for i in range(0, len(states)):
    
    print(i)
    print("")
    
    print(total_cases[i])
    print("")
    
    segment = cases_segment(total_cases[i])
    
    
    print("")
    
   
    
    print("State  {} has {} cases " .format(states[i],segment))
    
    print("")
77/5:

for index,state in enumerate(states):
    
    print(index)
    
    print(state)
77/6: import pandas as pd
77/7: pwd
77/8:
data_covid_cases = pd.read_csv("Latest Covid-19 India Status.csv")

data_covid_cases.sample(10, random_state=7)
77/9: data_covid_cases.describe().round(2)
77/10: data_covid_cases['Total Cases'].mean()
77/11: data_covid_cases['Total Cases'].median()
77/12: data_covid_cases[['Total Cases','Active']].median()
77/13: data_covid_cases.groupby('Total Cases').count().reset_index()
77/14: data_covid_cases['Active Ratio (%)'].value_counts()
77/15: data_covid_cases[data_covid_cases['Active Ratio (%)'] == 0.02]
77/16:
data_covid_cases['Segment']="NA"
data_covid_cases.head()
77/17:
def cases_segment(total_cases):
    
    if total_cases > data_covid_cases['Total Cases'].quantile(0.75):
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= data_covid_cases['Total Cases'].quantile(0.25):
        
        print("Medium")

    else:
        print("Low")
77/18: data_covid_cases['Total Cases'].quantile(0.25)
77/19: cases_segment(170000)
77/20:
data_covid_cases['Segment']=data_covid_cases['Total Cases'].apply(lambda x: cases_segment(x))
data_covid_cases.sample(5)
77/21:
data_covid_cases['Segment']= ""
data_covid_cases.head()
77/22:
def cases_segment(total_cases):
    
    if total_cases > data_covid_cases['Total Cases'].quantile(0.75):
        print("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= data_covid_cases['Total Cases'].quantile(0.25):
        
        print("Medium")

    else:
        print("Low")
77/23: data_covid_cases['Total Cases'].quantile(0.25)
77/24: cases_segment(170000)
77/25:
data_covid_cases['Segment']=data_covid_cases['Total Cases'].apply(lambda x: cases_segment(x))
data_covid_cases.sample(5)
77/26: data_covid_cases.head()
77/27:
def cases_segment(total_cases):
    
    if total_cases > data_covid_cases['Total Cases'].quantile(0.75):
        return("High")
        
#         print("Return is high")
#         print("Return is high")
#         print("Return is high")
    
    elif total_cases >= data_covid_cases['Total Cases'].quantile(0.25):
        
        return("Medium")

    else:
        return("Low")
77/28: data_covid_cases['Total Cases'].quantile(0.25)
77/29: cases_segment(170000)
77/30:
data_covid_cases['Segment']=data_covid_cases['Total Cases'].apply(lambda x: cases_segment(x))
data_covid_cases.sample(5)
77/31: data_covid_cases.head()
78/1:
import pandas as pd
import seaborn

data = seaborn.load_dataset("Iris")
78/2:
import pandas as pd
import seaborn

data = seaborn.load_dataset("iris")
78/3: type(data)
78/4: data.head()
78/5: data.tail()
78/6: data.shape
78/7: data.sample(5)
78/8: print(data[10:21])
78/9: specific_data = date["petal_width","species"]
78/10: specific_data = data["petal_width","species"]
78/11: specific_data = data[["petal_width","species"]]
78/12: print(specific_data.head())
78/13: data.loc[data[sepcies]=="setosa"]
78/14: data.loc[data[species]=="setosa"]
78/15: print(data[10:21])
78/16: specific_data = data[["petal_width","species"]]
78/17: print(specific_data.head())
78/18: data.loc[data[species]=="setosa"]
78/19: [data[species]=="setosa"]
78/20: [data["species"]=="setosa"]
78/21: [data["species"]=="setosa", "sepal_length"]
78/22: data.loc[data["species"]=="setosa", "sepal_length"]
78/23: data.loc[data["species"]=="setosa", "sepal_length"].sum()
78/24: data.loc[data["species"]=="setosa", "sepal_length"]#.sum()
78/25: data.loc[data["species"]=="setosa", "sepal_length"].sum()
81/1:
import pandas as pd
import numpy as np
81/2:
# Reading datasets by using read_csv from pandas package
ratings = pd.read_csv("ratings.csv")
movie = pd.read_csv("movie.csv")
user = pd.read_csv("user.csv")
81/3: ratings.head(5)
81/4: movie.head(5)
81/5: user.head(5)
81/6:
# ratings
ratings.shape
81/7:
# user
user.shape
81/8:
# movie
movie.shape
81/9:
# ratings
# We use dataframe.dtypes to get the data types of each column
ratings.dtypes
81/10:
# user
user.dtypes
81/11:
# movie
movie.dtypes
81/12:
# ratings
ratings.describe()
81/13:
# ratings
ratings.describe().round(1)
81/14:
# ratings
ratings.describe().round(2)
81/15:
# user
user.describe()
81/16:
# movie
movie.describe()
81/17:
# Getting all the column names
movie.columns
81/18:
# Taking all the genre columns and finding the sum for every column
movie[[ 'Action',
       'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary',
       'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
       'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']].sum()
81/19:
# Alternatively, we can also loc function
movie.loc[:,'Action':'Western'].sum()
81/20:
# Alternatively, we can also loc function
movie.loc[1:7,'Action':'Western'].sum()
81/21:
# Alternatively, we can also loc function
movie.loc[:,'Action':'Western'].sum()
81/22:
# Sorting the movies across genres
number = movie.loc[:,'Action':'Western'].sum()
number.sort_values(ascending = False)
81/23:
# Checking column names
movie.columns
81/24:
# we create a new dataframe using two columns of the movie dataframe
new_movie = movie[['movie id', 'movie title']].copy()
81/25: new_movie["Number of Genres"] = movie.loc[:, 'Action':'Western'].sum(axis=1)
81/26:
# Filtering movies that have more than 1 genres
new_movie[new_movie['Number of Genres']>1]
81/27: new_movie['Number of Genres']>1
82/1:
import pandas as pd
import numpy as np
82/2:
# Reading datasets by using read_csv from pandas package
ratings = pd.read_csv("ratings.csv")
movie = pd.read_csv("movie.csv")
user = pd.read_csv("user.csv")
82/3: ratings.head(5)
82/4: movie.head(5)
82/5: user.head(5)
82/6:
# ratings
ratings.shape
82/7:
# user
user.shape
82/8:
# movie
movie.shape
82/9:
# ratings
# We use dataframe.dtypes to get the data types of each column
ratings.dtypes
82/10:
# user
user.dtypes
82/11:
# movie
movie.dtypes
82/12:
# ratings
ratings.describe().round(2)
82/13:
# user
user.describe()
82/14:
# movie
movie.describe()
82/15:
# Getting all the column names
movie.columns
82/16:
# Taking all the genre columns and finding the sum for every column
movie[[ 'Action',
       'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary',
       'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
       'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']].sum()
82/17:
# Alternatively, we can also loc function
movie.loc[:,'Action':'Western'].sum()
82/18:
# Sorting the movies across genres
number = movie.loc[:,'Action':'Western'].sum()
number.sort_values(ascending = False)
82/19:
# Checking column names
movie.columns
82/20:
# we create a new dataframe using two columns of the movie dataframe
new_movie = movie[['movie id', 'movie title']].copy()
82/21: new_movie["Number of Genres"] = movie.loc[:, 'Action':'Western'].sum(axis=1)
82/22:
# Filtering movies that have more than 1 genres
new_movie[new_movie['Number of Genres']>1]
82/23: new_movie['Number of Genres']>1
82/24:
# Merging ratings dataset with movie dataset
df_merge = movie.merge(ratings, on = 'movie id', how = 'inner')
df_merge.head()
82/25:
# Merging ratings dataset with movie dataset
df_merge = movie.merge(ratings, on = 'movie id', how = 'inner')
df_merge.head()
82/26:
# Checking the dimensions of the merged dataframe
df_merge.shape
82/27:
# Finding the count of ratings for each movie using groupby() and count()
# reset_index() is used to shift movie title from being the dataframes (movie_counts) index to 
# being just a normal column 
movie_count = df_merge.groupby(['movie title'])['rating'].count().reset_index()
movie_count.head()
82/28:
# Extracting the movie titles that have more than 100 ratings 
movie_100 = movie_count[movie_count['rating']>100]['movie title']
movie_100.head()
82/29:
# Finding average ratings for each movie and sorting them out in descending order
# using groupby() and sort_values() on merged data frame
avg_rating = df_merge.groupby(['movie title'])['rating'].mean().sort_values(ascending=False).reset_index()
avg_rating
82/30:
# Extracting movie titles that have more than 100 ratings using movie titles in movie_100 and isin() function
# Displaying top 25 rows only
avg_rating[avg_rating['movie title'].isin(movie_100)].head(25)
82/31: avg_rating['movie title'].isin(movie_100)
82/32: avg_rating['movie title']
82/33: avg_rating['movie title'].isin(movie_100)
82/34:
# Merging user dataset with movie and ratings(already merged : df_merge) dataset
df_merge_all = df_merge.merge(user, on = 'user id', how = 'inner')
82/35:
# Group by occupation and aggregate with mean
df_merge_all.groupby('occupation').rating.mean()
82/36: df_merge_all.groupby('gender').rating.mean()
82/37: df_merge_all.groupby(['occupation','gender']).rating.mean()
82/38:
cols=df_merge_all.loc[:,'Action':'Western'].columns

for i in cols:
    print(i,':' , df_merge_all[df_merge_all[i]==1].rating.mean())
82/39:
#create a new column with name age_group
df_merge_all['age_group']=pd.cut(df_merge_all.age, bins=(0,20,40,55,100),labels=('Teenager','Adult','Middle Age','Elderly'))
82/40:
#print top 5 entries of the age and age_group columns
df_merge_all[['age','age_group']].head()
82/41:
#print last 5 entries of the age and age_group columns
df_merge_all[['age','age_group']].tail()
82/42: df_merge_all.groupby(['age_group']).rating.mean()
82/43: df_merge_all.groupby(['age_group']).rating.agg(['mean','median','std'])
82/44: df_merge_all.groupby('occupation').sum().loc[:,'Action':'Western'].loc['engineer']
82/45: df_merge_all.groupby('occupation')#.sum().loc[:,'Action':'Western'].loc['engineer']
82/46: df_merge_all.groupby('occupation').sum()#.loc[:,'Action':'Western'].loc['engineer']
82/47: df_merge_all.groupby('occupation').sum().loc[:,'Action':'Western']#.loc['engineer']
82/48: df_merge_all.groupby('occupation').sum()#.loc[:,'Action':'Western'].loc['engineer']
82/49: df_merge_all.groupby('occupation').sum().loc[:,'Action':'Western']#.loc['engineer']
82/50: df_merge_all.groupby('occupation').sum().loc[:,'Action':'Western'].loc['engineer']
83/1: l1 = [2.0001, 3.908, 4.56]
83/2: sum(l1)
83/3:
l1 = [1,2,3]
l2 = [4,5,6]
83/4: l1 + l2
83/5: l1 = ['learning', "Python", 'is fun?', True]
83/6: l1.delete("Python")
83/7: l1.remove("Python")
83/8: print(l1)
83/9: l1 = ['learning', "Python", 'is fun?', True]|
83/10: l1 = ['learning', "Python", 'is fun?', True]
83/11: l1.append("hello")
83/12: print(l1)
83/13:
l1 = [9, 7, 8, 1, 5]
l1.sort(reverse = True)
l1
83/14:
f = lambda x,y,z: x*y/z*z
f(1, 5, 78)
83/15:
squares = [x**2 for x in range(10)]
print(squares)
83/16:
set = [i for i in range(1,101) if i%2 == 0] 
print(set)
83/17:
sentence = 'great learning is a great platform to learn about data science as it has great faculty and content.'
count = [len(i) for i in sentence.split()]
print(count)
83/18: np.ones(2)
83/19:
import numpy as np 
np.ones(2)
83/20:
import numpy as np 
np.identity(2)
83/21: np.array([[9,8,7],[6,5,4],[3,2,1]])
83/22: np.arange(start = 9, stop = 0, step = -1).reshape(3,3)
83/23:
d1 = np.array([[9,8,7],[6,5,4],[3,2,1]])
d1.add(1)
83/24:
d1 = np.array([[9,8,7],[6,5,4],[3,2,1]])
add (d1, 1)
83/25:
d1 = np.array([[9,8,7],[6,5,4],[3,2,1]])
d1 + 1
83/26:
n6 = np.matrix([[ 1, 4, 9, 121, 144, 169], 
[ 16, 25, 36, 196, 225, 256], [ 49, 64, 81, 289, 324, 361]])
n6.mean()
83/27:
n1 = np.matrix([[121, 144, 169], 
[196, 225, 256], [ 289, 324, 361]])

n2 = np.matrix([[ 1, 4, 9], 
[ 1, 5, 4], [ 9, 4, 8]])
83/28: prod(n1,n2)
83/29: n1*n2
83/30: np.arange(20)[0::9]
83/31:
p = np.arange(20)
p[::-1]
83/32: np.matrix([[ 1, 4, 9], [ 1, 5, 4], [ 9, 4, 8]])[::-1]
83/33:
import pandas as pd
import numpy as np
83/34:
adult_data = pd.read_csv("dult_data.csv")
adult_test = pd.read_csv("adult_test.csv")
83/35:
adult_data = pd.read_csv("adult_data.csv")
adult_test = pd.read_csv("adult_test.csv")
83/36: adult_data.head()
83/37: adult_data["education"].nunique()
83/38: adult_data["education"]
83/39: adult_data["education"].sum()
83/40: adult_data["education"].count()
83/41: adult_data["education"].value_counts()
83/42: adult_data["hours-per-week"].mean()
83/43:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
83/44: adult_data.describe()
83/45: adult_data.describe().round(2)
83/46:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary ',y='count',hue='sex')
plt.show()
83/47:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary',y='count',hue='sex')
plt.show()
83/48:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary',y='sex',hue='sex')
plt.show()
83/49:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary',hue='sex')
plt.show()
83/50: adult_data.columns
83/51:
df1 = pd.concat([adult_data,adult_test],axis=1,sort=True)
df1
83/52: df1["hours-per-week"].mean()
83/53: df1.shape
83/54:
df1 = pd.concat([adult_data,adult_test],axis=0,sort=True)
df1
83/55: df1["hours-per-week"].mean()
83/56: df1.shape
83/57:
""""""Which country ("native-country") has the highest mean value of 'hours-per-week'
for the dataset "adult_data.csv"?""""""
83/58:
# Which country ("native-country") has the 
# highest mean value of "hours-per-week"
# for the dataset "adult_data.csv"?
83/59: adult_data.groupby("native-country").hours-per-week.mean()
83/60: df_merge_all.groupby(['age_group']).rating.agg(['mean','median','std'])
83/61: adult_data.groupby(["native-country"]).hours-per-week.agg(['mean','median','std'])
83/62: adult_data.groupby(["native-country"]).["hours-per-week"].mean()
83/63: adult_data.groupby(["native-country"]).("hours-per-week").mean()
83/64: adult_data["capital-gain"].mean()
88/1: sns.histplot("age ");
88/2:
import pandas as pd
import numpy as np
88/3:
adult_data = pd.read_csv("adult_data.csv")
adult_test = pd.read_csv("adult_test.csv")
88/4: adult_data.head()
88/5: adult_data["education"].nunique()
88/6: adult_data["education"].value_counts()
88/7: adult_data["hours-per-week"].mean()
88/8:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
88/9: adult_data.describe().round(2)
88/10:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary',hue='sex')
plt.show()
88/11: adult_data.columns
88/12:
df1 = pd.concat([adult_data,adult_test],axis=0,sort=True)
df1
88/13: df1["hours-per-week"].mean()
88/14: df1.shape
88/15:
# Which country ("native-country") has the 
# highest mean value of "hours-per-week"
# for the dataset "adult_data.csv"?
88/16: adult_data.groupby(["native-country"]).("hours-per-week").mean()
88/17: adult_data["capital-gain"].mean()
88/18: sns.histplot("age ");
88/19: sns.distplot("age ");
88/20: adult_data["age"].value_count()
88/21: adult_data["age"].value_counts()
88/22: sns.distplot("age","count")
88/23: sns.displot(penguins, x="age", binwidth=3)
88/24: sns.displot(adult_data, x="age", binwidth=3)
88/25: sns.displot(adult_data, x="age", binwidth=3,kind="kde")
88/26: sns.displot(adult_data, x="age",kind="kde")
88/27: sns.displot(adult_data, x="age")#,kind="kde")
88/28: adult_data[adult_data['capital-loss']>0].value_counts()
88/29: adult_data[adult_data['capital-loss']>0].value_counts().sum()
88/30: count= adult_dat["sex"].value_counts
88/31: count= adult_data["sex"].value_counts
88/32:
count= adult_data["sex"].value_counts
print(count)
88/33: ax = sns.barplot(x="salary", y="", hue="sex", data=adult_data)
88/34: ax = sns.barplot(x="salary", y="count", hue="sex", data=adult_data)
88/35: native =adult_data.groupby('native-country')
88/36: print(native)
88/37: hours=native.groupny('hours-per-week')
88/38: hours=native.groupby('hours-per-week')
88/39: native =adult_data.groupby(["native-country","hours-per-week" ])
88/40: print(native)
88/41: native.describe()
88/42: native.describe().mean()
88/43: adult_data.head()
88/44: adult_data.groupby('occupation').hours-per-week.mean()
88/45: adult_data.groupby('occupation').["hours-per-week'].mean()
88/46: adult_data['occupation'].apply(hours-per-week)
88/47: adult_data['occupation'].apply('hours-per-week')
96/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
96/2: honeyprod = pd.read_csv("honeyproduction1998-2016.csv")
96/3: honeyprod.head(10)
96/4: honeyprod.shape
96/5: honeyprod.dtypes
96/6:
honeyprod.year = honeyprod.year.astype('category') # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
96/7: honeyprod.describe()
96/8: honeyprod.describe().round(2)
96/9: sns.pairplot(honeyprod, diag_kind="kde")
96/10:
correlation = honeyprod.corr() # creating a 2-D Matrix with correlation plots
correlation
96/11:
# Uncomment the following code for information of the arguments
# help(sns.heatmap)
plt.figure(figsize=(15, 7))
sns.heatmap(correlation, annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()
96/12:
print(honeyprod.state.nunique())
print(honeyprod.year.nunique())
96/13:
plt.figure(figsize=(15, 7))
sns.pointplot(x='year', y='totalprod', data=honeyprod, estimator=sum, ci=None)
plt.xticks(rotation=90) # To rotate the x axis labls
plt.show()

# Uncomment the following code to check the actual values
# honeyprod.groupby(['year'])['totalprod'].sum().reset_index()
96/14:
plt.figure(figsize=(15, 7))
sns.pointplot(x='year', y='numcol', data=honeyprod, ci=None, estimator=sum)
plt.xticks(rotation=90) # To rotate the x axis labls
plt.show()
96/15:
plt.figure(figsize=(15, 7))
sns.pointplot(x='year', y='yieldpercol', data=honeyprod, estimator=sum, ci=None)
plt.xticks(rotation=90) # To rotate the x axis labls
plt.show()
96/16:
# Add hue parameter to the pointplot to plot for each state
plt.figure(figsize=(15, 7)) # To resize the plot
sns.pointplot(x='year', y='totalprod', data=honeyprod, estimator=sum, ci=None, hue = 'state')
plt.legend(bbox_to_anchor=(1, 1))
plt.xticks(rotation=90) # To rotate the x axis labls
plt.show()
96/17:
sns.catplot(x='year', y='totalprod', data=honeyprod,
                estimator=sum, col='state', kind="point",
                height=3,col_wrap = 5)
plt.show()
96/18:
# Uncomment the following code to look at the top 5 honey producing states in the US 
# honeyprod.groupby(['state'])['totalprod'].mean().sort_values(ascending = False).reset_index().head()
96/19:
# Uncomment the following code to look at the top 5 honey producing states in the US 
# honeyprod.groupby(['state'])['totalprod'].mean().sort_values(ascending = False).reset_index().head()
96/20:
cplot1=sns.catplot(x='year', y='numcol', 
            data=honeyprod[honeyprod["state"].isin(["North Dakota","California","South Dakota","Florida","Montana"])],
                estimator=sum, col='state', kind="point",
                height=3,col_wrap = 5)
cplot1.set_xticklabels(rotation=90)
plt.show()
96/21:
cplot2=sns.catplot(x='year', y='yieldpercol', 
            data=honeyprod[honeyprod["state"].isin(["North Dakota","California","South Dakota","Florida","Montana"])],
                estimator=sum, col='state', kind="point",
                height=3,col_wrap = 5)
cplot2.set_xticklabels(rotation=90)
plt.show()
96/22:
sns.pointplot(x="year", y="prodvalue", data=honeyprod, ci=None)
plt.xticks(rotation=90) # To rotate the x axis labls
plt.show()
96/23:
plt.figure(figsize = (20,20)) # To resize the plot

# Plot total production per state
sns.barplot(x="totalprod", y="state", data=honeyprod.sort_values("totalprod", ascending=False),
            label="Total Production", color="b", ci=None)

# Plot stocks per state
sns.barplot(x="stocks", y="state", data=honeyprod.sort_values("totalprod", ascending=False),
            label="Stocks", color="r", ci=None)

# Add a legend
plt.legend(ncol=2, loc="lower right", frameon=True)
plt.show()
96/24:
plt.figure(figsize=(15, 7))
sns.histplot(honeyprod.priceperlb)
plt.show()
96/25:
sns.boxplot(data = honeyprod, x = 'priceperlb')
plt.show()
96/26:
plt.figure(figsize=(15, 7)) # To resize the plot
sns.barplot(data = honeyprod, x = "state", y = "priceperlb", ci=None, color = "coral",
            order=honeyprod.groupby('state').priceperlb.mean().sort_values(ascending = False).index)
plt.xticks(rotation=90) # To rotate the x axis lables
plt.show()
96/27: honeyprod.groupby(['state'])['totalprod'].mean().sort_values(ascending = False).reset_index().head()
97/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
97/2: cardiofit = pd.read_csv("CardioGoodFitness.csv")
97/3: cardiofit.head()
97/4: cardiofit.shape
97/5: cardiofit.dtypes
97/6: cardiofit.info()
97/7: cardiofit.describe().round(2)
97/8: data.isnull().sum()
97/9: cardiofit.isnull().sum()
97/10:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(cardiofit, bins=50)
97/11:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(cardiofit, bins=10)
97/12:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(cardiofit, bins=2)
97/13:
ns.distplot(cardiofit) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
97/14:
sns.distplot(cardiofit) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
97/15: goodfit=cardiofit.copy()
97/16: goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
97/17: data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
97/18:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=50)
97/19:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
97/20:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=2)
97/21:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=10)
97/22:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=20)
97/23:
data= goodfit.groupby([['Age','Education']])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
97/24:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
97/25:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=10)
97/26:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=5)
97/27:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=2)
97/28:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=5)
97/29:
data.groupby(by=['Age'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/30:
cardiofit.groupby(by=['Age'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/31:
cardiofit.groupby(by=['Age'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='Gender',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/32:
cardiofit.groupby(by=['Age'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='Age',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/33:
cardiofit.groupby(by=['Income'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='Income',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/34:
cardiofit.groupby(by=['Income'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='Income',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/35:
cardiofit.groupby(by=['Income'])['Product'].sum().reset_index().sort_values(['Product']).tail(10).plot(x='Income',
                                                                                                           y='Product',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/36:
cardiofit.groupby(by=['Product'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='Product',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/37:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/38:
cardiofit.groupby(by=['Product'])['Age'].sum().reset_index().sort_values(['Age']).tail(10).plot(x='Product',
                                                                                                           y='Age',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
97/39:
plt.figure(figsize=(20,10)) # makes the plot wider
plt.hist(cardiofit, color='g') # plots a simple histogram
plt.axvline(cardiofit.mean(), color='m', linewidth=1)
plt.axvline(cardiofit.median(), color='b', linestyle='dashed', linewidth=1)
plt.axvline(cardiofit.mode()[0], color='w', linestyle='dashed', linewidth=1)
97/40:
data2= goodfit.groupby(['Income'])['Product'].mean().sort_values(ascending = False).reset_index().head()
data
97/41:
data2= goodfit.groupby(['Income'])['Fitness'].mean().sort_values(ascending = False).reset_index().head()
data
97/42:
data2= goodfit.groupby(['Income'])['Fitness'].mean().sort_values(ascending = False).reset_index().head()
data2
97/43:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
97/44:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
97/45: sns.pairplot(data)
97/46: sns.pairplot(data2)
97/47: sns.pairplot(data3)
97/48: sns.pairplot(cardiofit)
97/49: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
97/50: cardiofit.corr()
97/51: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
97/52:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Age', y='Income', data=data, palette='muted')  # barplot
97/53:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=data, palette='muted')  # barplot
97/54:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Age', y='Income', data=data, palette='muted')  # barplot
97/55:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
97/56:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Fitness', data=data2 palette='muted')  # barplot
97/57:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Fitness', data=data2, palette='muted')  # barplot
97/58:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
97/59:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
97/60:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
97/61:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
103/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
103/2: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
103/3: cardiofit.head() #To get the top 5 data printed
103/4: cardiofit.shape
103/5: cardiofit.dtypes
103/6: cardiofit.info()
103/7: cardiofit.isnull().sum()
103/8: goodfit=cardiofit.copy()
103/9: cardiofit.describe().round(2)
103/10:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
103/11:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
103/12:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
103/13:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
103/14:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
103/15:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
103/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
103/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
103/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
103/19: sns.pairplot(data)
103/20: sns.pairplot(data2)
103/21: sns.pairplot(data3)
103/22: sns.pairplot(cardiofit)
103/23: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
103/24: cardiofit.corr()
103/25: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
107/1: import pandas as pd
107/2:
df = pd.read_csv("salaries.csv")
df.head()
107/3: inputs = df.drop('salary_more_then_100k',axis='columns')
107/4: target = df['salary_more_then_100k']
107/5:
from sklearn.preprocessing import LabelEncoder
le_company = LabelEncoder()
le_job = LabelEncoder()
le_degree = LabelEncoder()
107/6:
inputs['company_n'] = le_company.fit_transform(inputs['company'])
inputs['job_n'] = le_job.fit_transform(inputs['job'])
inputs['degree_n'] = le_degree.fit_transform(inputs['degree'])
107/7: inputs
107/8: inputs_n = inputs.drop(['company','job','degree'],axis='columns')
107/9: inputs_n
107/10: target
107/11:
from sklearn import tree
model = tree.DecisionTreeClassifier()
107/12: model.fit(inputs_n, target)
107/13: model.score(inputs_n,target)
107/14: model.predict([[2,1,0]])
107/15: model.predict([[2,1,1]])
109/1:
inputs = df.drop('Survived',axis='columns')
target = df.Survived
target
109/2: import pandas as pd
109/3:
df = pd.read_csv("titanic.csv")
df.head()
109/4: df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
109/5: df.head()
109/6:
inputs = df.drop('Survived',axis='columns')
target = df.Survived
target
109/7:
inputs.Age = inputs.Age.fillna(inputs.Age.mean())
inputs.mean()
109/8:
inputs.Age = inputs.Age.fillna(inputs.Age.mean())
inputs.Age
109/9: inputs.head()
109/10: from sklearn.model_selection import train_test_split
109/11: X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
109/12: len(X_train)
109/13: len(X_test)
109/14:
from sklearn import tree
model = tree.DecisionTreeClassifier()
109/15: model.fit(X_train,y_train)
109/16: model.score(X_test,y_test)
109/17: inputs.Age = inputs.Age.fillna(inputs.Age.mean())
109/18: inputs.head()
109/19: from sklearn.model_selection import train_test_split
109/20: X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
109/21: len(X_train)
109/22: len(X_test)
109/23:
from sklearn import tree
model = tree.DecisionTreeClassifier()
109/24: model.fit(X_train,y_train)
109/25: model.fit(inputs,target)
111/1:
miles_sum_m = cardiofit['Miles'].groupby([data['Product'],data['Gender']]).sum()  # number of miles by country and gender
miles_sum_m = suic_sum_m.reset_index().sort_values(by='Miles',ascending=False) # sort in descending order
most_cont_m = miles_sum_m.head(10)  # getting the top ten products in terms of miles

fig = plt.figure(figsize=(15,5))
plt.title('Count of suicides for 31 years.')

sns.barplot(y='Product',x='Miles',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of miles')
plt.tight_layout()
111/2:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
111/3: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
111/4: cardiofit.head() #To get the top 5 data printed
111/5: cardiofit.shape
111/6: cardiofit.dtypes
111/7: cardiofit.info()
111/8: cardiofit.isnull().sum()
111/9: goodfit=cardiofit.copy()
111/10: cardiofit.describe().round(2)
111/11:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
111/12:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
111/13:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
111/14:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
111/15:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
111/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
111/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
111/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
111/19:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
111/20: sns.pairplot(data)
111/21: sns.pairplot(data2)
111/22: sns.pairplot(data3)
111/23: sns.pairplot(cardiofit)
111/24: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
111/25: cardiofit.corr()
111/26: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
111/27:
miles_sum_m = cardiofit['Miles'].groupby([data['Product'],data['Gender']]).sum()  # number of miles by country and gender
miles_sum_m = suic_sum_m.reset_index().sort_values(by='Miles',ascending=False) # sort in descending order
most_cont_m = miles_sum_m.head(10)  # getting the top ten products in terms of miles

fig = plt.figure(figsize=(15,5))
plt.title('Count of suicides for 31 years.')

sns.barplot(y='Product',x='Miles',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of miles')
plt.tight_layout()
111/28:
miles_sum_m = cardiofit['Miles'].groupby([data['MaritalStatus'],data['Gender']]).sum()  # number of miles by MaritalStatus and gender
miles_sum_m = suic_sum_m.reset_index().sort_values(by='Miles',ascending=False) # sort in descending order
most_cont_m = miles_sum_m.head(10)  # getting the top  MaritalStatus in terms of miles

fig = plt.figure(figsize=(15,5))
plt.title('Count of suicides for 31 years.')

sns.barplot(y='MaritalStatus',x='Miles',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of miles')
plt.tight_layout()
111/29:
miles_sum_m = cardiofit['Miles'].groupby([data['Age'],data['Gender']]).sum()  # number of miles by Age and gender
miles_sum_m = suic_sum_m.reset_index().sort_values(by='Miles',ascending=False) # sort in descending order
most_cont_m = miles_sum_m.head(10)  # getting the top  Age in terms of miles

fig = plt.figure(figsize=(15,5))
plt.title('Count of Miles for Age.')

sns.barplot(y='Age',x='Miles',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of miles')
plt.tight_layout()
111/30:
miles_sum_m = cardiofit['Miles'].groupby([data['Age'],data['Usage']]).sum()  # number of miles by Age and Usage
miles_sum_m = suic_sum_m.reset_index().sort_values(by='Miles',ascending=False) # sort in descending order
most_cont_m = miles_sum_m.head(10)  # getting the top  Age in terms of miles

fig = plt.figure(figsize=(15,5))
plt.title('Count of Miles for Age.')

sns.barplot(y='Age',x='Miles',hue='Usage',data=most_cont_m,palette='Set2');

plt.ylabel('Count of miles')
plt.tight_layout()
111/31:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = 'Gender',  data=data)
plt.show()
111/32:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = '',  data=cardiofit)
plt.show()
111/33:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = 'Gender',  data=cardiofit)
plt.show()
111/34:
cardiofit_sum_m = cardiofit['Income'].groupby([cardiofit['MaritalStatus'],cardiofit['Gender']]).sum()  # number of suicides by country and sex
cardiofit_sum_m = cardiofit_sum_m.reset_index().sort_values(by='suicides_no',ascending=False) # sort in descending order
most_cont_m = cardiofit_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of Income for people.')

sns.barplot(y='MaritalStatus',x='Income',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of Income')
plt.tight_layout()
111/35:
cardiofit_sum_m = cardiofit['Income'].groupby([cardiofit['MaritalStatus'],cardiofit['Gender']]).sum()  # number of suicides by country and sex
cardiofit_sum_m = cardiofit_sum_m.reset_index().sort_values(by='Income',ascending=False) # sort in descending order
most_cont_m = cardiofit_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of Income for people.')

sns.barplot(y='MaritalStatus',x='Income',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of Income')
plt.tight_layout()
111/36: Japan has a higher proportion of female suicides compared to the countries with overall suicide rates even more high
111/37:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=data['Income'])
plt.show()
111/38:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=data['Usage'])
plt.show()
111/39:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=data['Miles'])
plt.show()
111/40:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Miles'])
plt.show()
111/41:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Usage'])
plt.show()
111/42:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
111/43:
goodfit[['Product','Usage']].groupby(['Product']).sum().plot(figsize=(15,5))

plt.show()
111/44:
goodfit[['Product','Usage']].groupby(['Product']).plot(figsize=(15,5))

plt.show()
111/45:
goodfit[['Product','Income']].groupby(['Product']).plot(figsize=(15,5))

plt.show()
111/46:
goodfit[['Gender','Income']].groupby(['Gender']).plot(figsize=(15,5))

plt.show()
111/47:
plt.figure(figsize=(15, 7))
sns.goodfit(goodfit.priceperlb)
plt.show()
111/48:
plt.figure(figsize=(15, 7))
sns.goodfit(goodfit.Miles)
plt.show()
111/49:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
111/50:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
111/51:
Observations:

    Price per pound of honey has a right skewed distribution with a lot of outliers towards the higher end.
    The median price per pound of honey is 1.5
111/52: sns.violinplot(goodfit)
111/53:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
115/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
115/2: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
115/3: cardiofit.head() #To get the top 5 data printed
115/4: cardiofit.shape
115/5: cardiofit.dtypes
115/6: cardiofit.info()
115/7: cardiofit.isnull().sum()
115/8: goodfit=cardiofit.copy()
115/9: goodfit=cardiofit.copy() # creating a copy of the provided dataset is a good practice
115/10: cardiofit.describe().round(2)
115/11:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
115/12:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
115/13:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
115/14:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
115/15:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
115/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=goodfit, palette='muted')  # barplot
115/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
115/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
115/19:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
115/20:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = 'Gender',  data=cardiofit)
plt.show()
115/21:
cardiofit_sum_m = cardiofit['Income'].groupby([cardiofit['MaritalStatus'],cardiofit['Gender']]).sum()  # number of suicides by country and sex
cardiofit_sum_m = cardiofit_sum_m.reset_index().sort_values(by='Income',ascending=False) # sort in descending order
most_cont_m = cardiofit_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of Income for people.')

sns.barplot(y='MaritalStatus',x='Income',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of Income')
plt.tight_layout()
115/22:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Usage'])
plt.show()
115/23:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
115/24:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
115/25:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
115/26:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
115/27:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
115/28:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
115/29: sns.pairplot(data)
115/30: sns.pairplot(data2)
115/31: sns.pairplot(data3)
115/32: sns.pairplot(goodfit)
115/33: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
115/34: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
115/35: goodfit.corr()
115/36: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
117/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
117/2: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
117/3: cardiofit.head() #To get the top 5 data printed
117/4: cardiofit.shape
117/5: cardiofit.dtypes
117/6: cardiofit.info()
117/7: cardiofit.isnull().sum()
117/8: goodfit=cardiofit.copy() # creating a copy of the provided dataset is a good practice
117/9: cardiofit.describe().round(2)
117/10:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
117/11:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
117/12:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
117/13:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
117/14: ### To take some subsets of the given data [data, data2, data3,data4]that will be used later
117/15:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
117/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=goodfit, palette='muted')  # barplot
117/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
117/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
117/19:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
117/20:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Usage'])
plt.show()
117/21:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
117/22:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
117/23:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
117/24:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = 'Gender',  data=cardiofit)
plt.show()
117/25:
cardiofit_sum_m = cardiofit['Income'].groupby([cardiofit['MaritalStatus'],cardiofit['Gender']]).sum()  # number of suicides by country and sex
cardiofit_sum_m = cardiofit_sum_m.reset_index().sort_values(by='Income',ascending=False) # sort in descending order
most_cont_m = cardiofit_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of Income for people.')

sns.barplot(y='MaritalStatus',x='Income',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of Income')
plt.tight_layout()
117/26: sns.pairplot(data)
117/27: sns.pairplot(data2)
117/28: sns.pairplot(data3)
117/29: sns.pairplot(data4)
117/30: sns.pairplot(goodfit)
117/31: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
117/32: goodfit.corr()
117/33: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
118/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
118/2: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
118/3: cardiofit.head() #To get the top 5 data printed
118/4: cardiofit.shape
118/5: cardiofit.dtypes
118/6: cardiofit.info()
118/7: cardiofit.isnull().sum()
118/8: goodfit=cardiofit.copy() # creating a copy of the provided dataset is a good practice
118/9: cardiofit.describe().round(2)
118/10:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
118/11:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
118/12:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
118/13:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
118/14: ### To take some subsets of the given data [data, data2, data3,data4]that will be used later
118/15:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
118/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=goodfit, palette='muted')  # barplot
118/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
118/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
118/19:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
118/20:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Usage'])
plt.show()
118/21:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
118/22:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
118/23:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
118/24:
plt.figure(figsize=(15,5))

sns.pointplot(x="Product", y="Income", hue = 'Gender',  data=cardiofit)
plt.show()
118/25:
cardiofit_sum_m = cardiofit['Income'].groupby([cardiofit['MaritalStatus'],cardiofit['Gender']]).sum()  # number of suicides by country and sex
cardiofit_sum_m = cardiofit_sum_m.reset_index().sort_values(by='Income',ascending=False) # sort in descending order
most_cont_m = cardiofit_sum_m.head(10)  # getting the top ten countries in terms of suicides

fig = plt.figure(figsize=(15,5))
plt.title('Count of Income for people.')

sns.barplot(y='MaritalStatus',x='Income',hue='Gender',data=most_cont_m,palette='Set2');

plt.ylabel('Count of Income')
plt.tight_layout()
118/26: sns.pairplot(data)
118/27: sns.pairplot(data2)
118/28: sns.pairplot(data3)
118/29: sns.pairplot(data4)
118/30: sns.pairplot(goodfit)
118/31: sns.scatterplot(cardiofit['Age'], cardiofit['Income'])
118/32: goodfit.corr()
118/33: sns.heatmap(cardiofit.corr(), annot=True)  # plot the correlation coefficients as a heatmap
119/1: import pandas as pd
119/2:
df = pd.read_csv("titanic.csv")
df.head()
119/3: df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)
119/4: df.head()
119/5:
inputs = df.drop('Survived',axis='columns')
target = df.Survived
target
119/6: inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})
119/7: inputs.Age[:10]
119/8: inputs.Age = inputs.Age.fillna(inputs.Age.mean())
119/9: inputs.head()
119/10: from sklearn.model_selection import train_test_split
119/11: X_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.2)
119/12: len(X_train)
119/13: len(X_test)
119/14:
from sklearn import tree
model = tree.DecisionTreeClassifier()
119/15: model.fit(inputs,target)
119/16: model.score(X_test,y_test)
121/1:
import pandas as pd
from sklearn.datasets import load_digits
digits = load_digits()
121/2: dir(digits)
121/3:
%matplotlib inline
import matplotlib.pyplot as plt
121/4:
plt.gray() 
for i in range(4):
    plt.matshow(digits.images[i])
121/5:
df = pd.DataFrame(digits.data)
df.head()
121/6: digits.data[:5]
121/7: df['target'] = digits.target
121/8: df[0:12]
121/9:
X = df.drop('target',axis='columns')
y = df.target
121/10:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
121/11:
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=30)
model.fit(X_train, y_train)
121/12: model.score(X_test, y_test)
121/13: y_predicted = model.predict(X_test)
121/14:
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_predicted)
cm
121/15:
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')
123/1:
from sklearn.datasets import load_iris
iris = load_iris()
dir(iris)
123/2:
import pandas as pd
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df.head()
123/3:
df['target'] = iris.target
df.head()
123/4:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2)
123/5:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['target'],axis='columns'),iris.target,test_size=0.2)
123/6:
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)
123/7: model.score(X_test,y_test)
123/8:
model = RandomForestClassifier(n_estimators=40)
model.fit(X_train, y_train)
model.score(X_test,y_test)
123/9: df.drop(['target'],axis='columns'
123/10:
df.drop(['target'],axis='columns')
df
124/1: from sklearn.datasets import load_iris
124/2: from sklearn import tree
124/3: iris = load_iris()
124/4: X, y = iris.data, iris.target
124/5: clf = tree.DecisionTreeClassifier()
124/6: clf = clf.fit(X, y)
124/7: tree.plot_tree(clf)
124/8: import graphviz
124/9: from sklearn.datasets import load_iris
124/10: from sklearn.tree import DecisionTreeClassifier
124/11: from sklearn.tree import export_text
124/12: iris = load_iris()
124/13: decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
124/14: decision_tree = decision_tree.fit(iris.data, iris.target)
124/15: r = export_text(decision_tree, feature_names=iris['feature_names'])
124/16: print(r)
124/17: from sklearn import tree
124/18: X = [[0, 0], [2, 2]]
124/19: y = [0.5, 2.5]
124/20: clf = tree.DecisionTreeRegressor()
124/21: clf = clf.fit(X, y)
124/22: clf.predict([[1, 1]])
124/23:
print(__doc__)

# Import the necessary modules and libraries
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# Create a random dataset
rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

# Fit regression model
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
regr_1.fit(X, y)
regr_2.fit(X, y)

# Predict
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)

# Plot the results
plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",
            c="darkorange", label="data")
plt.plot(X_test, y_1, color="cornflowerblue",
         label="max_depth=2", linewidth=2)
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()
124/24: from sklearn.ensemble import RandomForestClassifier
124/25: from sklearn.datasets import make_classification
124/26:
X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
124/27: clf = RandomForestClassifier(max_depth=2, random_state=0)
124/28: clf.fit(X, y)
124/29: print(clf.predict([[0, 0, 0, 0]]))
124/30:
# Authors: Chirag Nagpal
#          Christos Aridas
print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.base import BaseEstimator, clone
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.metaestimators import if_delegate_has_method


N_SAMPLES = 5000
RANDOM_STATE = 42


class InductiveClusterer(BaseEstimator):
    def __init__(self, clusterer, classifier):
        self.clusterer = clusterer
        self.classifier = classifier

    def fit(self, X, y=None):
        self.clusterer_ = clone(self.clusterer)
        self.classifier_ = clone(self.classifier)
        y = self.clusterer_.fit_predict(X)
        self.classifier_.fit(X, y)
        return self

    @if_delegate_has_method(delegate='classifier_')
    def predict(self, X):
        return self.classifier_.predict(X)

    @if_delegate_has_method(delegate='classifier_')
    def decision_function(self, X):
        return self.classifier_.decision_function(X)


def plot_scatter(X,  color, alpha=0.5):
    return plt.scatter(X[:, 0],
                       X[:, 1],
                       c=color,
                       alpha=alpha,
                       edgecolor='k')


# Generate some training data from clustering
X, y = make_blobs(n_samples=N_SAMPLES,
                  cluster_std=[1.0, 1.0, 0.5],
                  centers=[(-5, -5), (0, 0), (5, 5)],
                  random_state=RANDOM_STATE)


# Train a clustering algorithm on the training data and get the cluster labels
clusterer = AgglomerativeClustering(n_clusters=3)
cluster_labels = clusterer.fit_predict(X)

plt.figure(figsize=(12, 4))

plt.subplot(131)
plot_scatter(X, cluster_labels)
plt.title("Ward Linkage")


# Generate new samples and plot them along with the original dataset
X_new, y_new = make_blobs(n_samples=10,
                          centers=[(-7, -1), (-2, 4), (3, 6)],
                          random_state=RANDOM_STATE)

plt.subplot(132)
plot_scatter(X, cluster_labels)
plot_scatter(X_new, 'black', 1)
plt.title("Unknown instances")


# Declare the inductive learning model that it will be used to
# predict cluster membership for unknown instances
classifier = RandomForestClassifier(random_state=RANDOM_STATE)
inductive_learner = InductiveClusterer(clusterer, classifier).fit(X)

probable_clusters = inductive_learner.predict(X_new)


plt.subplot(133)
plot_scatter(X, cluster_labels)
plot_scatter(X_new, probable_clusters)

# Plotting decision regions
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

Z = inductive_learner.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.title("Classify unknown instances")

plt.show()
126/1: from sklearn.ensemble import AdaBoostClassifier
126/2: from sklearn.datasets import make_classification
126/3:
X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
126/4: clf = AdaBoostClassifier(n_estimators=100, random_state=0)
126/5: clf.fit(X, y)
126/6: clf.predict([[0, 0, 0, 0]])
126/7: clf.score(X, y)
127/1:
#!/usr/bin/python3

""" 
    Starter code for exploring the Enron dataset (emails + finances);
    loads up the dataset (pickled dict of dicts).

    The dataset has the form:
    enron_data["LASTNAME FIRSTNAME MIDDLEINITIAL"] = { features_dict }

    {features_dict} is a dictionary of features associated with that person.
    You should explore features_dict as part of the mini-project,
    but here's an example to get you started:

    enron_data["SKILLING JEFFREY K"]["bonus"] = 5600000
    
"""

import joblib

enron_data = joblib.load(open("../final_project/final_project_dataset.pkl", "rb"))
127/2: enron_data
127/3: enron_data.describe()
127/4: enron_data.describe
127/5: enron_data.nunique()
127/6: enron_data.shape
127/7: enron_data.size
127/8: len(enron_data)
127/9: len(enron_data)
127/10: len(enron_data[enron_data.keys()[0]])
127/11: enron_data
127/12: enron_data.nunique()
127/13: len(enron_data)
127/14: len(enron_data[enron_data.keys()[0]])
127/15: len(enron_data[enron_data.keys()[0]])
127/16: data[person_name]["poi"]==1.count()
127/17: data[person_name]["poi"]==1.value_counts
127/18: len(data[data.keys()[0]])
127/19: count(enron_data[person_name]["poi"]==1
127/20: count(enron_data["person_name"]["poi"]==1
127/21: count(enron_data["person_name"]["poi"]==1)
127/22: (enron_data["person_name"]["poi"]==1).value_counts
127/23: (enron_data[person_name]["poi"]==1).value_counts
127/24: enron_data["LASTNAME FIRSTNAME"]["feature_name"]
127/25: enron_data["LASTNAME"]["feature_name"]
127/26: len(enron_data[enron_data.keys()[0]])
127/27: lenron_data[enron_data.keys()[0]]
127/28: enron_data[enron_data.keys()[0]]
127/29: data[data.keys()[0]]
127/30: len(enron_data)
127/31: enron_data.get('METTS MARK')
127/32: len(enron_data.get('METTS MARK'))
127/33: data['METTS MARK']["poi"]==1
127/34: enron_data['METTS MARK']["poi"]==1
127/35: len(enron_data['METTS MARK']["poi"]==1)
127/36: len(enron_data[person_name]["poi"]==1)
128/1: import pandas as pd
128/2: pd.read("poi_names.txt)")
128/3: pd.read("poi_names.txt")
128/4: pd.read_csv("poi_names.txt")
128/5: import numpy as np
128/6: from sklearn.linear_model import LinearRegression
128/7: X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
128/8: y = np.dot(X, np.array([1, 2])) + 3
128/9: reg = LinearRegression().fit(X, y)
128/10: reg.score(X, y)
128/11: reg.coef_
128/12: reg.intercept_
128/13: reg.predict(np.array([[3, 5]]))
135/1:
import numpy as np
from sklearn.linear_model import LinearRegression
136/1:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 0.0
weight2 = 0.0
bias = 0.0
136/2:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/3:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/4:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
136/5:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/6: print(is_correct_string)
136/7: print(outputs)
136/8:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/9:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/10:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string) #(index=False)
136/11:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/12:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=True))
136/13:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/14:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 0.3
weight2 = 0.2
bias = 0.1
136/15:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/16:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/17: print(is_correct_string)
136/18: print(outputs)
136/19:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/20:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/21:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/22:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/23:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 0.7
weight2 = 0.8
bias = 0.3
136/24:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/25:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/26: print(is_correct_string)
136/27: print(outputs)
136/28:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/29:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/30:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/31:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/32:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 2.7
weight2 = 3.8
bias = 0.8
136/33:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/34:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/35: print(is_correct_string)
136/36: print(outputs)
136/37:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/38:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/39:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/40:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/41:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 2.7
weight2 = 3.8
bias = 2.8
136/42:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/43:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/44: print(is_correct_string)
136/45: print(outputs)
136/46:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/47:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/48:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/49:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 21.7
weight2 = 33.8
bias = 23.8
136/50:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/51:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/52: print(is_correct_string)
136/53: print(outputs)
136/54:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(4, 3), (7, 6), (8, 0), (5, 4)]
correct_outputs = [False, False, False, True]
outputs = []
136/55:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/56: print(is_correct_string)
136/57: print(outputs)
136/58:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/59:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/60:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 1.7
weight2 = 3.8
bias = 0.8
136/61:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/62:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/63: print(is_correct_string)
136/64: print(outputs)
136/65:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/66:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/67:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/68:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/69:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 1.7
weight2 = 3.8
bias = -0.8
136/70:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/71:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/72: print(is_correct_string)
136/73: print(outputs)
136/74:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/75:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/76:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/77:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/78:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 2.7
weight2 = 5.8
bias = -0.9
136/79:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/80:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/81: print(is_correct_string)
136/82: print(outputs)
136/83:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/84:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/85:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 6.7
weight2 = 5.8
bias = -1.9
136/86:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/87:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/88: print(is_correct_string)
136/89: print(outputs)
136/90:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/91:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/92:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/93:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 26.7
weight2 = 15.8
bias = -1.9
136/94:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/95:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/96: print(is_correct_string)
136/97: print(outputs)
136/98:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/99:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/100:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/101:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/102:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 26.7
weight2 = 15.8
bias = -5.9
136/103:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/104:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/105: print(is_correct_string)
136/106: print(outputs)
136/107:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/108:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/109:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
136/110:
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
136/111:
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 26.7
weight2 = 15.8
bias = -5.9
136/112:
# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []
136/113:
# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])
136/114: print(is_correct_string)
136/115: print(outputs)
136/116:
# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
136/117:
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
print(num_wrong)
136/118:
output_frame = pd.DataFrame(outputs, columns=['Input 1', 'Input 2', 'Linear Combination', 'Activation Output', 'Is Correct'])
output_frame
137/1:
import numpy as np
# Setting the random seed, feel free to change it and see different solutions.
np.random.seed(42)

def stepFunction(t):
    if t >= 0:
        return 1
    return 0

def prediction(X, W, b):
    return stepFunction((np.matmul(X,W)+b)[0])

# TODO: Fill in the code below to implement the perceptron trick.
# The function should receive as inputs the data X, the labels y,
# the weights W (as an array), and the bias b,
# update the weights and bias W, b, according to the perceptron algorithm,
# and return W and b.
def perceptronStep(X, y, W, b, learn_rate = 0.01):
    # Fill in code
    
    
    
    
    return W, b
    
# This function runs the perceptron algorithm repeatedly on the dataset,
# and returns a few of the boundary lines obtained in the iterations,
# for plotting purposes.
# Feel free to play with the learning rate and the num_epochs,
# and see your results plotted below.
def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):
    x_min, x_max = min(X.T[0]), max(X.T[0])
    y_min, y_max = min(X.T[1]), max(X.T[1])
    W = np.array(np.random.rand(2,1))
    b = np.random.rand(1)[0] + x_max
    # These are the solution lines that get plotted below.
    boundary_lines = []
    for i in range(num_epochs):
        # In each epoch, we apply the perceptron step.
        W, b = perceptronStep(X, y, W, b, learn_rate)
        boundary_lines.append((-W[0]/W[1], -b/W[1]))
    return boundary_lines
137/2:
import numpy as np
# Setting the random seed, feel free to change it and see different solutions.
np.random.seed(42)

def stepFunction(t):
    if t >= 0:
        return 1
    return 0

def prediction(X, W, b):
    return stepFunction((np.matmul(X,W)+b)[0])
137/3: step function(0.1)
137/4: stepFunction(0.1)
137/5: prediction(7,0.5,0.1)
137/6: prediction(0.1,0.5,0.1)
137/7: prediction(1.0,0.5,0.1)
137/8:
for i in range(len(X)):
        y_hat = prediction(X[i],W,b)
        if y[i]-y_hat == 1:
            W[0] += X[i][0]*learn_rate
            W[1] += X[i][1]*learn_rate
            b += learn_rate
137/9:
import numpy as np
# Setting the random seed, feel free to change it and see different solutions.
np.random.seed(42)

def stepFunction(t):
    if t >= 0:
        return 1
    return 0

def prediction(X, W, b):
    return stepFunction((np.matmul(X,W)+b)[0])


def perceptronStep(X, y, W, b, learn_rate = 0.01):
    for i in range(len(X)):
        y_hat = prediction(X[i],W,b)
        if y[i]-y_hat == 1:
            W[0] += X[i][0]*learn_rate
            W[1] += X[i][1]*learn_rate
            b += learn_rate
137/10: stepFunction(0.1)
137/11: prediction(1.0,0.5,0.1)
137/12: perceptronStep((1.0,0.5, 0.7, 0.1)
137/13: perceptronStep(1.0,0.5, 0.7, 0.1)
137/14: perceptronStep(1,5,7,1)
138/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
138/2:
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
138/3:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -ylog(y_hat)-(1-y)log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for i=1 and i<n:
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/4:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for i=1 and i<n:
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/5:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for i==1 and i<n:
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/6:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for i=1:
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/7:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for i =1:
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/8:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for (i = 1):
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
138/9:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
# def update_weights(x, y, weights, bias, learnrate):
#     for (i = 1):
#         weights = weights + learnrate(y-y_hat)x
#         b = b + learnrate(y-y_hat)
138/10:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
138/11: features.shape
138/12:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
138/13:
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
138/14:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
# def update_weights(x, y, weights, bias, learnrate):
#     for (i = 1):
#         weights = weights + learnrate(y-y_hat)x
#         b = b + learnrate(y-y_hat)
138/15:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
138/16:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
138/17: features.shape
138/18: features.shape
138/19: size
138/20: ephocs
139/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
139/2:
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
139/3:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
# def update_weights(x, y, weights, bias, learnrate):
#     for (i = 1):
#         weights = weights + learnrate(y-y_hat)x
#         b = b + learnrate(y-y_hat)
139/4:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
139/5:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
139/6: ephocs
139/7: weights
139/8:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
139/9: train(X, y, epochs, learnrate, True)
140/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
140/2:
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
140/3:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

# Gradient descent step
# def update_weights(x, y, weights, bias, learnrate):
#     for (i = 1):
#         weights = weights + learnrate(y-y_hat)x
#         b = b + learnrate(y-y_hat)
140/4:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
140/5: train(X, y, epochs, learnrate, True)
140/6:
# Implement the following functions
w1= 0.0
x1= 0.0
w2= 0.0
x2= 0.0
b= 0

# Activation (sigmoid) function

def sigmoid(x):
    result = 1/(1+np.exp(-x))
    return result

# Output (prediction) formula
def output_formula(features, weights, bias):
    y_hat = sigmoid(w1x1+ w2x2+ b)
    return y_hat

# Error (log-loss) formula
def error_formula(y, output):
    error_function = -y*log(y_hat)-(1-y)*log(1-y_hat)

Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    for (i = 1):
        weights = weights + learnrate(y-y_hat)x
        b = b + learnrate(y-y_hat)
140/7:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
140/8: train(X, y, epochs, learnrate, True)
140/9:
# Implement the following functions

# Activation (sigmoid) function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def output_formula(features, weights, bias):
    return sigmoid(np.dot(features, weights) + bias)

def error_formula(y, output):
    return - y*np.log(output) - (1 - y) * np.log(1-output)

def update_weights(x, y, weights, bias, learnrate):
    output = output_formula(x, weights, bias)
    d_error = y - output
    weights += learnrate * d_error * x
    bias += learnrate * d_error
    return weights, bias
143/1:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
143/2:
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
143/3:
# Implement the following functions

# Activation (sigmoid) function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def output_formula(features, weights, bias):
    return sigmoid(np.dot(features, weights) + bias)

def error_formula(y, output):
    return - y*np.log(output) - (1 - y) * np.log(1-output)

def update_weights(x, y, weights, bias, learnrate):
    output = output_formula(x, weights, bias)
    d_error = y - output
    weights += learnrate * d_error * x
    bias += learnrate * d_error
    return weights, bias
143/4:
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            
            # Converting the output (float) to boolean as it is a binary classification
            # e.g. 0.95 --> True (= 1), 0.31 --> False (= 0)
            predictions = out > 0.5
            
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # Plotting the solution boundary
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # Plotting the data
    plot_points(features, targets)
    plt.show()

    # Plotting the error
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
143/5: train(X, y, epochs, learnrate, True)
144/1:
# Importing pandas and numpy
import pandas as pd
import numpy as np

# Reading the csv file into a pandas DataFrame
data = pd.read_csv('student_data.csv')

# Printing out the first 10 rows of our data
data.head()
144/2:
# %matplotlib inline
import matplotlib.pyplot as plt

# Function to help us plot
def plot_points(data):
    X = np.array(data[["gre","gpa"]])
    y = np.array(data["admit"])
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')
    plt.xlabel('Test (GRE)')
    plt.ylabel('Grades (GPA)')
    
# Plotting the points
plot_points(data)
plt.show()
144/3:
# %matplotlib inline
import matplotlib.pyplot as plt

# Function to help us plot
def plot_points(data):
    X = np.array(data[["gre","gpa"]])
    y = np.array(data["admit"])
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')
    #plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')
    plt.xlabel('Test (GRE)')
    plt.ylabel('Grades (GPA)')
    
# Plotting the points
plot_points(data)
plt.show()
144/4:
# %matplotlib inline
import matplotlib.pyplot as plt

# Function to help us plot
def plot_points(data):
    X = np.array(data[["gre","gpa"]])
    y = np.array(data["admit"])
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected],  s = 25, color = 'red', edgecolor = 'k') #[s[0][1] for s in rejected],
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')
    plt.xlabel('Test (GRE)')
    plt.ylabel('Grades (GPA)')
    
# Plotting the points
plot_points(data)
plt.show()
144/5:
# %matplotlib inline
import matplotlib.pyplot as plt

# Function to help us plot
def plot_points(data):
    X = np.array(data[["gre","gpa"]])
    y = np.array(data["admit"])
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')
    plt.xlabel('Test (GRE)')
    plt.ylabel('Grades (GPA)')
    
# Plotting the points
plot_points(data)
plt.show()
145/1:
# Importing pandas and numpy
import pandas as pd
import numpy as np

# Reading the csv file into a pandas DataFrame
data = pd.read_csv('student_data.csv')

# Printing out the first 10 rows of our data
data.head()
145/2:
# %matplotlib inline
import matplotlib.pyplot as plt

# Function to help us plot
def plot_points(data):
    X = np.array(data[["gre","gpa"]])
    y = np.array(data["admit"])
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')
    plt.xlabel('Test (GRE)')
    plt.ylabel('Grades (GPA)')
    
# Plotting the points
plot_points(data)
plt.show()
145/3:
# Separating the ranks
data_rank1 = data[data["rank"]==1]
data_rank2 = data[data["rank"]==2]
data_rank3 = data[data["rank"]==3]
data_rank4 = data[data["rank"]==4]

# Plotting the graphs
plot_points(data_rank1)
plt.title("Rank 1")
plt.show()
plot_points(data_rank2)
plt.title("Rank 2")
plt.show()
plot_points(data_rank3)
plt.title("Rank 3")
plt.show()
plot_points(data_rank4)
plt.title("Rank 4")
plt.show()
145/4:
a = pd.get_dummies(data['rank'])
a
145/5:
a = pd.get_dummies(data['rank'], prefix='rank')
a
145/6:
a = [data, pd.get_dummies(data['rank'], prefix='rank')]
a
145/7:
a = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)
a
145/8: one_hot_data
145/9:
# TODO:  Make dummy variables for rank and concat existing columns
one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)

# TODO: Drop the previous rank column
one_hot_data = one_hot_data.drop('rank', axis=1)

# Print the first 10 rows of our data
one_hot_data[:10]
145/10:
# TODO:  Make dummy variables for rank and concat existing columns
one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)

# TODO: Drop the previous rank column
one_hot_data = one_hot_data.drop('rank', axis=1)

# # Print the first 10 rows of our data
# one_hot_data[:10]
145/11: one_hot_data
145/12:
# TODO:  Make dummy variables for rank and concat existing columns
one_hot_data = pd.concat([data, pd.get_dummies(data['rank'], prefix='rank')], axis=1)

# TODO: Drop the previous rank column
one_hot_data = one_hot_data.drop('rank', axis=1)

# Print the first 10 rows of our data
one_hot_data[:10]
145/13:
# Making a copy of our data
processed_data = one_hot_data[:]

# TODO: Scale the columns

# Printing the first 10 rows of our procesed data
processed_data[:10]
145/14:
# Making a copy of our data
processed_data = one_hot_data[:]

# TODO: Scale the columns
processed_data["gre"] = processed_data["gre"]/800
processed_data["gpa"] = processed_data["gpa"]/4.0

# Printing the first 10 rows of our procesed data
processed_data[:10]
145/15:
sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)
train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)

print("Number of training samples is", len(train_data))
print("Number of testing samples is", len(test_data))
print(train_data[:10])
print(test_data[:10])
145/16:
features = train_data.drop('admit', axis=1)
targets = train_data['admit']
features_test = test_data.drop('admit', axis=1)
targets_test = test_data['admit']

print(features[:10])
print(targets[:10])
145/17:
# Activation (sigmoid) function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_prime(x):
    return sigmoid(x) * (1-sigmoid(x))
def error_formula(y, output):
    return - y*np.log(output) - (1 - y) * np.log(1-output)
145/18:
# TODO: Write the error term formula
def error_term_formula(x, y, output):
     return (y - output)*x
#    for mean square error
#    return (y - output)*sigmoid_prime(x)*x
145/19:
# Neural Network hyperparameters
epochs = 1000
learnrate = 0.0001

# Training function
def train_nn(features, targets, epochs, learnrate):
    
    # Use to same seed to make debugging easier
    np.random.seed(42)

    n_records, n_features = features.shape
    last_loss = None

    # Initialize weights
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)

    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features.values, targets):
            # Loop through all records, x is the input, y is the target

            # Activation of the output unit
            #   Notice we multiply the inputs and the weights here 
            #   rather than storing h as a separate variable 
            output = sigmoid(np.dot(x, weights))

            # The error term
            error_term = error_term_formula(x, y, output)

            # The gradient descent step, the error times the gradient times the inputs
            del_w += error_term

        # Update the weights here. The learning rate times the 
        # change in weights
        # don't have to divide by n_records since it is compensated by the learning rate
        weights += learnrate * del_w #/ n_records  

        # Printing out the mean square error on the training set
        if e % (epochs / 10) == 0:
            out = sigmoid(np.dot(features, weights))
            loss = np.mean(error_formula(targets, out))
            print("Epoch:", e)
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            print("=========")
    print("Finished training!")
    return weights
    
weights = train_nn(features, targets, epochs, learnrate)
145/20:
# Calculate accuracy on test data
test_out = sigmoid(np.dot(features_test, weights))
predictions = test_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))
145/21:
# Calculate accuracy on test data
test_out = sigmoid(np.dot(features_test, weights))
predictions = test_out > 0.5
# accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))
145/22:
# Calculate accuracy on test data
test_out = sigmoid(np.dot(features_test, weights))
predictions = test_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))
148/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
148/2:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
148/3:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
148/4:
#dropping/ignoring car_name 
cData = cData.drop('car name', axis=1)
# Also replacing the categorical var with actual values
cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
cData.head()
148/5:
cData = pd.get_dummies(cData, columns=['origin'])
cData.head()
148/6:
#A quick summary of the data columns
cData.describe()
148/7:
# hp is missing cause it does not seem to be reqcognized as a numerical column!
cData.dtypes
148/8:
# isdigit()? on 'horsepower' 
hpIsDigit = pd.DataFrame(cData.horsepower.str.isdigit())  # if the string is made of digits store True else False

#print isDigit = False!
cData[hpIsDigit['horsepower'] == False]   # from temp take only those rows where hp has false
148/9:
# Missing values have a'?''
# Replace missing values with NaN
cData = cData.replace('?', np.nan)
cData[hpIsDigit['horsepower'] == False]
148/10:
#instead of dropping the rows, lets replace the missing values with median value. 
cData.median()
148/11:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
cData = cData.apply(medianFiller,axis=0)

cData['horsepower'] = cData['horsepower'].astype('float64')  # converting the hp column from object / string type to float
148/12:
cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr, diag_kind='kde')   # to plot density curve instead of histogram on the diag
148/13:
cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr)   # to plot density curve instead of histogram on the diag
148/14:
cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr, diag_kind='kde')   # to plot density curve instead of histogram on the diag
148/15:
# lets build our linear model
# independant variables
X = cData.drop(['mpg','origin_europe'], axis=1)
# the dependent variable
y = cData[['mpg']]
148/16:
# Split X and y into training and test set in 70:30 ratio

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)
148/17:
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)
148/18:
for idx, col_name in enumerate(X_train.columns):
    print("The coefficient for {} is {}".format(col_name, regression_model.coef_[0][idx]))
148/19:
intercept = regression_model.intercept_[0]
print("The intercept for our model is {}".format(intercept))
148/20: regression_model.score(X_train, y_train)
148/21:
#out of sample score (R^2)

regression_model.score(X_test, y_test)
148/22:
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train2 = poly.fit_transform(X_train)
X_test2 = poly.fit_transform(X_test)

poly_clf = linear_model.LinearRegression()

poly_clf.fit(X_train2, y_train)

y_pred = poly_clf.predict(X_test2)

#print(y_pred)

#In sample (training) R^2 will always improve with the number of variables!
print(poly_clf.score(X_train2, y_train))
148/23:
#Out off sample (testing) R^2 is our measure of sucess and does improve
print(poly_clf.score(X_test2, y_test))
148/24:
# but this improves as the cost of 29 extra variables!
print(X_train.shape)
print(X_train2.shape)
150/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
150/2:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
150/3:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
150/4:
#dropping/ignoring car_name 
cData = cData.drop('car name', axis=1)
# Also replacing the categorical var with actual values
cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
cData.head()
150/5:
cData = pd.get_dummies(cData, columns=['origin'])
cData.head()
150/6:
#A quick summary of the data columns
cData.describe()
150/7:
# hp is missing cause it does not seem to be reqcognized as a numerical column!
cData.dtypes
150/8:
# isdigit()? on 'horsepower' 
hpIsDigit = pd.DataFrame(cData.horsepower.str.isdigit())  # if the string is made of digits store True else False

#print isDigit = False!
cData[hpIsDigit['horsepower'] == False]   # from temp take only those rows where hp has false
150/9:
# Missing values have a'?''
# Replace missing values with NaN
cData = cData.replace('?', np.nan)
cData[hpIsDigit['horsepower'] == False]
150/10:
#instead of dropping the rows, lets replace the missing values with median value. 
cData.median()
150/11:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
cData = cData.apply(medianFiller,axis=0)

cData['horsepower'] = cData['horsepower'].astype('float64')  # converting the hp column from object / string type to float
150/12:
cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr, diag_kind='kde')   # to plot density curve instead of histogram on the diag
150/13:
# lets build our linear model
# independant variables
X = cData.drop(['mpg','origin_europe'], axis=1)
# the dependent variable
y = cData[['mpg']]
150/14:
# Split X and y into training and test set in 70:30 ratio

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)
150/15:
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)
150/16:
for idx, col_name in enumerate(X_train.columns):
    print("The coefficient for {} is {}".format(col_name, regression_model.coef_[0][idx]))
150/17:
for idx, col_name in enumerate(X_train.columns):
    print("The coefficient for {} is {}".format(col_name, regression_model.coef_[0][idx]))
150/18:
intercept = regression_model.intercept_[0]
print("The intercept for our model is {}".format(intercept))
150/19: regression_model.score(X_train, y_train)
150/20:
#out of sample score (R^2)

regression_model.score(X_test, y_test)
150/21:
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train2 = poly.fit_transform(X_train)
X_test2 = poly.fit_transform(X_test)

poly_clf = linear_model.LinearRegression()

poly_clf.fit(X_train2, y_train)

y_pred = poly_clf.predict(X_test2)

#print(y_pred)

#In sample (training) R^2 will always improve with the number of variables!
print(poly_clf.score(X_train2, y_train))
150/22:
#Out off sample (testing) R^2 is our measure of sucess and does improve
print(poly_clf.score(X_test2, y_test))
150/23:
# but this improves as the cost of 29 extra variables!
print(X_train.shape)
print(X_train2.shape)
151/1:
x = Cigarettes smoked per day = [4, 5, 8, 10, 11]

y = Years lived =  [50, 48, 45 , 42, 41]
151/2:
x = [4, 5, 8, 10, 11]

y =  [50, 48, 45 , 42, 41]
151/3:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr)
151/4:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr,round=2)
151/5:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr,round(3))
151/6:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr)
155/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
156/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
157/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
157/2:
# loading the dataset
df = pd.read_csv("anime_data.csv")
157/3:
# checking shape of the data
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")
157/4:
# to view first 5 rows of the dataset
df.head()
157/5:
# to view last 5 rows of the dataset
df.tail()
157/6:
# let's create a copy of the data to avoid any changes to original data
data = df.copy()
157/7:
# checking for duplicate values in the data
data.duplicated().sum()
157/8:
# checking the names of the columns in the data
print(data.columns)
157/9:
# checking column datatypes and number of non-null values
data.info()
157/10:
# checking for missing values in the data
data.isnull().sum()
157/11:
# Let's look at the statistical summary of the data
data.describe().T
157/12:
# filtering non-numeric columns
cat_columns = data.select_dtypes(exclude=np.number).columns
cat_columns
157/13:
# we will skip the title and description columns as they will have a lot of unique values
cat_col = ["mediaType", "ongoing", "sznOfRelease", "studio_primary"]

# printing the number of occurrences of each unique value in each categorical column
for column in cat_col:
    print(data[column].value_counts())
    print("-" * 50)
157/14:
# we will skip the title and description columns as they will have a lot of unique values
cat_col = ["mediaType", "ongoing", "sznOfRelease", "studio_primary"]

# printing the number of occurrences of each unique value in each categorical column
for column in cat_col:
    print(data[column].value_counts())
    print("-" * 50)
157/15: data.drop(["title", "description"], axis=1, inplace=True)
157/16:
data.dropna(inplace=True)
data.shape
157/17:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
157/18: histogram_boxplot(data, "rating")
157/19: histogram_boxplot(data, "eps", bins=100)
157/20: histogram_boxplot(data, "duration")
157/21: histogram_boxplot(data, "watched", bins=50)
157/22: histogram_boxplot(data, "years_running")
157/23:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
157/24: labeled_barplot(data, "mediaType", perc=True)
157/25: labeled_barplot(data, "ongoing", perc=True)
157/26: labeled_barplot(data, "sznOfRelease", perc=True)
157/27: labeled_barplot(data, "studio_primary", perc=True)
157/28:
# creating a list of tag columns
tag_cols = [item for item in data.columns if "tag" in item]

# printing the number of occurrences of each unique value in each categorical column
for column in tag_cols:
    print(data[column].value_counts())
    print("-" * 50)
157/29:
# creating a list of non-tag columns
corr_cols = [item for item in data.columns if "tag" not in item]
print(corr_cols)
157/30:
plt.figure(figsize=(15, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
157/31:
plt.figure(figsize=(10, 5))
sns.boxplot(x="mediaType", y="rating", data=data)
plt.show()
157/32:
plt.figure(figsize=(10, 5))
sns.boxplot(x="sznOfRelease", y="rating", data=data)
plt.show()
157/33:
plt.figure(figsize=(15, 5))
sns.boxplot(x="studio_primary", y="rating", data=data)
plt.xticks(rotation=90)
plt.show()
157/34:
data.drop("votes", axis=1, inplace=True)
data.shape
157/35:
X = data.drop(["rating"], axis=1)
y = data["rating"]
157/36:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)
X.head()
157/37: X.shape
157/38:
# splitting the data in 70:30 ratio for train to test data

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
157/39:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
159/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
159/2:
# loading the dataset
df = pd.read_csv("anime_data.csv")
159/3:
# checking shape of the data
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")
159/4:
# to view first 5 rows of the dataset
df.head()
159/5:
# to view last 5 rows of the dataset
df.tail()
159/6:
# let's create a copy of the data to avoid any changes to original data
data = df.copy()
159/7:
# checking for duplicate values in the data
data.duplicated().sum()
159/8:
# checking the names of the columns in the data
print(data.columns)
159/9:
# checking column datatypes and number of non-null values
data.info()
159/10:
# checking for missing values in the data
data.isnull().sum()
159/11:
# Let's look at the statistical summary of the data
data.describe().T
159/12:
# filtering non-numeric columns
cat_columns = data.select_dtypes(exclude=np.number).columns
cat_columns
159/13:
# we will skip the title and description columns as they will have a lot of unique values
cat_col = ["mediaType", "ongoing", "sznOfRelease", "studio_primary"]

# printing the number of occurrences of each unique value in each categorical column
for column in cat_col:
    print(data[column].value_counts())
    print("-" * 50)
159/14: data.drop(["title", "description"], axis=1, inplace=True)
159/15:
data.dropna(inplace=True)
data.shape
159/16:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
159/17: histogram_boxplot(data, "rating")
159/18: histogram_boxplot(data, "eps", bins=100)
159/19: histogram_boxplot(data, "duration")
159/20: histogram_boxplot(data, "watched", bins=50)
159/21: histogram_boxplot(data, "years_running")
159/22:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
159/23: labeled_barplot(data, "mediaType", perc=True)
159/24: labeled_barplot(data, "ongoing", perc=True)
159/25: labeled_barplot(data, "sznOfRelease", perc=True)
159/26: labeled_barplot(data, "studio_primary", perc=True)
159/27:
# creating a list of tag columns
tag_cols = [item for item in data.columns if "tag" in item]

# printing the number of occurrences of each unique value in each categorical column
for column in tag_cols:
    print(data[column].value_counts())
    print("-" * 50)
159/28:
# creating a list of non-tag columns
corr_cols = [item for item in data.columns if "tag" not in item]
print(corr_cols)
159/29:
plt.figure(figsize=(15, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
159/30:
plt.figure(figsize=(10, 5))
sns.boxplot(x="mediaType", y="rating", data=data)
plt.show()
159/31:
plt.figure(figsize=(10, 5))
sns.boxplot(x="sznOfRelease", y="rating", data=data)
plt.show()
159/32:
plt.figure(figsize=(15, 5))
sns.boxplot(x="studio_primary", y="rating", data=data)
plt.xticks(rotation=90)
plt.show()
159/33:
data.drop("votes", axis=1, inplace=True)
data.shape
159/34:
X = data.drop(["rating"], axis=1)
y = data["rating"]
159/35:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)
X.head()
159/36: X.shape
159/37:
# splitting the data in 70:30 ratio for train to test data

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
159/38:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
159/39:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
159/40:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
159/41:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
159/42:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
159/43:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
159/44:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
159/45:
# please uncomment and run the next line if mlxtend library is not previously installed
#!pip install mlxtend
159/46:
# please uncomment and run the next line if mlxtend library is not previously installed
#!pip install mlxtend
159/47:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes the number of features to select
    floating=False,
    scoring="r2",
    n_jobs=-1,  # n_jobs=-1 means all processor cores will be used
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
159/48:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes the number of features to select
    floating=False,
    scoring="r2",
    n_jobs=-1,  # n_jobs=-1 means all processor cores will be used
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
159/49:
# to plot the performance with addition of each feature
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

fig1 = plot_sfs(sfs.get_metric_dict(), kind="std_err", figsize=(15, 5))
plt.title("Sequential Forward Selection (w. StdErr)")
plt.xticks(rotation=90)
plt.show()
159/50:
reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=30,
    forward=True,
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
159/51:
# let us select the features which are important
feat_cols = list(sfs.k_feature_idx_)
print(feat_cols)
159/52:
# let us look at the names of the important features
x_train.columns[feat_cols]
159/53: x_train_final = x_train[x_train.columns[feat_cols]]
159/54:
# Creating new x_test with the same 20 variables that we selected for x_train
x_test_final = x_test[x_train_final.columns]
159/55:
# Fitting linear model
lin_reg_model2 = LinearRegression()
lin_reg_model2.fit(x_train_final, y_train)

# let us check the coefficients and intercept of the model

coef_df = pd.DataFrame(
    np.append(lin_reg_model2.coef_.flatten(), lin_reg_model2.intercept_),
    index=x_train_final.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
print(coef_df)
159/56:
# model performance on train set
print("Training Performance\n")
lin_reg_model2_train_perf = model_performance_regression(
    lin_reg_model2, x_train_final, y_train
)
lin_reg_model2_train_perf
159/57:
# model performance on test set
print("Test Performance\n")
lin_reg_model2_test_perf = model_performance_regression(
    lin_reg_model2, x_test_final, y_test
)
lin_reg_model2_test_perf
159/58:
# training performance comparison

models_train_comp_df = pd.concat(
    [lin_reg_model_train_perf.T, lin_reg_model2_train_perf.T],
    axis=1,
)

models_train_comp_df.columns = [
    "Linear Regression sklearn",
    "Linear Regression sklearn (SFS features)",
]

print("Training performance comparison:")
models_train_comp_df
159/59:
# test performance comparison

models_test_comp_df = pd.concat(
    [lin_reg_model_test_perf.T, lin_reg_model2_test_perf.T],
    axis=1,
)

models_test_comp_df.columns = [
    "Linear Regression sklearn",
    "Linear Regression sklearn (SFS features)",
]

print("Test performance comparison:")
models_test_comp_df
160/1:
# First, import PyTorch
import torch
160/2:
# First, import PyTorch
import torch
161/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
162/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
162/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
163/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
163/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
163/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
163/4: df.info()  # down to 82 columns after the initial 88
163/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
163/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
163/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
163/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
163/9: df.head()
163/10: colname.head()
163/11: colname
163/12: df[position_cols].head()
163/13:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
163/14: df[['Height','Weight']]
163/15:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
163/16:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
163/17: df["Work Rate"].head()
163/18:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
163/19:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
163/20: df.head(2)
165/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
165/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
165/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
165/4: df.info()  # down to 82 columns after the initial 88
165/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
165/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
165/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
165/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
165/9: df[position_cols].head()
165/10:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
165/11: df[['Height','Weight']]
165/12:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
165/13:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
165/14: df["Work Rate"].head()
165/15:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
165/16:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
165/17: df.head(2)
165/18:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
165/19:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
165/20: df.info()
165/21: df.describe().T  # quick summary of numeric features
166/1:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
dataframe-1.png
166/2:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/3:
import pandas as pd
import numpy as np
166/4:
x = [4, 5, 8, 10, 11]

y =  [50, 48, 45 , 42, 41]
166/5:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr)
166/6:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/7: data.mean(axis=1)
166/8: np.mean(data, axis=1)
166/9:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/10: data["Col4"].replace("Nature","Beauty", inplace=False)
166/11: data["Col4"].str.replace("Nature","Beauty")
166/12:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/13: data["Col5"] = data[["Col1","Col2","Col3"]].sum(axis=1)
166/14: data
166/15:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/16: data["Col6"] = pd.diff(data["Col3"],data["Col1"],axis=0)
166/17: data["Col6"] = data["Col3"] - data["Col1"]
166/18: data
166/19:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
166/20:
data.drop(["Col2"])
data
166/21:
data.drop(["Col2"],axis=1)
data
166/22:
data.drop(["Col2"],axis=0)
data
166/23:
data.drop(["Col2"],axis=1)
data
166/24: data.drop(["Col2"],axis=1)
166/25: data
168/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
168/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
168/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
168/4: df.info()  # down to 82 columns after the initial 88
168/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
168/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
168/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
168/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
168/9: df[position_cols].head()
168/10:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
168/11: df[['Height','Weight']]
168/12:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
168/13:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
168/14: df["Work Rate"].head()
168/15:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
168/16:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
168/17: df.head(2)
168/18:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
168/19:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
169/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
169/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
169/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
169/4: df.info()  # down to 82 columns after the initial 88
169/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
169/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
169/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
169/8:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
169/9:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
169/10: df[position_cols].head()
169/11:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
169/12: df[['Height','Weight']]
169/13:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
169/14:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
169/15: df["Work Rate"].head()
169/16:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
169/17:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
169/18: df.head(2)
169/19:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
169/20:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
169/21: summaryname_to_cols
169/22: df.info()
171/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
171/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
171/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
171/4: df.info()  # down to 82 columns after the initial 88
171/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
171/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
171/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
171/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
171/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
171/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
171/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
171/12: df["Work Rate"].head()
171/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
171/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
171/15: df.head(2)
171/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
171/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
171/18: cols_to_drop
171/19: df.info()
171/20: df.describe().T  # quick summary of numeric features
171/21:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
171/22: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
171/23: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
171/24:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
171/25: df['Contract Valid Until'].value_counts(dropna=False)
171/26:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
171/27:
# df['is_GK'] = df['Position'] == 'GK'  # for hue
# cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
# sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
# df.drop(['is_GK'], axis=1, inplace=True)
171/28:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
171/29:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
171/30: df.drop(['Release Clause'], axis=1, inplace=True)
171/31: df.drop(['Release Clause'], axis=1, inplace=True)
172/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
172/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
172/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
172/4: df.info()  # down to 82 columns after the initial 88
172/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
172/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
172/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
172/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
172/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
172/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
172/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
172/12: df["Work Rate"].head()
172/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
172/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
172/15: df.head(2)
172/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
172/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
172/18: df.info()
172/19: df.describe().T  # quick summary of numeric features
172/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
172/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
172/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
172/23: df['Contract Valid Until'].value_counts(dropna=False)
172/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
172/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
172/26: df.drop(['Release Clause'], axis=1, inplace=True)
172/27:
cols_to_log = ['Wage', 'Value']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
172/28:
plt.hist(np.log(df['Wage'] + 1), 50)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
172/29:
for colname in cols_to_log:
    df[colname + '_log'] = np.log(df[colname] + 1)
df.drop(cols_to_log, axis=1, inplace=True)
172/30:
# Height is in inches
binned_ht = pd.cut(df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf])
binned_ht
172/31: binned_ht.value_counts(dropna=False)
172/32:
# can add custom labels
df['height_bin'] = pd.cut(
    df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf], 
    labels = ["Under 5'", "5' to 5'6", "5'6 to 6'", "Over 6'"]
)
df.drop(['Height'], axis=1, inplace=True)
df['height_bin'].value_counts(dropna=False)
172/33:
print(df['Weight'].head(2))
df['Weight'] = df['Weight'].apply(lambda wt: round(wt * 0.4535, 2))
df['Weight'].head(2)
173/1:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
173/2:
import pandas as pd
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
173/3: np.product(data['Col1'],5)
173/4:
import pandas as pd
import numpy as np
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
173/5: np.product(data['Col1'],5)
173/6: pd.product('Col1', data=data, value=5)
173/7: data['Col1'] * 5
173/8: data['Col1'].apply(lambda x : x*5)
175/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
175/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
175/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
175/4: df.info()  # down to 82 columns after the initial 88
175/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
175/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
175/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
175/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
175/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
175/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
175/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
175/12: df["Work Rate"].head()
175/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
175/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
175/15: df.head(2)
175/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
175/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
175/18: df.info()
175/19: df.describe().T  # quick summary of numeric features
175/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
175/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
175/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
175/23: df['Contract Valid Until'].value_counts(dropna=False)
175/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
175/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
175/26: df.drop(['Release Clause'], axis=1, inplace=True)
176/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
176/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
176/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
176/4: df.info()  # down to 82 columns after the initial 88
176/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
176/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
176/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
176/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
176/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
176/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
176/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
176/12: df["Work Rate"].head()
176/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
176/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
176/15: df.head(2)
176/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
176/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
176/18: df.info()
176/19: df.describe().T  # quick summary of numeric features
176/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
176/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
176/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
176/23: df['Contract Valid Until'].value_counts(dropna=False)
176/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
176/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
176/26: df.drop(['Release Clause'], axis=1, inplace=True)
176/27:
cols_to_log = ['Wage', 'Value']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
176/28:
plt.hist(np.log(df['Wage'] + 1), 50)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
176/29:
for colname in cols_to_log:
    df[colname + '_log'] = np.log(df[colname] + 1)
df.drop(cols_to_log, axis=1, inplace=True)
176/30:
# Height is in inches
binned_ht = pd.cut(df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf])
binned_ht
176/31: binned_ht.value_counts(dropna=False)
176/32:
# can add custom labels
df['height_bin'] = pd.cut(
    df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf], 
    labels = ["Under 5'", "5' to 5'6", "5'6 to 6'", "Over 6'"]
)
df.drop(['Height'], axis=1, inplace=True)
df['height_bin'].value_counts(dropna=False)
176/33:
print(df['Weight'].head(2))
df['Weight'] = df['Weight'].apply(lambda wt: round(wt * 0.4535, 2))
df['Weight'].head(2)
176/34:
cat_vars = ['Preferred Foot', 'Body Type', 'Position',
            'Workrate_attack', 'Workrate_defense']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df[colname] = df[colname].astype('category')
    
df.info()
176/35:
# how many players are in clubs that start with 'FC'?
df['Club'].str.startswith('FC').sum()
176/36:
# how many letters and words in the unique club names?

# doing i == i as a quick check for NaNs
# using .title() in case of capitalization issues
club_data = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in df['Club'].str.strip().str.title().unique()],
    columns = ['Club', 'Number of Letters', 'Number of Words']
)
club_data.head()
176/37:
club_data['Number of Letters'].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

club_data['Number of Words'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()

print('These are the clubs with the most words in the name:')
club_data.loc[club_data['Number of Words'] == club_data['Number of Words'].max(), 'Club']
176/38: from sklearn.preprocessing import StandardScaler, MinMaxScaler
176/39:
std_scaler = StandardScaler()

df['Weight'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['Weight']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['Weight'], axis=1, inplace=True)
176/40:
# replacing with scaled 
df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate before minmax scaling')
plt.show()

df[['Attack_rate', 'Midfield_rate', 'Defense_rate']] = MinMaxScaler().fit_transform(
    df[['Attack_rate', 'Midfield_rate', 'Defense_rate']]
)

df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate after minmax scaling')
plt.show()

# if the minimum and maximum are treated as fixed, this is also a linear transformation
# so the shape is the same
176/41: df.isnull().sum() # lots of columns don't have missingness
176/42:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
176/43: df.drop(['Loaned From'], axis=1, inplace=True)
176/44:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
176/45:
# these are missing `Joined` and `Joined year`
df[num_missing == 2].sample(n=5)
176/46:
# these are missing `Joined` and `Joined year`
df[num_missing == 2].sample(n=5)
176/47:
# these are missing `Attack_rate`, `Midfield_rate`, and `Defense_rate`
df[num_missing == 3].sample(n=5)
176/48:
for n in num_missing.value_counts().sort_index().index:
    if n > 0:
        print(f'For the rows with exactly {n} missing values, NAs are found in:')
        n_miss_per_col = df[num_missing == n].isnull().sum()
        print(n_miss_per_col[n_miss_per_col > 0])
        print('\n\n')
176/49:
# nans are floats so they become strings here
# we also need this to be strings because we're adding a category that's not present
df['height_bin'] = df['height_bin'].astype(str).replace('nan', 'is_missing').astype('category')
176/50:
# now using `fillna` with a numeric column
print(df['Passing'].isnull().sum())
df['Passing'].fillna(df['Passing'].mean(), inplace=True)  # mean imputation
df['Passing'].isnull().sum()
177/1: pd.get_dummies(df['height_bin'], drop_first=True)
177/2:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
177/3: pd.get_dummies(df['height_bin'], drop_first=True)
177/4:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
177/5: pd.get_dummies(df['height_bin'], drop_first=True)
177/6:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
177/7:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
177/8: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
177/9: df.info()  # down to 82 columns after the initial 88
177/10:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
177/11:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
177/12:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
177/13:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
177/14:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
177/15:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
177/16:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
177/17: df["Work Rate"].head()
177/18:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
177/19:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
177/20: df.head(2)
177/21:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
177/22:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
177/23: df.info()
177/24: df.describe().T  # quick summary of numeric features
177/25:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
177/26: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
177/27:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
177/28: df.drop(['Release Clause'], axis=1, inplace=True)
177/29:
cols_to_log = ['Wage', 'Value']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
177/30:
plt.hist(np.log(df['Wage'] + 1), 50)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
177/31:
for colname in cols_to_log:
    df[colname + '_log'] = np.log(df[colname] + 1)
df.drop(cols_to_log, axis=1, inplace=True)
177/32:
# Height is in inches
binned_ht = pd.cut(df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf])
binned_ht
177/33: binned_ht.value_counts(dropna=False)
177/34:
# can add custom labels
df['height_bin'] = pd.cut(
    df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf], 
    labels = ["Under 5'", "5' to 5'6", "5'6 to 6'", "Over 6'"]
)
df.drop(['Height'], axis=1, inplace=True)
df['height_bin'].value_counts(dropna=False)
177/35:
print(df['Weight'].head(2))
df['Weight'] = df['Weight'].apply(lambda wt: round(wt * 0.4535, 2))
df['Weight'].head(2)
177/36:
cat_vars = ['Preferred Foot', 'Body Type', 'Position',
            'Workrate_attack', 'Workrate_defense']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df[colname] = df[colname].astype('category')
    
df.info()
177/37:
# how many players are in clubs that start with 'FC'?
df['Club'].str.startswith('FC').sum()
177/38:
# how many letters and words in the unique club names?

# doing i == i as a quick check for NaNs
# using .title() in case of capitalization issues
club_data = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in df['Club'].str.strip().str.title().unique()],
    columns = ['Club', 'Number of Letters', 'Number of Words']
)
club_data.head()
177/39:
club_data['Number of Letters'].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

club_data['Number of Words'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()

print('These are the clubs with the most words in the name:')
club_data.loc[club_data['Number of Words'] == club_data['Number of Words'].max(), 'Club']
177/40: from sklearn.preprocessing import StandardScaler, MinMaxScaler
177/41:
std_scaler = StandardScaler()

df['Weight'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['Weight']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['Weight'], axis=1, inplace=True)
177/42:
# replacing with scaled 
df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate before minmax scaling')
plt.show()

df[['Attack_rate', 'Midfield_rate', 'Defense_rate']] = MinMaxScaler().fit_transform(
    df[['Attack_rate', 'Midfield_rate', 'Defense_rate']]
)

df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate after minmax scaling')
plt.show()

# if the minimum and maximum are treated as fixed, this is also a linear transformation
# so the shape is the same
177/43: df.isnull().sum() # lots of columns don't have missingness
177/44:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
177/45: df.drop(['Loaned From'], axis=1, inplace=True)
177/46:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
177/47:
# these are missing `Joined` and `Joined year`
df[num_missing == 2].sample(n=5)
177/48:
# these are missing `Attack_rate`, `Midfield_rate`, and `Defense_rate`
df[num_missing == 3].sample(n=5)
177/49:
for n in num_missing.value_counts().sort_index().index:
    if n > 0:
        print(f'For the rows with exactly {n} missing values, NAs are found in:')
        n_miss_per_col = df[num_missing == n].isnull().sum()
        print(n_miss_per_col[n_miss_per_col > 0])
        print('\n\n')
177/50:
# nans are floats so they become strings here
# we also need this to be strings because we're adding a category that's not present
df['height_bin'] = df['height_bin'].astype(str).replace('nan', 'is_missing').astype('category')
177/51:
# now using `fillna` with a numeric column
print(df['Passing'].isnull().sum())
df['Passing'].fillna(df['Passing'].mean(), inplace=True)  # mean imputation
df['Passing'].isnull().sum()
177/52: pd.get_dummies(df['height_bin'], drop_first=True)
177/53:
# can do one hot encoding with get_dummies
pd.get_dummies(df['height_bin'], drop_first=False).iloc[:10, :]
177/54:
# or we can use sklearn
from sklearn.preprocessing import OneHotEncoder

OneHotEncoder(sparse=False).fit_transform(df[['height_bin']])[:10,:]
177/55:
def z_transform(x):
    return (x - np.mean(x)) / np.std(x)

np.random.seed(1)
x1 = np.random.normal(size=1000)
x2 = np.random.lognormal(size=1000)


plt.hist(z_transform(x1))
plt.title('z-transformed normal data')
plt.show()


plt.hist(z_transform(x2))
plt.title('z-transformed lognormal data')
plt.show()
177/56:
def frac_outside_1pt5_IQR(x):
    length = 1.5 * np.diff(np.quantile(x, [.25, .75]))
    return np.mean(np.abs(x - np.median(x)) > length)

print(frac_outside_1pt5_IQR(x1))
print(frac_outside_1pt5_IQR(x2))
177/57:
plt.hist(df['Power'], 20)
plt.title('Histogram of Power')
plt.show()

sns.boxplot(df['Power'])
plt.title('Boxplot of Power')
plt.show()
177/58:
quartiles = np.quantile(df['Power'][df['Power'].notnull()], [.25, .75])
power_4iqr = 4 * (quartiles[1] - quartiles[0])
print(f'Q1 = {quartiles[0]}, Q3 = {quartiles[1]}, 4*IQR = {power_4iqr}')
outlier_powers = df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power']
outlier_powers
177/59:
# making the situation more extreme
df['Power'].hist(bins=20)
plt.title('Power before exaggerating the outliers')
plt.show()
print(df['Power'].mean())
df.loc[outlier_powers.index, 'Power'] = [-200000.0, -1200000.0]
df['Power'].hist(bins=20)
plt.title('Power after exaggerating outliers')
plt.show()
177/60:
# if we wanted to make these NA we could just do this
# [not run]
df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power'] = np.nan
177/61:
# dropping these rows
# [not run]
df.drop(outlier_powers.index, axis=0, inplace=True)
177/62:
power = df['Power'][df['Power'].notnull()]

print(power.mean())  # the mean is being pulled
print(power.median())
177/63:
from scipy.stats import tmean

print(tmean(power, limits=np.quantile(power, [.1, .9])))
print(tmean(power, limits=[0,100]))
178/1:
import pandas as pd
import numpy as np
178/2:
x = [4, 5, 8, 10, 11]

y =  [50, 48, 45 , 42, 41]
178/3:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr)
178/4:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/5: data.mean(axis=1)
178/6: np.mean(data, axis=1)
178/7:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/8: data["Col4"].replace("Nature","Beauty", inplace=False)
178/9: data["Col4"].str.replace("Nature","Beauty")
178/10:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/11: data["Col5"] = data[["Col1","Col2","Col3"]].sum(axis=1)
178/12: data
178/13:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/14: data["Col6"] = data["Col3"] - data["Col1"]
178/15: data
178/16:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/17: data.drop(["Col2"],axis=1)
178/18: data
178/19:
import pandas as pd
import numpy as np
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
178/20: data['Col1'].apply(lambda x : x*5)
178/21:
data = [2, 5, 12, 15, 19, 4, 6, 11, 16, 18, 12, 12, 42, 6, 56, 34, 23, 11]
data
178/22: np.quantile(data, [.25, .75])
179/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
179/2:
df = pd.read_csv("KickStarterProjects.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
179/3:
df.drop(['currency'], axis=1, inplace=True)
df.drop(['goal'], axis=1, inplace=True)
df
179/4: print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')
179/5:
df.drop(['currency','goal'], axis=1, inplace=True)

df
179/6:
df.drop(['currency'], axis=1, inplace=True)
df.drop(['goal'], axis=1, inplace=True)
df
179/7:
df.drop(['currency'], axis=1, inplace=True)
df.drop(['goal'], axis=1, inplace=True)
df
179/8:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
179/9:
df = pd.read_csv("KickStarterProjects.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
179/10:
df.drop(['currency'], axis=1, inplace=True)
df.drop(['goal'], axis=1, inplace=True)
df
179/11: df.isnull().sum().sort_values(ascending=False)
179/12:
df.drop(['name'], axis=1, inplace=True)
df
179/13: state.unique()
179/14: df['state'].unique()
179/15: df['state'].unique().value_counts()
179/16: df['state'].unique()[0].value_counts()
179/17:
df['failed'].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

df['successful'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()
179/18:
df[state[0]].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

df['successful'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()
179/19:
df['state'][0].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

df['successful'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()
179/20: df.describe().T
179/21:
plt.hist(df['backers'], 20)
plt.title('Histogram of Power')
plt.show()

sns.boxplot(df['backers'])
plt.title('Boxplot of Power')
plt.show()
179/22:
plt.hist(df['backers'], 10)
plt.title('Histogram of Power')
plt.show()

sns.boxplot(df['backers'])
plt.title('Boxplot of Power')
plt.show()
179/23:
state_df = pd.DataFrame(
    df = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in df['state'].str.strip().str.title().unique()],
    columns = ['failed', 'successful']
)
state_df.head()
179/24:
state_df = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in data['state'].str.strip().str.title().unique()],
    columns = ['failed', 'successful']
)
state_df.head()
179/25:
state_df = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in state_df['state'].str.strip().str.title().unique()],
    columns = ['failed', 'successful']
)
state_df.head()
179/26: df['launched'].value_counts(dropna=False)
179/27:
contract_dates = pd.to_datetime(df['launched']).dt.month
print(contract_dates.value_counts(dropna=False))
df['launched'] = contract_dates
179/28:
cols_to_log = ['usd_goal_real']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
179/29:
cols_to_log = ['usd_goal_real']
for colname in cols_to_log:
    plt.hist(df[colname], bins=10)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
179/30:
plt.hist(np.log(df['usd_goal_real'] + 1), 10)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
179/31:
plt.hist(np.log(df['usd_goal_real'] + 1), 10)
plt.title('log(usd_goal_real + 1)')
plt.show()
plt.hist(np.arcsinh(df['usd_goal_real']), 50)
plt.title('arcsinh(usd_goal_real)')
plt.show()
plt.hist(np.sqrt(df['usd_goal_real']), 50)
plt.title('sqrt(usd_goal_real)')
plt.show()
179/32:
plt.hist(np.log(df['usd_goal_real'] + 1), 10)
plt.title('log(usd_goal_real + 1)')
plt.show()
# plt.hist(np.arcsinh(df['usd_goal_real']), 50)
# plt.title('arcsinh(usd_goal_real)')
# plt.show()
# plt.hist(np.sqrt(df['usd_goal_real']), 50)
# plt.title('sqrt(usd_goal_real)')
# plt.show()
179/33:
num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
179/34: from sklearn.preprocessing import StandardScaler, MinMaxScaler
179/35:
std_scaler = StandardScaler()

df['goal'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['goal']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['goal'], axis=1, inplace=True)
179/36:
std_scaler = StandardScaler()

df['usd_goal_real'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['usd_goal_real']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['usd_goal_real'], axis=1, inplace=True)
179/37:
# replacing with scaled 
df['usd_goal_real'].hist(bins=20)
plt.title('Attack_rate before minmax scaling')
plt.show()

df['usd_goal_real'] = MinMaxScaler().fit_transform(
    df['usd_goal_real']
)

df['usd_goal_real'].hist(bins=20)
plt.title('Attack_rate after minmax scaling')
plt.show()

# if the minimum and maximum are treated as fixed, this is also a linear transformation
# so the shape is the same
180/1:
import pandas as pd
df = pd.read_csv('winequality-red.csv', sep=';')
df.head()
180/2:
new_df = df.rename(columns={'fixed acidity': 'fixed_acidity',
                             'volatile acidity': 'volatile_acidity',
                             'citric acid': 'citric_acid',
                             'residual sugar': 'residual_sugar',
                             'free sulfur dioxide': 'free_sulfur_dioxide',
                             'total sulfur dioxide': 'total_sulfur_dioxide'
                            })
new_df.head()
180/3:

labels = list(df.columns)
labels[0] = labels[0].replace(' ', '_')
labels[1] = labels[1].replace(' ', '_')
labels[2] = labels[2].replace(' ', '_')
labels[3] = labels[3].replace(' ', '_')
labels[5] = labels[5].replace(' ', '_')
labels[6] = labels[6].replace(' ', '_')
df.columns = labels

df.head()
180/4:
def function(arr, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(arr)
180/5:
def function(arr, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(arr)
function(arr,1)
180/6:
list=[]
def function(list, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(arr)
function(arr,1)
180/7:
list=[]
def function(list, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(list)
function(list,1)
180/8:
list=[]
def function(list, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(list)
function(1)
180/9:
list=[]
def function(list, n):
    for labels in list(df.columns):
        labels[n] = labels[n].replace(' ', '_')
        print(list)
function(1)
180/10:
median_alcohol = df.alcohol.median()
for i, alcohol in enumerate(df.alcohol):
    if alcohol >= median_alcohol:
        df.loc[i, 'alcohol'] = 'high'
    else:
        df.loc[i, 'alcohol'] = 'low'
df.groupby('alcohol').quality.mean()
180/11:
median_pH = df.pH.median()
for i, pH in enumerate(df.pH):
    if pH >= median_pH:
        df.loc[i, 'pH'] = 'high'
    else:
        df.loc[i, 'pH'] = 'low'
df.groupby('pH').quality.mean()
180/12:
median_sugar = df.residual_sugar.median()
for i, sugar in enumerate(df.residual_sugar):
    if sugar >= median_sugar:
        df.loc[i, 'residual_sugar'] = 'high'
    else:
        df.loc[i, 'residual_sugar'] = 'low'
df.groupby('residual_sugar').quality.mean()
180/13:
median_citric_acid = df.citric_acid.median()
for i, citric_acid in enumerate(df.citric_acid):
    if citric_acid >= median_citric_acid:
        df.loc[i, 'citric_acid'] = 'high'
    else:
        df.loc[i, 'citric_acid'] = 'low'
df.groupby('citric_acid').quality.mean()
180/14: def function():
180/15:
def median_alcohol():
    median_alcohol = df.alcohol.median()
    for i, alcohol in enumerate(df.alcohol):
        if alcohol >= median_alcohol:
            df.loc[i, 'alcohol'] = 'high'
        else:
            df.loc[i, 'alcohol'] = 'low'
    df.groupby('alcohol').quality.mean()
    
def median_pH():    
    median_pH = df.pH.median()
    for i, pH in enumerate(df.pH):
        if pH >= median_pH:
            df.loc[i, 'pH'] = 'high'
        else:
            df.loc[i, 'pH'] = 'low'
    df.groupby('pH').quality.mean()
    
def median_sugar():    
    median_sugar = df.residual_sugar.median()
    for i, sugar in enumerate(df.residual_sugar):
        if sugar >= median_sugar:
            df.loc[i, 'residual_sugar'] = 'high'
        else:
            df.loc[i, 'residual_sugar'] = 'low'
    df.groupby('residual_sugar').quality.mean()
    
def median_citric():    
    median_citric_acid = df.citric_acid.median()
    for i, citric_acid in enumerate(df.citric_acid):
        if citric_acid >= median_citric_acid:
            df.loc[i, 'citric_acid'] = 'high'
        else:
            df.loc[i, 'citric_acid'] = 'low'
    df.groupby('citric_acid').quality.mean()
180/16: median_citric()
180/17: median_citric
180/18: median_citric(0.7)
180/19: median_citric()
180/20: median_citric
181/1:
import pandas as pd
df = pd.read_csv('winequality-red.csv', sep=';')
df.head()

## Renaming Columns

df.columns = [label.replace(' ', '_') for label in df.columns]
df.head()
181/2:
## Analyzing Features

def numeric_to_buckets(df, column_name):
    median = df[column_name].median()
    for i, val in enumerate(df[column_name]):
        if val >= median:
            df.loc[i, column_name] = 'high'
        else:
            df.loc[i, column_name] = 'low' 

for feature in df.columns[:-1]:
    numeric_to_buckets(df, feature)
    print(df.groupby(feature).quality.mean(), '\n')
181/3: print(df.groupby(feature))
181/4: print(df.groupby(feature).quality)
181/5: print(df.groupby(feature).quality.mean(), '\n')
182/1:
import time
import pandas as pd
import numpy as np
182/2:
with open('books_published_last_two_years.txt') as f:
    recent_books = f.read().split('\n')
    
with open('all_coding_books.txt') as f:
    coding_books = f.read().split('\n')
183/1:
import time
import pandas as pd
import numpy as np
183/2:
with open('books_published_last_two_years.txt') as f:
    recent_books = f.read().split('\n')
    
with open('all_coding_books.txt') as f:
    coding_books = f.read().split('\n')
183/3:
with open('books-published-last-two-years.txt') as f:
    recent_books = f.read().split('\n')
    
with open('all_coding_books.txt') as f:
    coding_books = f.read().split('\n')
183/4:
with open('books-published-last-two-years.txt') as f:
    recent_books = f.read().split('\n')
    
with open('all-coding-books.txt') as f:
    coding_books = f.read().split('\n')
183/5:
start = time.time()
recent_coding_books = []

for book in recent_books:
    if book in coding_books:
        recent_coding_books.append(book)

print(len(recent_coding_books))
print('Duration: {} seconds'.format(time.time() - start))
183/6:

start = time.time()
recent_coding_books = []

numpy.intersect1d(recent_books, coding_books,
                  assume_unique=False,
                  return_indices=False)
recent_coding_books = 
print(len(recent_coding_books))
print('Duration: {} seconds'.format(time.time() - start))
183/7:
start = time.time()
recent_coding_books = []


recent_coding_books = numpy.intersect1d(recent_books, coding_books,
                      assume_unique=False,
                      return_indices=False)

print(len(recent_coding_books))

print('Duration: {} seconds'.format(time.time() - start))
184/1:
import time
import pandas as pd
import numpy as np
184/2:
with open('books-published-last-two-years.txt') as f:
    recent_books = f.read().split('\n')
    
with open('all-coding-books.txt') as f:
    coding_books = f.read().split('\n')
184/3:
start = time.time()
recent_coding_books = []

for book in recent_books:
    if book in coding_books:
        recent_coding_books.append(book)

print(len(recent_coding_books))
print('Duration: {} seconds'.format(time.time() - start))
184/4:
start = time.time()
recent_coding_books = []


recent_coding_books = numpy.intersect1d(recent_books, coding_books,
                      assume_unique=False,
                      return_indices=False)

print(len(recent_coding_books))

print('Duration: {} seconds'.format(time.time() - start))
184/5:
start = time.time()
recent_coding_books = []


recent_coding_books = np.intersect1d(recent_books, coding_books,
                      assume_unique=False,
                      return_indices=False)

print(len(recent_coding_books))

print('Duration: {} seconds'.format(time.time() - start))
184/6:

start = time.time()
recent_coding_books = list(set(recent_books).intersection(coding_books))
print(len(recent_coding_books))
print('Duration: {} seconds'.format(time.time() - start))
185/1:
import time
import numpy as np
185/2:
with open('gift_costs.txt') as f:
    gift_costs = f.read().split('\n')
    
gift_costs = np.array(gift_costs).astype(int)  # convert string to int
185/3:
with open('gift-costs.txt') as f:
    gift_costs = f.read().split('\n')
    
gift_costs = np.array(gift_costs).astype(int)  # convert string to int
185/4:
start = time.time()

total_price = 0
for cost in gift_costs:
    if cost < 25:
        total_price += cost * 1.08  # add cost after tax

print(total_price)
print('Duration: {} seconds'.format(time.time() - start))
185/5:
start = time.time()

total_price =  (gift_costs[gift_costs < 25]).sum() * 1.08


print(total_price)
print('Duration: {} seconds'.format(time.time() - start))
185/6: print(gift_costs[gift_costs < 25])
185/7: print([gift_costs < 25])
185/8: print(gift_costs[gift_costs < 25])
185/9: print((gift_costs[gift_costs < 25]).sum())
185/10: print((gift_costs[gift_costs < 25]).sum()*1.08)
185/11: print((gift_costs[gift_costs < 25]).sum()*1.08)
187/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
187/2:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
187/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
187/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
187/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
187/6:
# checking for duplicate values in the data
df.duplicated().sum()
187/7:
# checking the names of the columns in the data
print(df.columns)
187/8:
# checking column datatypes and number of non-null values
df.info()
187/9:
# checking column datatypes and number of non-null values
df.info()
187/10:
# checking for missing values in the data.
df.isnull().sum()
187/11:
# Let's look at the statistical summary of the data
df.describe(include="all").T
187/12: df.dropna(subset=["rating"], inplace=True)
187/13:
# checking missing values in rest of the data
df.isnull().sum()
187/14: df[df.startYr.isnull()]
187/15: df.dropna(subset=["startYr"], inplace=True)
187/16:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
187/17:
# checking missing values in rest of the data
df.isnull().sum()
187/18: df[df.finishYr.isnull()]
187/19:
# checking the summary of the data with missing values in finishYr
df[df.finishYr.isnull()].describe(include="all").T
187/20:
df["finishYr"].fillna(2020, inplace=True)

# checking missing values in rest of the data
df.isnull().sum()
187/21:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
187/22:
# we define a function to convert the duration column to numeric


def time_to_minutes(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "hr" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            hr = (
                float(spl[0].replace("hr", "")) * 60
            )  # taking numeric part and converting hours to minutes
            mt = float(spl[1].replace("min", ""))  # taking numeric part of minutes
            return hr + mt
        else:
            return float(var.replace("min", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
187/23:
# let's apply the function to the duration column and overwrite the column
df["duration"] = df["duration"].apply(time_to_minutes)
df.head()
187/24:
# let's check the summary of the duration column
df["duration"].describe()
189/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
189/2:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
189/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
189/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
189/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
189/6:
# checking for duplicate values in the data
df.duplicated().sum()
189/7:
# checking the names of the columns in the data
print(df.columns)
189/8:
# checking column datatypes and number of non-null values
df.info()
189/9:
# checking for missing values in the data.
df.isnull().sum()
189/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
189/11: df.dropna(subset=["rating"], inplace=True)
189/12:
# checking missing values in rest of the data
df.isnull().sum()
189/13: df[df.startYr.isnull()]
189/14: df.dropna(subset=["startYr"], inplace=True)
189/15:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
189/16:
# checking missing values in rest of the data
df.isnull().sum()
189/17: df[df.finishYr.isnull()]
189/18:
# checking the summary of the data with missing values in finishYr
df[df.finishYr.isnull()].describe(include="all").T
189/19:
df["finishYr"].fillna(2020, inplace=True)

# checking missing values in rest of the data
df.isnull().sum()
189/20:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
189/21:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
189/22:
# we define a function to convert the duration column to numeric


def time_to_minutes(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "hr" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            hr = (
                float(spl[0].replace("hr", "")) * 60
            )  # taking numeric part and converting hours to minutes
            mt = float(spl[1].replace("min", ""))  # taking numeric part of minutes
            return hr + mt
        else:
            return float(var.replace("min", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
189/23:
# let's apply the function to the duration column and overwrite the column
df["duration"] = df["duration"].apply(time_to_minutes)
df.head()
189/24:
# let's check the summary of the duration column
df["duration"].describe()
189/25:
df["sznOfRelease"].fillna("is_missing", inplace=True)
df.isnull().sum()
189/26: df.mediaType.value_counts()
189/27:
df.mediaType.fillna("Other", inplace=True)

# checking the number of unique values and the number of times they occur
df.mediaType.value_counts()
189/28:
cols_with_list_vals = ["studios", "tags", "contentWarn"]

for col in cols_with_list_vals:
    df[col] = (
        df[col].str.lstrip("[").str.rstrip("]")
    )  # remove the leading and trailing square braces
    df[col] = df[col].replace("", np.nan)  # mark as NaN if the value is a blank string

df.head()
189/29:
# checking missing values in rest of the data
df.isnull().sum()
189/30:
df.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
189/31:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
189/32:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
190/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
190/2:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
190/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
190/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
190/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
190/6:
# checking for duplicate values in the data
df.duplicated().sum()
190/7:
# checking the names of the columns in the data
print(df.columns)
190/8:
# checking column datatypes and number of non-null values
df.info()
190/9:
# checking for missing values in the data.
df.isnull().sum()
190/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
190/11: df.dropna(subset=["rating"], inplace=True)
190/12:
# checking missing values in rest of the data
df.isnull().sum()
190/13: df[df.startYr.isnull()]
190/14: df.dropna(subset=["startYr"], inplace=True)
190/15:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
190/16:
# checking missing values in rest of the data
df.isnull().sum()
190/17: df[df.finishYr.isnull()]
190/18:
# checking the summary of the data with missing values in finishYr
df[df.finishYr.isnull()].describe(include="all").T
190/19:
df["finishYr"].fillna(2020, inplace=True)

# checking missing values in rest of the data
df.isnull().sum()
190/20:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
190/21:
# we define a function to convert the duration column to numeric


def time_to_minutes(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "hr" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            hr = (
                float(spl[0].replace("hr", "")) * 60
            )  # taking numeric part and converting hours to minutes
            mt = float(spl[1].replace("min", ""))  # taking numeric part of minutes
            return hr + mt
        else:
            return float(var.replace("min", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
190/22:
# let's apply the function to the duration column and overwrite the column
df["duration"] = df["duration"].apply(time_to_minutes)
df.head()
190/23:
# let's check the summary of the duration column
df["duration"].describe()
190/24:
df["sznOfRelease"].fillna("is_missing", inplace=True)
df.isnull().sum()
190/25: df.mediaType.value_counts()
190/26:
df.mediaType.fillna("Other", inplace=True)

# checking the number of unique values and the number of times they occur
df.mediaType.value_counts()
190/27:
cols_with_list_vals = ["studios", "tags", "contentWarn"]

for col in cols_with_list_vals:
    df[col] = (
        df[col].str.lstrip("[").str.rstrip("]")
    )  # remove the leading and trailing square braces
    df[col] = df[col].replace("", np.nan)  # mark as NaN if the value is a blank string

df.head()
190/28:
# checking missing values in rest of the data
df.isnull().sum()
190/29:
df.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
190/30:
studio_df = pd.DataFrame(
    df.studios.str.split(", ", expand=True).values.flatten(), columns=["Studios"]
)
val_c = studio_df.Studios.value_counts()
val_c
190/31:
# we take 100 as threshold
threshold = 100
val_c[val_c.values >= threshold]
190/32:
# list of studios
studios_list = val_c[val_c.values >= threshold].index.tolist()
print("Studio names taken into consideration:", len(studios_list), studios_list)
190/33:
# let us create a copy of our dataframe
df1 = df.copy()
190/34:
# first we will fill missing values in the columns by 'Others'
df1.studios.fillna("'Others'", inplace=True)
df1.studios.isnull().sum()
190/35:
studio_val = []

for i in range(df1.shape[0]):  # iterate over all rows in data
    txt = df1.studios.values[i]  # getting the values in studios column
    flag = 0  # flag variable
    for item in studios_list:  # iterate over the list of studios considered
        if item in txt and flag == 0:  # checking if studio name is in the row
            studio_val.append(item)
            flag = 1
    if flag == 0:  # if the row values is different from the list of studios considered
        studio_val.append("'Others'")

# we will strip the leading and trailing ', and assign the values to a column
df1["studio_primary"] = [item.strip("'") for item in studio_val]
df1.tail()
190/36:
# we will create a list defining whether there is a collaboration between studios
# we will check if the second split has None values, which will mean no collaboration between studios
studio_val2 = [
    0 if item is None else 1
    for item in df1.studios.str.split(", ", expand=True).iloc[:, 1]
]

df1["studios_colab"] = studio_val2
df1.tail()
190/37: df1.drop("studios", axis=1, inplace=True)
190/38:
df1.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
190/39:
tag_df = pd.DataFrame(
    df1.tags.str.split(", ", expand=True).values.flatten(), columns=["Tags"]
)
val_c = tag_df.Tags.value_counts()
val_c
190/40:
# we take 500 as threshold
threshold = 500
val_c[val_c.values >= threshold]
190/41:
# list of tags
tags_list = val_c[val_c.values >= threshold].index.tolist()
print("Tags taken into consideration:", len(tags_list), tags_list)
190/42:
# let us create a copy of our dataframe
df2 = df1.copy()
190/43:
# first we will fill missing values in the columns by 'Others'
df2.tags.fillna("Others", inplace=True)
df2.tags.isnull().sum()
190/44:
tags_df = df2.loc[:, ["title", "tags"]].copy()

for item in tags_list:
    tags_df["tag_" + item] = 0

# creating a column to denote tags other than the ones in the list
tags_df["tag_Others"] = 0

tags_df.head()
190/45: tags_df.shape
190/46:
for i in range(tags_df.shape[0]):  # iterate over all rows in data
    txt = tags_df.tags.values[i]  # getting the values in tags column
    flag = 0  # flag variable
    for item in tags_list:  # iterate over the list of tags considered
        if item in txt:  # checking if tag is in the row
            tags_df.loc[i, "tag_" + item] = 1
            flag = 1
    if flag == 0:  # if the row values is different from the list of tags considered
        tags_df.loc[i, "tag_Others"] = 1

tags_df.head()
190/47:
# concatenating the tags dataframe (except the tags and title columns) to the original data
df2 = pd.concat([df2, tags_df.iloc[:, 2:]], axis=1)
df2.head()
190/48:
df2.drop("tags", axis=1, inplace=True)
df2.shape
190/49:
df2.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
190/50:
cw_df = pd.DataFrame(
    df2.contentWarn.str.split(", ", expand=True).values.flatten(), columns=["CW"]
)
val_c = cw_df.CW.value_counts()
val_c
190/51:
df2["contentWarn"].fillna(0, inplace=True)
df2["contentWarn"] = [1 if item != 0 else 0 for item in df2.contentWarn.values]

df2["contentWarn"].value_counts()
190/52:
df2["contentWarn"].fillna(0, inplace=True)
df2["contentWarn"] = [1 if item != 0 else 0 for item in df2.contentWarn.values]

df2["contentWarn"].value_counts()
190/53:
# checking missing values in rest of the data
df2.isnull().sum()
190/54:
df3 = df2.copy()

df3[["duration", "watched"]] = df3.groupby(["studio_primary", "mediaType"])[
    ["duration", "watched"]
].transform(lambda x: x.fillna(x.median()))
df3.isnull().sum()
190/55:
df3["duration"].fillna(df3.duration.median(), inplace=True)
df3.isnull().sum()
190/56:
df3.drop(["description", "title"], axis=1, inplace=True)

# let's check the summary of our data
df3.describe(include="all").T
190/57:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
190/58: histogram_boxplot(df3, "rating")
190/59: histogram_boxplot(df3, "eps", bins=100)
190/60: histogram_boxplot(df3, "duration")
190/61: histogram_boxplot(df3, "watched", bins=50)
190/62: histogram_boxplot(df3, "years_running")
190/63:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
190/64: labeled_barplot(df3, "mediaType", perc=True)
190/65: labeled_barplot(df3, "ongoing", perc=True)
190/66: labeled_barplot(df3, "sznOfRelease", perc=True)
190/67: labeled_barplot(df3, "studio_primary", perc=True)
190/68:
# creating a list of tag columns
tag_cols = [item for item in df3.columns if "tag" in item]
190/69:
# checking the values in tag columns
for column in tag_cols:
    print(df3[column].value_counts())
    print("-" * 50)
190/70:
# creating a list of non-tag columns
corr_cols = [item for item in df3.columns if "tag" not in item]
print(corr_cols)
190/71:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df3[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
190/72:
plt.figure(figsize=(10, 5))
sns.boxplot(x="mediaType", y="rating", data=df3)
plt.show()
190/73:
plt.figure(figsize=(10, 5))
sns.boxplot(x="sznOfRelease", y="rating", data=df3)
plt.show()
190/74:
plt.figure(figsize=(15, 5))
sns.boxplot(x="studio_primary", y="rating", data=df3)
plt.xticks(rotation=90)
plt.show()
190/75:
# creating a list of non-tag columns
dist_cols = [
    item for item in df3.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
190/76:
# creating a copy of the dataframe
df4 = df3.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("contentWarn")
dist_cols.remove("studios_colab")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("rating")
190/77:
# using log transforms on some columns

for col in dist_cols:
    df4[col + "_log"] = np.log(df4[col] + 1)

# dropping the original columns
df4.drop(dist_cols, axis=1, inplace=True)
df4.head()
190/78:
# creating a list of non-tag columns
dist_cols = [
    item for item in df4.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df4[dist_cols[i]], bins=50)
    # sns.histplot(data=df4, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
190/79:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df4[dist_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
190/80:
df4.drop(["votes_log"], axis=1, inplace=True)
df4.shape
190/81:
X = df4.drop(["rating"], axis=1)
y = df4["rating"]
190/82:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
190/83:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
190/84:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
190/85:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
190/86:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
190/87:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
190/88:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
190/89:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
191/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)
191/2:
# loading the dataset
df = pd.read_csv("laptop_price.csv", engine="python")
192/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)
192/2:
# loading the dataset
df = pd.read_csv("laptop_price.csv", engine="python")
192/3:
# checking the shape of the data
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string
192/4:
# let's view a sample of the data
df.sample(n=10, random_state=1)
192/5:
# checking column datatypes and number of non-null values
df.info()
192/6:
# checking column datatypes and number of non-null values
df.info()
192/7:
# checking for missing values
df.isnull().sum()
192/8:
# Let's look at the statistical summary of the data
df.describe(include="all").T
192/9:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
192/10:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
192/11: df.Company.value_counts()
192/12: labeled_barplot(df, "Company")
192/13: df.groupby("Company")["Price_euros"].mean()
192/14:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df, y="Price_euros", x="Company")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df, y="Price_euros", x="Company")
plt.xticks(rotation=90)

plt.show()
192/15: df.groupby("TypeName")["Price_euros"].mean()
192/16:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df, y="Price_euros", x="TypeName")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df, y="Price_euros", x="TypeName")
plt.xticks(rotation=45)

plt.show()
192/17:
# let's create a copy of our data
df1 = df.copy()
192/18: df1.head()
192/19:
# defining a function to extract the amount of RAM


def ram_to_num(ram_val):
    """
    This function takes in a string representing the amount of RAM
    and converts it to a number. For example, '8GB' becomes 8.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan.
    """
    if isinstance(ram_val, str):  # checks if 'ram_val' is a string
        if ram_val.endswith("GB"):
            return float(ram_val.replace("GB", ""))
        elif ram_val.endswith("MB"):
            return (
                float(ram_val.replace("MB", "")) / 1024
            )  # converting MB to GB by dividing by 1024
    else:  # this happens when the current ram is np.nan
        return np.nan
192/20:
# extract the amount of RAM
df1["RAM_GB"] = df1["Ram"].apply(ram_to_num)
df1[["RAM_GB", "Ram"]].head()
192/21:
df1.drop("Ram", axis=1, inplace=True)
df1["RAM_GB"].describe()
192/22:
df_gaming = df1[df1.TypeName == "Gaming"]
df_gaming.groupby("Company")["RAM_GB"].mean()
192/23:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df_gaming, y="RAM_GB", x="Company")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df_gaming, y="RAM_GB", x="Company")
plt.xticks(rotation=45)

plt.show()
192/24:
# we create a new column to indicate if a laptop has the NVIDIA Geforce GTX GPU
df1["GPU_Nvidia_GTX"] = [
    1 if "Nvidia GeForce GTX" in item else 0 for item in df1["Gpu"].values
]
df1["GPU_Nvidia_GTX"].value_counts()
192/25:
df_gaming_nvidia = df1[(df1.TypeName == "Gaming") & (df1.GPU_Nvidia_GTX == 1)]
df_gaming_nvidia.groupby("Company")["Price_euros"].mean()
192/26:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df_gaming_nvidia, y="Price_euros", x="Company")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df_gaming_nvidia, y="Price_euros", x="Company")
plt.xticks(rotation=45)

plt.show()
192/27:
# checking the units of weight
weight_units = list(set([item[-2:] for item in df1.Weight]))
weight_units
192/28:
# removing the units and converting to float
df1["Weight_kg"] = df1["Weight"].str.replace("kg", "").astype(float)
df1.drop("Weight", axis=1, inplace=True)
df1["Weight_kg"].describe()
192/29:
df_ultrabook = df1[df1.TypeName == "Ultrabook"]
df_ultrabook.groupby("Company")["Weight_kg"].mean()
192/30:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df_ultrabook, y="Weight_kg", x="Company")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df_ultrabook, y="Weight_kg", x="Company")
plt.xticks(rotation=90)

plt.show()
192/31: df1.OpSys.value_counts()
192/32:
df1["OS"] = df1["OpSys"].str.split(" ").str[0]
df1["OS"].value_counts()
192/33:
df1["OS"] = [
    "NA" if item == "No" else "MacOS" if item.lower().startswith("mac") else item
    for item in df1.OS.values
]
df1["OS"].value_counts()
192/34:
df_win_small = df1[(df1.OS == "Windows") & (df1.Inches <= 14)]
df_win_small.Company.value_counts()
192/35: labeled_barplot(df_win_small, "Company", perc=True)
192/36:
df_linux_chrome = df1[(df1.OS == "Linux") | (df1.OS == "Chrome")]
df_linux_chrome.shape[0]
192/37: df_linux_chrome.groupby(["OS", "Company"]).Product.count()
192/38:
plt.figure(figsize=(12, 6))
sns.countplot(data=df_linux_chrome, x="OS", hue="Company")
plt.xticks(rotation=45)
192/39:
# extract the screen resolution
df1["ScrRes"] = df1["ScreenResolution"].str.split(" ").str[-1]
df1[["ScrRes", "ScreenResolution"]].head()
192/40:
df1.drop("ScreenResolution", axis=1, inplace=True)
df1.ScrRes.value_counts()
192/41:
df1["ScrRes_C1"] = df1.ScrRes.str[:4].astype(float)
df1["ScrRes_C1"].describe()
192/42: df1["ScrRes_C1"] = df1.ScrRes
192/43:
df1["ScrRes_C1"] = df1.ScrRes
df1.ScrRes
192/44:
df1["ScrRes_C1"] = df1.ScrRes
df1.ScrRes.str[:4]
192/45:
df1["ScrRes_C1"] = df1.ScrRes
df1.ScrRes
192/46:
df1["ScrRes_C1"] = df1.ScrRes
df1.ScrRes.str[:4]
192/47:
df1["ScrRes_C1"] = df1.ScrRes
df1.ScrRes.str[:4].astype(float)
192/48:
df_highres = df1[df1.ScrRes_C1 > 1600]
df_highres.Company.value_counts()
192/49:
df_highres = df1[df1.ScrRes_C1 >1600]
df_highres.Company.value_counts()
192/50:
df_highres = df1[df1.ScrRes_C1 > 1600]
df_highres.Company.value_counts()
192/51:
df1["ScrRes_C1"] = df1.ScrRes.str[:4].astype(float)
df1["ScrRes_C1"].describe()
192/52:
df_highres = df1[df1.ScrRes_C1 > 1600]
df_highres.Company.value_counts()
192/53: labeled_barplot(df_highres, "Company")
192/54: df_highres.Company.unique()
192/55: df.Company.unique()
192/56:
# let us compute the percentage of laptops in each company having high resolution screens
df_highres.Company.value_counts() / df[df.Company != "Fujitsu"].Company.value_counts()
192/57: df1.head()
192/58:
df1["CPU_mnfc"] = df1.Cpu.str.split().str[0]
df1["CPU_speed"] = df1.Cpu.str.split().str[-1]
192/59: df1.head()
192/60:
df1["CPU_mnfc"] = df1.Cpu.str.split().str[0]
df1["CPU_speed"] = df1.Cpu.str.split().str[-1]
192/61: df1.CPU_mnfc.value_counts()
192/62:
# checking the units of CPU speed
cpu_units = list(set([item[-3:] for item in df1.CPU_speed]))
cpu_units
192/63:
# extract the amount of RAM
df1["CPU_speed"] = df1["CPU_speed"].str.replace("GHz", "").astype(float)
df1[["CPU_speed", "Cpu"]].head()
192/64:
df_notebook = df1[df1.TypeName == "Notebook"]
df_notebook.groupby("CPU_mnfc")["CPU_speed"].mean()
192/65:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df_notebook, y="CPU_speed", x="CPU_mnfc")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df_notebook, y="CPU_speed", x="CPU_mnfc")
plt.xticks(rotation=45)

plt.show()
192/66:
np.random.seed(2)
df1.sample(10)
192/67:
np.random.seed(2)
df1.sample(10)
192/68:
df1["Memory"] = [
    item + " + NaN" if "+" not in item else item for item in df1["Memory"].values
]
df1.head()
192/69:
df1["Storage1"] = df1["Memory"].str.split("+").str[0].str.strip()
df1["Storage2"] = df1["Memory"].str.split("+").str[1].str.strip()
df1.head()
192/70:
np.random.seed(2)
df1.sample(10)
192/71:
np.random.seed(2)
df1.sample(10)
192/72:
df1["Storage1_Type"] = df1["Storage1"].str.split(" ").str[1]
df1["Storage1_Volume"] = df1["Storage1"].str.split(" ").str[0]

df1["Storage2_Type"] = df1["Storage2"].str.split(" ").str[1]
df1["Storage2_Volume"] = df1["Storage2"].str.split(" ").str[0]

df1.head()
192/73:
np.random.seed(2)
df1.sample(10)
192/74:
def storage_volume_to_num(str_vol_val):
    """This function takes in a string representing the volume of a storage device
    and converts it to a number.
    For example, '256GB' becomes 256.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(str_vol_val, str):  # checks if `str_vol_val` is a string
        multiplier = 1  # handles GB vs TB
        if str_vol_val.endswith("TB"):
            multiplier = 1024
        return float(str_vol_val.replace("GB", "").replace("TB", "")) * multiplier
    else:  # this happens when the str_vol is np.nan
        return np.nan
192/75:
df1["Storage1_Volume"] = df1["Storage1_Volume"].apply(storage_volume_to_num)
df1["Storage2_Volume"] = df1["Storage2_Volume"].apply(storage_volume_to_num)

df1.head()
192/76:
np.random.seed(2)
df1.sample(10)
192/77:
df_apple = df1[df1.Company == "Apple"]
df_apple[["Storage1_Volume", "Storage2_Volume"]].describe()
192/78: df_apple["Storage1_Type"].value_counts()
192/79: df_apple["Storage2_Type"].value_counts()
194/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
194/2:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
194/3:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
194/4:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
194/5:
#dropping/ignoring car_name 
cData = cData.drop('car name', axis=1)
# Also replacing the categorical var with actual values
cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
cData.head()
194/6:
#dropping/ignoring car_name 
cData = cData.drop('car name', axis=1)
# Also replacing the categorical var with actual values
cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
cData.head()
194/7:
cData = pd.get_dummies(cData, columns=['origin'])
cData.head()
195/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
195/2:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
195/3:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
195/4:
#dropping/ignoring car_name 
cData = cData.drop('car name', axis=1)
# Also replacing the categorical var with actual values
cData['origin'] = cData['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})
cData.head()
195/5:
cData = pd.get_dummies(cData, columns=['origin'])
cData.head()
195/6:
#A quick summary of the data columns
cData.describe()
195/7:
# hp is missing cause it does not seem to be reqcognized as a numerical column!
cData.dtypes
195/8:
# isdigit()? on 'horsepower' 
hpIsDigit = pd.DataFrame(cData.horsepower.str.isdigit())  # if the string is made of digits store True else False

#print isDigit = False!
cData[hpIsDigit['horsepower'] == False]   # from temp take only those rows where hp has false
195/9:
# Missing values have a'?''
# Replace missing values with NaN
cData = cData.replace('?', np.nan)
cData[hpIsDigit['horsepower'] == False]
195/10:
#instead of dropping the rows, lets replace the missing values with median value. 
cData.median()
195/11:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
cData = cData.apply(medianFiller,axis=0)

cData['horsepower'] = cData['horsepower'].astype('float64')  # converting the hp column from object / string type to float
195/12:
cData_attr = cData.iloc[:, 0:7]
sns.pairplot(cData_attr, diag_kind='kde')   # to plot density curve instead of histogram on the diag
195/13:
# lets build our linear model
# independant variables
X = cData.drop(['mpg','origin_europe'], axis=1)
# the dependent variable
y = cData[['mpg']]
195/14:
# Split X and y into training and test set in 70:30 ratio

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)
195/15:
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)
195/16:
for idx, col_name in enumerate(X_train.columns):
    print("The coefficient for {} is {}".format(col_name, regression_model.coef_[0][idx]))
195/17:
intercept = regression_model.intercept_[0]
print("The intercept for our model is {}".format(intercept))
195/18: regression_model.score(X_train, y_train)
195/19:
#out of sample score (R^2)

regression_model.score(X_test, y_test)
195/20:
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train2 = poly.fit_transform(X_train)
X_test2 = poly.fit_transform(X_test)

poly_clf = linear_model.LinearRegression()

poly_clf.fit(X_train2, y_train)

y_pred = poly_clf.predict(X_test2)

#print(y_pred)

#In sample (training) R^2 will always improve with the number of variables!
print(poly_clf.score(X_train2, y_train))
195/21:
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_train2 = poly.fit_transform(X_train)
X_test2 = poly.fit_transform(X_test)

poly_clf = linear_model.LinearRegression()

poly_clf.fit(X_train2, y_train)

y_pred = poly_clf.predict(X_test2)

#print(y_pred)

#In sample (training) R^2 will always improve with the number of variables!
print(poly_clf.score(X_train2, y_train))
195/22:
#Out off sample (testing) R^2 is our measure of sucess and does improve
print(poly_clf.score(X_test2, y_test))
195/23:
# but this improves as the cost of 29 extra variables!
print(X_train.shape)
print(X_train2.shape)
197/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
197/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
197/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
197/4: df.info()  # down to 82 columns after the initial 88
197/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
197/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
197/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
197/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
197/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
197/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
197/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
197/12: df["Work Rate"].head()
197/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
197/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
197/15: df.head(2)
197/16: df.head(2)
197/17:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
197/18:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
197/19: df.info()
197/20: df.describe().T  # quick summary of numeric features
197/21:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
197/22: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
197/23:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
197/24: df['Contract Valid Until'].value_counts(dropna=False)
197/25:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
197/26:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
197/27:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
198/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
198/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
198/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
198/4: df.info()  # down to 82 columns after the initial 88
198/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
198/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
198/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
198/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
198/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
198/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
198/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
198/12: df["Work Rate"].head()
198/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
198/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
198/15: df.head(2)
198/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
198/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
198/18: df.info()
198/19: df.describe().T  # quick summary of numeric features
198/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
198/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
198/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
198/23: df['Contract Valid Until'].value_counts(dropna=False)
198/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
198/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
199/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
199/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
199/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
199/4: df.info()  # down to 82 columns after the initial 88
199/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
199/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
199/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
199/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
199/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
199/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
199/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
199/12: df["Work Rate"].head()
199/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
199/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
199/15: df.head(2)
199/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
199/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
199/18: df.info()
199/19: df.describe().T  # quick summary of numeric features
199/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
199/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
199/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
199/23: df['Contract Valid Until'].value_counts(dropna=False)
199/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
199/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
201/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
201/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
201/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
201/4: df.info()  # down to 82 columns after the initial 88
201/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
201/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
201/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
201/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
201/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
201/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
201/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
201/12: df["Work Rate"].head()
201/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
201/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
201/15: df.head(2)
201/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
201/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
201/18: df.info()
201/19: df.describe().T  # quick summary of numeric features
201/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
201/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
201/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
201/23: df['Contract Valid Until'].value_counts(dropna=False)
201/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
201/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
201/26: df.drop(['Release Clause'], axis=1, inplace=True)
201/27:
cols_to_log = ['Wage', 'Value']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
201/28:
plt.hist(np.log(df['Wage'] + 1), 50)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
201/29:
for colname in cols_to_log:
    df[colname + '_log'] = np.log(df[colname] + 1)
df.drop(cols_to_log, axis=1, inplace=True)
201/30:
# Height is in inches
binned_ht = pd.cut(df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf])
binned_ht
201/31: binned_ht.value_counts(dropna=False)
201/32:
# can add custom labels
df['height_bin'] = pd.cut(
    df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf], 
    labels = ["Under 5'", "5' to 5'6", "5'6 to 6'", "Over 6'"]
)
df.drop(['Height'], axis=1, inplace=True)
df['height_bin'].value_counts(dropna=False)
201/33:
print(df['Weight'].head(2))
df['Weight'] = df['Weight'].apply(lambda wt: round(wt * 0.4535, 2))
df['Weight'].head(2)
201/34:
cat_vars = ['Preferred Foot', 'Body Type', 'Position',
            'Workrate_attack', 'Workrate_defense']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df[colname] = df[colname].astype('category')
    
df.info()
201/35:
# how many players are in clubs that start with 'FC'?
df['Club'].str.startswith('FC').sum()
201/36:
# how many letters and words in the unique club names?

# doing i == i as a quick check for NaNs
# using .title() in case of capitalization issues
club_data = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in df['Club'].str.strip().str.title().unique()],
    columns = ['Club', 'Number of Letters', 'Number of Words']
)
club_data.head()
201/37:
club_data['Number of Letters'].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

club_data['Number of Words'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()

print('These are the clubs with the most words in the name:')
club_data.loc[club_data['Number of Words'] == club_data['Number of Words'].max(), 'Club']
201/38: from sklearn.preprocessing import StandardScaler, MinMaxScaler
201/39:
std_scaler = StandardScaler()

df['Weight'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['Weight']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['Weight'], axis=1, inplace=True)
201/40:
# replacing with scaled 
df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate before minmax scaling')
plt.show()

df[['Attack_rate', 'Midfield_rate', 'Defense_rate']] = MinMaxScaler().fit_transform(
    df[['Attack_rate', 'Midfield_rate', 'Defense_rate']]
)

df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate after minmax scaling')
plt.show()

# if the minimum and maximum are treated as fixed, this is also a linear transformation
# so the shape is the same
201/41: df.isnull().sum() # lots of columns don't have missingness
201/42: df.isnull().sum() # lots of columns don't have missingness
201/43:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
201/44: df.drop(['Loaned From'], axis=1, inplace=True)
201/45:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
201/46:
# these are missing `Joined` and `Joined year`
df[num_missing == 2].sample(n=5)
201/47:
# these are missing `Attack_rate`, `Midfield_rate`, and `Defense_rate`
df[num_missing == 3].sample(n=5)
201/48:
for n in num_missing.value_counts().sort_index().index:
    if n > 0:
        print(f'For the rows with exactly {n} missing values, NAs are found in:')
        n_miss_per_col = df[num_missing == n].isnull().sum()
        print(n_miss_per_col[n_miss_per_col > 0])
        print('\n\n')
201/49:
# nans are floats so they become strings here
# we also need this to be strings because we're adding a category that's not present
df['height_bin'] = df['height_bin'].astype(str).replace('nan', 'is_missing').astype('category')
201/50:
# nans are floats so they become strings here
# we also need this to be strings because we're adding a category that's not present
df['height_bin'] = df['height_bin'].astype(str).replace('nan', 'is_missing').astype('category')
201/51:
# now using `fillna` with a numeric column
print(df['Passing'].isnull().sum())
df['Passing'].fillna(df['Passing'].mean(), inplace=True)  # mean imputation
df['Passing'].isnull().sum()
201/52: pd.get_dummies(df['height_bin'], drop_first=True)
201/53:
# can do one hot encoding with get_dummies
pd.get_dummies(df['height_bin'], drop_first=False).iloc[:10, :]
201/54:
# or we can use sklearn
from sklearn.preprocessing import OneHotEncoder

OneHotEncoder(sparse=False).fit_transform(df[['height_bin']])[:10,:]
204/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
204/2:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
204/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
204/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
201/55:
def z_transform(x):
    return (x - np.mean(x)) / np.std(x)

np.random.seed(1)
x1 = np.random.normal(size=1000)
x2 = np.random.lognormal(size=1000)


plt.hist(z_transform(x1))
plt.title('z-transformed normal data')
plt.show()


plt.hist(z_transform(x2))
plt.title('z-transformed lognormal data')
plt.show()
201/56:
def frac_outside_1pt5_IQR(x):
    length = 1.5 * np.diff(np.quantile(x, [.25, .75]))
    return np.mean(np.abs(x - np.median(x)) > length)

print(frac_outside_1pt5_IQR(x1))
print(frac_outside_1pt5_IQR(x2))
201/57:
plt.hist(df['Power'], 20)
plt.title('Histogram of Power')
plt.show()

sns.boxplot(df['Power'])
plt.title('Boxplot of Power')
plt.show()
201/58:
quartiles = np.quantile(df['Power'][df['Power'].notnull()], [.25, .75])
power_4iqr = 4 * (quartiles[1] - quartiles[0])
print(f'Q1 = {quartiles[0]}, Q3 = {quartiles[1]}, 4*IQR = {power_4iqr}')
outlier_powers = df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power']
outlier_powers
201/59:
# making the situation more extreme
df['Power'].hist(bins=20)
plt.title('Power before exaggerating the outliers')
plt.show()
print(df['Power'].mean())
df.loc[outlier_powers.index, 'Power'] = [-200000.0, -1200000.0]
df['Power'].hist(bins=20)
plt.title('Power after exaggerating outliers')
plt.show()
201/60:
# if we wanted to make these NA we could just do this
# [not run]
df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power'] = np.nan
201/61:
# dropping these rows
# [not run]
df.drop(outlier_powers.index, axis=0, inplace=True)
201/62:
power = df['Power'][df['Power'].notnull()]

print(power.mean())  # the mean is being pulled
print(power.median())
201/63:
from scipy.stats import tmean

print(tmean(power, limits=np.quantile(power, [.1, .9])))
print(tmean(power, limits=[0,100]))
206/1:
import pandas as pd
import numpy as np
206/2:
x = [4, 5, 8, 10, 11]

y =  [50, 48, 45 , 42, 41]
206/3:
from scipy.stats import pearsonr
corr, pvalue= pearsonr(x,y)
print(corr)
206/4:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/5: data.mean(axis=1)
206/6: np.mean(data, axis=1)
206/7:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/8: data["Col4"].replace("Nature","Beauty", inplace=False)
206/9: data["Col4"].str.replace("Nature","Beauty")
206/10:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/11: data["Col5"] = data[["Col1","Col2","Col3"]].sum(axis=1)
206/12: data
206/13:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/14: data["Col6"] = data["Col3"] - data["Col1"]
206/15: data
206/16:
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/17: data.drop(["Col2"],axis=1)
206/18: data
206/19:
import pandas as pd
import numpy as np
data = pd.DataFrame({"Col1": [100,200,300,400], "Col2":[500,600,700,800], "Col3":[900,1000,1100,1200], "Col4":["Nature","Wildlife","Animals","Humans"]})
data
206/20: data['Col1'].apply(lambda x : x*5)
206/21:
data = [2, 5, 12, 15, 19, 4, 6, 11, 16, 18, 12, 12, 42, 6, 56, 34, 23, 11]
data
206/22: np.quantile(data, [.25, .75])
206/23:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
206/24:
# loading the dataset
data = pd.read_csv("insurance.csv")
206/25:
# loading the dataset

df = pd.read_csv("insurance.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
206/26: df.isnull().sum()
206/27:
# loading the dataset

df = pd.read_csv("insurance.csv")
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
206/28: df.isnull().sum()
204/5:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
204/6:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
204/7:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
204/8:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
204/9:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
204/10:
# checking for duplicate values in the data
df.duplicated().sum()
204/11:
# checking the names of the columns in the data
print(df.columns)
204/12:
# checking column datatypes and number of non-null values
df.info()
204/13:
# checking for missing values in the data.
df.isnull().sum()
204/14:
# Let's look at the statistical summary of the data
df.describe(include="all").T
204/15: df.dropna(subset=["rating"], inplace=True)
204/16:
# checking missing values in rest of the data
df.isnull().sum()
204/17: df[df.startYr.isnull()]
204/18: df.dropna(subset=["startYr"], inplace=True)
204/19:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
204/20:
# checking missing values in rest of the data
df.isnull().sum()
204/21: df[df.finishYr.isnull()]
204/22:
# checking the summary of the data with missing values in finishYr
df[df.finishYr.isnull()].describe(include="all").T
204/23:
df["finishYr"].fillna(2020, inplace=True)

# checking missing values in rest of the data
df.isnull().sum()
204/24:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
204/25:
# we define a function to convert the duration column to numeric


def time_to_minutes(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "hr" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            hr = (
                float(spl[0].replace("hr", "")) * 60
            )  # taking numeric part and converting hours to minutes
            mt = float(spl[1].replace("min", ""))  # taking numeric part of minutes
            return hr + mt
        else:
            return float(var.replace("min", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
204/26:
# let's apply the function to the duration column and overwrite the column
df["duration"] = df["duration"].apply(time_to_minutes)
df.head()
204/27:
# let's check the summary of the duration column
df["duration"].describe()
204/28:
df["sznOfRelease"].fillna("is_missing", inplace=True)
df.isnull().sum()
204/29: df.mediaType.value_counts()
204/30:
df.mediaType.fillna("Other", inplace=True)

# checking the number of unique values and the number of times they occur
df.mediaType.value_counts()
204/31:
cols_with_list_vals = ["studios", "tags", "contentWarn"]

for col in cols_with_list_vals:
    df[col] = (
        df[col].str.lstrip("[").str.rstrip("]")
    )  # remove the leading and trailing square braces
    df[col] = df[col].replace("", np.nan)  # mark as NaN if the value is a blank string

df.head()
204/32:
# checking missing values in rest of the data
df.isnull().sum()
204/33:
df.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
204/34:
studio_df = pd.DataFrame(
    df.studios.str.split(", ", expand=True).values.flatten(), columns=["Studios"]
)
val_c = studio_df.Studios.value_counts()
val_c
204/35:
# we take 100 as threshold
threshold = 100
val_c[val_c.values >= threshold]
204/36:
# list of studios
studios_list = val_c[val_c.values >= threshold].index.tolist()
print("Studio names taken into consideration:", len(studios_list), studios_list)
204/37:
# let us create a copy of our dataframe
df1 = df.copy()
204/38:
# first we will fill missing values in the columns by 'Others'
df1.studios.fillna("'Others'", inplace=True)
df1.studios.isnull().sum()
204/39:
studio_val = []

for i in range(df1.shape[0]):  # iterate over all rows in data
    txt = df1.studios.values[i]  # getting the values in studios column
    flag = 0  # flag variable
    for item in studios_list:  # iterate over the list of studios considered
        if item in txt and flag == 0:  # checking if studio name is in the row
            studio_val.append(item)
            flag = 1
    if flag == 0:  # if the row values is different from the list of studios considered
        studio_val.append("'Others'")

# we will strip the leading and trailing ', and assign the values to a column
df1["studio_primary"] = [item.strip("'") for item in studio_val]
df1.tail()
204/40:
# we will create a list defining whether there is a collaboration between studios
# we will check if the second split has None values, which will mean no collaboration between studios
studio_val2 = [
    0 if item is None else 1
    for item in df1.studios.str.split(", ", expand=True).iloc[:, 1]
]

df1["studios_colab"] = studio_val2
df1.tail()
204/41: df1.drop("studios", axis=1, inplace=True)
204/42:
df1.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
204/43:
tag_df = pd.DataFrame(
    df1.tags.str.split(", ", expand=True).values.flatten(), columns=["Tags"]
)
val_c = tag_df.Tags.value_counts()
val_c
204/44:
# we take 500 as threshold
threshold = 500
val_c[val_c.values >= threshold]
204/45:
# list of tags
tags_list = val_c[val_c.values >= threshold].index.tolist()
print("Tags taken into consideration:", len(tags_list), tags_list)
204/46:
# let us create a copy of our dataframe
df2 = df1.copy()
204/47:
# first we will fill missing values in the columns by 'Others'
df2.tags.fillna("Others", inplace=True)
df2.tags.isnull().sum()
204/48:
tags_df = df2.loc[:, ["title", "tags"]].copy()

for item in tags_list:
    tags_df["tag_" + item] = 0

# creating a column to denote tags other than the ones in the list
tags_df["tag_Others"] = 0

tags_df.head()
204/49: tags_df.shape
204/50:
for i in range(tags_df.shape[0]):  # iterate over all rows in data
    txt = tags_df.tags.values[i]  # getting the values in tags column
    flag = 0  # flag variable
    for item in tags_list:  # iterate over the list of tags considered
        if item in txt:  # checking if tag is in the row
            tags_df.loc[i, "tag_" + item] = 1
            flag = 1
    if flag == 0:  # if the row values is different from the list of tags considered
        tags_df.loc[i, "tag_Others"] = 1

tags_df.head()
204/51:
# concatenating the tags dataframe (except the tags and title columns) to the original data
df2 = pd.concat([df2, tags_df.iloc[:, 2:]], axis=1)
df2.head()
204/52:
df2.drop("tags", axis=1, inplace=True)
df2.shape
204/53:
df2.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
204/54:
cw_df = pd.DataFrame(
    df2.contentWarn.str.split(", ", expand=True).values.flatten(), columns=["CW"]
)
val_c = cw_df.CW.value_counts()
val_c
204/55:
df2["contentWarn"].fillna(0, inplace=True)
df2["contentWarn"] = [1 if item != 0 else 0 for item in df2.contentWarn.values]

df2["contentWarn"].value_counts()
204/56:
# checking missing values in rest of the data
df2.isnull().sum()
204/57:
df3 = df2.copy()

df3[["duration", "watched"]] = df3.groupby(["studio_primary", "mediaType"])[
    ["duration", "watched"]
].transform(lambda x: x.fillna(x.median()))
df3.isnull().sum()
204/58:
df3["duration"].fillna(df3.duration.median(), inplace=True)
df3.isnull().sum()
204/59:
df3.drop(["description", "title"], axis=1, inplace=True)

# let's check the summary of our data
df3.describe(include="all").T
204/60:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
204/61: histogram_boxplot(df3, "rating")
204/62: histogram_boxplot(df3, "eps", bins=100)
204/63: histogram_boxplot(df3, "duration")
204/64: histogram_boxplot(df3, "watched", bins=50)
204/65: histogram_boxplot(df3, "years_running")
204/66:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
204/67: labeled_barplot(df3, "mediaType", perc=True)
204/68: labeled_barplot(df3, "ongoing", perc=True)
204/69: labeled_barplot(df3, "sznOfRelease", perc=True)
204/70: labeled_barplot(df3, "studio_primary", perc=True)
204/71:
# creating a list of tag columns
tag_cols = [item for item in df3.columns if "tag" in item]
204/72:
# checking the values in tag columns
for column in tag_cols:
    print(df3[column].value_counts())
    print("-" * 50)
204/73:
# creating a list of non-tag columns
corr_cols = [item for item in df3.columns if "tag" not in item]
print(corr_cols)
204/74:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df3[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
204/75:
plt.figure(figsize=(10, 5))
sns.boxplot(x="mediaType", y="rating", data=df3)
plt.show()
204/76:
plt.figure(figsize=(10, 5))
sns.boxplot(x="sznOfRelease", y="rating", data=df3)
plt.show()
204/77:
plt.figure(figsize=(15, 5))
sns.boxplot(x="studio_primary", y="rating", data=df3)
plt.xticks(rotation=90)
plt.show()
204/78:
# creating a list of non-tag columns
dist_cols = [
    item for item in df3.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
204/79:
# creating a copy of the dataframe
df4 = df3.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("contentWarn")
dist_cols.remove("studios_colab")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("rating")
204/80:
# using log transforms on some columns

for col in dist_cols:
    df4[col + "_log"] = np.log(df4[col] + 1)

# dropping the original columns
df4.drop(dist_cols, axis=1, inplace=True)
df4.head()
204/81:
# creating a list of non-tag columns
dist_cols = [
    item for item in df4.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df4[dist_cols[i]], bins=50)
    # sns.histplot(data=df4, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
204/82:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df4[dist_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
204/83:
df4.drop(["votes_log"], axis=1, inplace=True)
df4.shape
204/84:
X = df4.drop(["rating"], axis=1)
y = df4["rating"]
204/85:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
204/86:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
204/87:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
204/88:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
204/89:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
204/90:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
204/91:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
204/92:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
201/64:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
201/65:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
201/66:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
201/67:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
201/68:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
207/1:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
# This is so I can see the entire dataframe when I print it
pd.set_option('display.max_columns', None)
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_rows', 200)
207/2:
df = pd.read_csv("FIFA2019.csv", index_col=0)
print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
207/3: df.drop(['ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Jersey Number'],axis=1,inplace=True)
207/4: df.info()  # down to 82 columns after the initial 88
207/5:
# looking at which columns have the most missing values
df.isnull().sum().sort_values(ascending=False)
207/6:
# this loop prints the names of the columns where there is
# at least one entry beginning the character ''
money_cols = []
for colname in df.columns[df.dtypes == 'object']:  # only need to consider string columns
    if df[colname].str.startswith('').any():  # using `.str` so I can use an element-wise string method
        money_cols.append(colname)
print(money_cols)
207/7:
def income_to_num(income_val):
    """This function takes in a string representing a salary in Euros
    and converts it to a number. For example, '220K' becomes 220000.
    If the input is already numeric, which probably means it's NaN,
    this function just returns np.nan."""
    if isinstance(income_val, str):  # checks if `income_val` is a string
        multiplier = 1  # handles K vs M salaries
        if income_val.endswith('K'):
            multiplier = 1000
        elif income_val.endswith('M'):
            multiplier = 1000000
        return float(income_val.replace('', '').replace('K', '').replace('M', '')) * multiplier
    else:  # this happens when the current income is np.nan
        return np.nan

for colname in money_cols:
    df[colname] = df[colname].apply(income_to_num)
    
df[money_cols].head()  # good to go!
207/8:
def position_to_num(pos_val):
    """For each value, take the number before the '+'
    unless it is not a string value. This will only happen
    for NaNs so in that case we just return NaN.
    """
    if isinstance(pos_val, str):
        return float(pos_val.split('+')[0])
    else:
        return np.nan

position_cols = [
    'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW',
    'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM',
    'RM', 'LWB', 'LDM', 'CDM', 'RDM', 'RWB', 'LB',
    'LCB', 'CB', 'RCB', 'RB'
]

for colname in position_cols:
    df[colname] = df[colname].apply(position_to_num)
207/9:
def height_to_num(height):
    """Converts a height of the form 5'11 (i.e. feet then inches) to inches
    and makes it an integer. Non-string heights are treated as missing."""
    if isinstance(height, str):
        splt = height.split("'")
        return float(splt[0]) * 12 + float(splt[1])
    else:
        return np.nan
    
    
def weight_to_num(weight):
    """In the weight column I'm replacing the terminal 'lbs' with
    the empty string and converting to a float. Non-strings are 
    np.nans and are kept as np.nans."""
    if isinstance(weight, str):
        return float(weight.replace('lbs', ''))
    else:
        return np.nan
    
    
    
# I could just do this by copy-pasting one line and editing which column and
# which function I'm using for each one in turn. With only two columns
# that's not so bad, but it gets cumbersome quickly

# df['Height'] = df['Height'].apply(height_to_num)
# df['Weight'] = df['Weight'].apply(weight_to_num)

# A more general way is to collect the columns and column-processing functions
# into a data structure and loop over that. That avoids bugs like forgetting to
# change the second 'Height' into 'Weight' when I copy-paste the first line.
# Here, the keys of the dictionary are the column names and the values are 
# the function that I'll use to replace that column's values. I now don't
# have to worry about mixing up column names and processing functions.
col_transforms = {
    'Height': height_to_num,
    'Weight': weight_to_num
}

# k is the key, so the column name here
# v is the value, which a function in this case and is
#     either `height_to_num` or `weight_to_num`
for k,v in col_transforms.items():
    df[k] = df[k].map(v)
207/10:
df['Joined'] = pd.to_datetime(df['Joined'])
df['Joined year'] = df['Joined'].dt.year  # adding in a feature that's just the year
print(min(df['Joined']), max(df['Joined']))
df['Joined'].head()
207/11:
# investigating the players with this earliest Joined date
df[df['Joined'] == min(df['Joined'])]
207/12: df["Work Rate"].head()
207/13:
workrt = df["Work Rate"].str.split("/ ", n = 1, expand = True) 
workrt.head()
207/14:
df.drop(['Work Rate'], axis=1, inplace=True)
df["Workrate_attack"]= workrt[0]   
df["Workrate_defense"]= workrt[1]

del workrt  # don't need to do this but can keep things tidy
207/15: df.head(2)
207/16:
# replacing the position columns with attack, midfield, and defense averages
positiontype_to_cols = {
    'Attack': ['LS', 'ST', 'RS', 'LF', 'CF', 'RF'],
    'Midfield': ['LW', 'RW', 'LAM', 'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'CDM', 'RDM', 'LDM'],
    'Defense': ['LWB', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']
}

for pos_type, colvec in positiontype_to_cols.items():
    df[pos_type + '_rate'] = round(df[colvec].mean(axis=1))

# we've summarized these and are content with these aggregates so we drop these columns
df.drop(position_cols, axis=1, inplace=True)

print(df.shape)  # down to 61 columns
207/17:
summaryname_to_cols = {
    'Passing': ['Crossing', 'ShortPassing', 'LongPassing'],
    'Shooting': ['Finishing', 'Volleys', 'FKAccuracy', 'ShotPower',
                 'LongShots', 'Penalties', 'HeadingAccuracy'],
    'Defending': ['Marking', 'StandingTackle', 'SlidingTackle', 'Interceptions'],
    'Speed': ['SprintSpeed', 'Agility', 'Acceleration', 'Reactions', 'Stamina'],
    'Control': ['BallControl', 'Curve', 'Dribbling'],
    'GoalKeeping': ['GKDiving', 'GKHandling', 'GKPositioning', 'GKKicking', 'GKReflexes'],
    'Mental': ['Composure', 'Vision', 'Aggression', 'Positioning'],
    'Power': ['Strength', 'Balance', 'Jumping'],
    'Avg_rating': ['Overall', 'Potential']
    
}

for summarycol, colvec in summaryname_to_cols.items():
    df[summarycol] = round(df[colvec].mean(axis=1))
    
# now I don't have a vector that contains all of these old columns in one so I need to make it
# this is a "nested list comprehension" and is a common way to flatten a list of lists,
# which is what summaryname_to_cols.values() effectively is, so that the result is a 
# single list. The syntax says to loop colvec through each element of summaryname_to_cols.values(),
# and then within each iteration of that loop we loop colname through colvec and we simply
# keep colvec without doing anything else to it.
cols_to_drop = [colname for colvec in summaryname_to_cols.values() for colname in colvec]
df.drop(cols_to_drop, axis=1, inplace=True)
print(df.shape)  # big reduction in size. Getting much more manageable
df.head(2)
207/18: df.info()
207/19: df.describe().T  # quick summary of numeric features
207/20:
# looking at value counts for non-numeric features

num_to_display = 10  # defining this up here so it's easy to change later if I want
for colname in df.dtypes[df.dtypes == 'object'].index:
    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts
    print(val_counts[:num_to_display])
    if len(val_counts) > num_to_display:
        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')
    print('\n\n') # just for more space between
207/21: df[df['Body Type'] == 'PLAYER_BODY_TYPE_25']
207/22:
df.loc[df['Body Type'] == 'PLAYER_BODY_TYPE_25', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Neymar', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Shaqiri', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Messi', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'C. Ronaldo', 'Body Type'] = 'Stocky'
df.loc[df['Body Type'] == 'Courtois', 'Body Type'] = 'Lean'
df.loc[df['Body Type'] == 'Akinfenwa', 'Body Type'] = 'Stocky'
# why do these work with NaNs?

df['Body Type'].value_counts(dropna=False)
207/23: df['Contract Valid Until'].value_counts(dropna=False)
207/24:
contract_dates = pd.to_datetime(df['Contract Valid Until']).dt.year
print(contract_dates.value_counts(dropna=False))
df['Contract Valid Until'] = contract_dates
207/25:
df['is_GK'] = df['Position'] == 'GK'  # for hue
cols_to_exclude = ['International Reputation', 'Weak Foot', 'Skill Moves']
sns.pairplot(df[[colname for colname in df.columns if colname not in cols_to_exclude]], hue = 'is_GK')
df.drop(['is_GK'], axis=1, inplace=True)
206/29:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
206/30:
# loading the dataset

data = pd.read_csv("insurance.csv")
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
206/31: data.isnull().sum()
206/32:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
206/33: histogram_boxplot(df3, "charges")
206/34: histogram_boxplot(data, "charges")
206/35: histogram_boxplot(data, "bmi")
206/36:
plt.hist(np.log(df['bmi'] + 1), 50)
plt.title('log(bmi + 1)')
plt.show()
208/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)
206/37: df.groupby("region")["charges"].median()
206/38: np.corrcoef('age', 'charges')
206/39: df.corrcoef("age", "charges")
206/40:
df3 =np.corr("age", "charges")
df3
207/26: df.drop(['Release Clause'], axis=1, inplace=True)
207/27:
cols_to_log = ['Wage', 'Value']
for colname in cols_to_log:
    plt.hist(df[colname], bins=50)
    plt.title(colname)
    plt.show()
    print(np.sum(df[colname] <= 0))
207/28:
plt.hist(np.log(df['Wage'] + 1), 50)
plt.title('log(Wage + 1)')
plt.show()
plt.hist(np.arcsinh(df['Wage']), 50)
plt.title('arcsinh(Wage)')
plt.show()
plt.hist(np.sqrt(df['Wage']), 50)
plt.title('sqrt(Wage)')
plt.show()
207/29:
for colname in cols_to_log:
    df[colname + '_log'] = np.log(df[colname] + 1)
df.drop(cols_to_log, axis=1, inplace=True)
207/30:
# Height is in inches
binned_ht = pd.cut(df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf])
binned_ht
207/31: binned_ht.value_counts(dropna=False)
207/32:
# can add custom labels
df['height_bin'] = pd.cut(
    df['Height'], [-np.inf, 5*12, 5*12+6, 6*12, np.inf], 
    labels = ["Under 5'", "5' to 5'6", "5'6 to 6'", "Over 6'"]
)
df.drop(['Height'], axis=1, inplace=True)
df['height_bin'].value_counts(dropna=False)
207/33:
print(df['Weight'].head(2))
df['Weight'] = df['Weight'].apply(lambda wt: round(wt * 0.4535, 2))
df['Weight'].head(2)
207/34:
cat_vars = ['Preferred Foot', 'Body Type', 'Position',
            'Workrate_attack', 'Workrate_defense']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df[colname] = df[colname].astype('category')
    
df.info()
207/35:
# how many players are in clubs that start with 'FC'?
df['Club'].str.startswith('FC').sum()
207/36:
# how many letters and words in the unique club names?

# doing i == i as a quick check for NaNs
# using .title() in case of capitalization issues
club_data = pd.DataFrame(
    data = [(i, len(i), len(i.split())) if i == i else (i, 0, 0)
            for i in df['Club'].str.strip().str.title().unique()],
    columns = ['Club', 'Number of Letters', 'Number of Words']
)
club_data.head()
207/37:
club_data['Number of Letters'].value_counts().plot.bar()
plt.title('Distribution of Number of Letters')
plt.show()

club_data['Number of Words'].value_counts().plot.bar()
plt.title('Distribution of Number of Words')
plt.show()

print('These are the clubs with the most words in the name:')
club_data.loc[club_data['Number of Words'] == club_data['Number of Words'].max(), 'Club']
207/38: from sklearn.preprocessing import StandardScaler, MinMaxScaler
207/39:
std_scaler = StandardScaler()

df['Weight'].hist(bins=20)
plt.title('Weight before z transformation')
plt.show()
# fit_transform requires a DataFrame, not a Series, hence
# the double brackets to keep df[['Weight']] as a 1 column 
# DataFrame rather than a Series, like if I did df['Weight']


df['Weight_z_std'] = std_scaler.fit_transform(df[['Weight']])
df['Weight_z_std'].hist(bins=20)
plt.title('Weight after z transformation')
plt.show()
# exact same shape since it's a linear transformation.
df.drop(['Weight'], axis=1, inplace=True)
207/40:
# replacing with scaled 
df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate before minmax scaling')
plt.show()

df[['Attack_rate', 'Midfield_rate', 'Defense_rate']] = MinMaxScaler().fit_transform(
    df[['Attack_rate', 'Midfield_rate', 'Defense_rate']]
)

df['Attack_rate'].hist(bins=20)
plt.title('Attack_rate after minmax scaling')
plt.show()

# if the minimum and maximum are treated as fixed, this is also a linear transformation
# so the shape is the same
207/41: df.isnull().sum() # lots of columns don't have missingness
207/42:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
207/43: df.drop(['Loaned From'], axis=1, inplace=True)
207/44:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
207/45:
# these are missing `Joined` and `Joined year`
df[num_missing == 2].sample(n=5)
207/46:
# these are missing `Attack_rate`, `Midfield_rate`, and `Defense_rate`
df[num_missing == 3].sample(n=5)
207/47:
for n in num_missing.value_counts().sort_index().index:
    if n > 0:
        print(f'For the rows with exactly {n} missing values, NAs are found in:')
        n_miss_per_col = df[num_missing == n].isnull().sum()
        print(n_miss_per_col[n_miss_per_col > 0])
        print('\n\n')
207/48:
# nans are floats so they become strings here
# we also need this to be strings because we're adding a category that's not present
df['height_bin'] = df['height_bin'].astype(str).replace('nan', 'is_missing').astype('category')
207/49:
# now using `fillna` with a numeric column
print(df['Passing'].isnull().sum())
df['Passing'].fillna(df['Passing'].mean(), inplace=True)  # mean imputation
df['Passing'].isnull().sum()
207/50: pd.get_dummies(df['height_bin'], drop_first=True)
207/51:
# can do one hot encoding with get_dummies
pd.get_dummies(df['height_bin'], drop_first=False).iloc[:10, :]
206/41: pd.get_dummies(df['region'], drop_first=True)
207/52:
# or we can use sklearn
from sklearn.preprocessing import OneHotEncoder

OneHotEncoder(sparse=False).fit_transform(df[['height_bin']])[:10,:]
207/53:
def z_transform(x):
    return (x - np.mean(x)) / np.std(x)

np.random.seed(1)
x1 = np.random.normal(size=1000)
x2 = np.random.lognormal(size=1000)


plt.hist(z_transform(x1))
plt.title('z-transformed normal data')
plt.show()


plt.hist(z_transform(x2))
plt.title('z-transformed lognormal data')
plt.show()
207/54:
def frac_outside_1pt5_IQR(x):
    length = 1.5 * np.diff(np.quantile(x, [.25, .75]))
    return np.mean(np.abs(x - np.median(x)) > length)

print(frac_outside_1pt5_IQR(x1))
print(frac_outside_1pt5_IQR(x2))
207/55:
plt.hist(df['Power'], 20)
plt.title('Histogram of Power')
plt.show()

sns.boxplot(df['Power'])
plt.title('Boxplot of Power')
plt.show()
207/56:
quartiles = np.quantile(df['Power'][df['Power'].notnull()], [.25, .75])
power_4iqr = 4 * (quartiles[1] - quartiles[0])
print(f'Q1 = {quartiles[0]}, Q3 = {quartiles[1]}, 4*IQR = {power_4iqr}')
outlier_powers = df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power']
outlier_powers
207/57:
# making the situation more extreme
df['Power'].hist(bins=20)
plt.title('Power before exaggerating the outliers')
plt.show()
print(df['Power'].mean())
df.loc[outlier_powers.index, 'Power'] = [-200000.0, -1200000.0]
df['Power'].hist(bins=20)
plt.title('Power after exaggerating outliers')
plt.show()
207/58:
# if we wanted to make these NA we could just do this
# [not run]
df.loc[np.abs(df['Power'] - df['Power'].median()) > power_4iqr, 'Power'] = np.nan
207/59:
# dropping these rows
# [not run]
df.drop(outlier_powers.index, axis=0, inplace=True)
207/60:
power = df['Power'][df['Power'].notnull()]

print(power.mean())  # the mean is being pulled
print(power.median())
207/61:
from scipy.stats import tmean

print(tmean(power, limits=np.quantile(power, [.1, .9])))
print(tmean(power, limits=[0,100]))
206/42:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/43:
X = df.drop(["charges"], axis=1)
y = df["charges"]
206/44:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/45:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/46:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/47:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/48:
lin_reg_model = LinerRegression()
lin_reg_model.fit(x_train, y_train)
206/49:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/50:
X = df.drop(["age"], axis=1)
y = df["age"]
206/51:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/52:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/53:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/54:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/55: df.drop(['sex'],axis=1)
206/56:
X = df.drop(["charges"], axis=1)
y = df["charges"]
206/57:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/58:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/59:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/60:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/61:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/62: df_copy = data.drop(["sex"], axis=1)
206/63:
X = df.drop(["charges"], axis=1)
y = df["charges"]
206/64:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/65:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/66:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/67:
X = df_copy.drop(["charges"], axis=1)
y = df_copy["charges"]
206/68:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/69:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/70:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/71:
df_copy = data.drop(["sex"], axis=1)
df_copy
206/72:
df_copy = data.drop(["sex","somker","region"], axis=1)
df_copy
206/73:
X = df_copy.drop(["charges"], axis=1)
y = df_copy["charges"]
206/74:
df_copy = data.drop(["sex", "smoker", "region"], axis=1)
df_copy
206/75:
X = df_copy.drop(["charges"], axis=1)
y = df_copy["charges"]
206/76:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
206/77:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
206/78:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
206/79:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
206/80:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
206/81:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
206/82:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
206/83:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
206/84:
X = df_copy.drop(["charges"], axis=1)
y = df_copy["charges"]
206/85:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
211/1:
X = df_copy.drop(["charges"], axis=1)
y = df_copy["charges"]
211/2:
X = df.drop(["charges"], axis=1)
y = df["charges"]
211/3:
X = data.drop(["charges"], axis=1)
y = data["charges"]
211/4:
X = df.drop(["charges"], axis=1)
y = df["charges"]
211/5:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
211/6:
# loading the dataset

data = pd.read_csv("insurance.csv")
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
211/7: histogram_boxplot(data, "charges")
211/8: histogram_boxplot(data, "bmi")
212/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
212/2:
# loading the dataset

data = pd.read_csv("insurance.csv")
print(f"There are {df.shape[0]} rows and {df.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
212/3: data.isnull().sum()
212/4:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
212/5: histogram_boxplot(data, "charges")
212/6: histogram_boxplot(data, "bmi")
212/7:
plt.hist(np.log(df["bmi"] + 1), 50)
plt.title("log(bmi + 1)")
plt.show()
212/8: df.groupby("region")["charges"].median()
212/9:
plt.hist(np.log(data|["bmi"] + 1), 50)
plt.title("log(bmi + 1)")
plt.show()
212/10:
plt.hist(np.log(data["bmi"] + 1), 50)
plt.title("log(bmi + 1)")
plt.show()
212/11: data.groupby("region")["charges"].median()
212/12:
data = np.corr("age", "charges")
data
212/13: pd.get_dummies(data["region"], drop_first=True)
212/14:
X = data.drop(["charges"], axis=1)
y = data["charges"]
212/15:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)
212/16:
# X.head()
# df_copy = data.drop(["sex", "smoker", "region"], axis=1)
# df_copy
212/17:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
212/18:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
212/19:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
212/20:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
212/21:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
212/22:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
212/23:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
212/24:
# loading the dataset

data = pd.read_csv("insurance.csv")
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
df.sample(n=10)
212/25:
# loading the dataset

data = pd.read_csv("insurance.csv")
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string

# I'm now going to look at 10 random rows
# I'm setting the random seed via np.random.seed so that
# I get the same random results every time
np.random.seed(1)
data.sample(n=10)
212/26:
corr_cols = [item for item in data.columns if "tag" not in item]
print(corr_cols)
212/27:
plt.figure(figsize=(15, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
212/28:
plt.figure(figsize=(15, 7))
sns.heatmap(
    data[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
212/29: data.groupby("region")["charges"].median().round(1)
212/30:
df3=data["children"]
df3.describe()
215/1:
import pandas as pd
import numpy as np
215/2:
adult_data = pd.read_csv("adult_data.csv")
adult_test = pd.read_csv("adult_test.csv")
215/3: adult_data.head()
215/4: adult_data["education"].nunique()
215/5: adult_data["education"].value_counts()
215/6: adult_data["hours-per-week"].mean()
215/7:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
215/8: adult_data.describe().round(2)
215/9:
plt.figure(figsize=(15,5))
sns.barplot(data=adult_data,x='salary',hue='sex')
plt.show()
215/10: adult_data.columns
215/11:
df1 = pd.concat([adult_data,adult_test],axis=0,sort=True)
df1
215/12: df1["hours-per-week"].mean()
215/13: df1.shape
215/14:
# Which country ("native-country") has the 
# highest mean value of "hours-per-week"
# for the dataset "adult_data.csv"?
215/15: adult_data.groupby(["native-country"]).("hours-per-week").mean()
215/16: adult_data["capital-gain"].mean()
215/17: sns.distplot("age");
215/18: count=adult_data["age"].value_counts()
215/19: sns.distplot("age","count")
221/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
221/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
221/3:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
221/4:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
221/5:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
221/6:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
222/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
222/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
222/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
222/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
222/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
222/6:
# checking for duplicate values in the data
df.duplicated().sum()
222/7:
# checking the names of the columns in the data
print(df.columns)
222/8:
# checking column datatypes and number of non-null values
df.info()
222/9:
# checking for missing values in the data.
df.isnull().sum()
222/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
222/11:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
228/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
228/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
228/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
228/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
228/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
228/6:
# checking for duplicate values in the data
df.duplicated().sum()
228/7:
# checking the names of the columns in the data
print(df.columns)
228/8:
# checking column datatypes and number of non-null values
df.info()
228/9:
# checking for missing values in the data.
df.isnull().sum()
228/10:
# checking for missing values in the data.
df.isnull().sum()
228/11:
# Let's look at the statistical summary of the data
df.describe(include="all").T
233/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
233/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
233/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
233/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
233/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
233/6:
# checking for duplicate values in the data
df.duplicated().sum()
233/7:
# checking the names of the columns in the data
print(df.columns)
233/8:
# checking column datatypes and number of non-null values
df.info()
233/9:
# checking for missing values in the data.
df.isnull().sum()
233/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
233/11: df.dropna(subset=["Price"], inplace=True)
233/12:
# checking missing values in rest of the data
df.isnull().sum()
233/13: df.dropna(subset=["Price"], inplace=True)
233/14: df
233/15: df.dropna(subset=["New_Price"], inplace=True)
233/16: df
233/17: df.dropna["New_Price"]
233/18: df
233/19: df.drop["New_Price"]
233/20: df.drop("New_Price")
233/21: df1.drop("New_Price", axis=1, inplace=True)
233/22: df.drop("New_Price", axis=1, inplace=True)
233/23: df.drop(['New_Price'], axis=1, inplace=True)
233/24: df.drop(["New_Price"], axis=0, inplace=True)
233/25: df.drop(['New_Price'], axis=0, inplace=True)
233/26: df.drop(["New_Price"], axis=1, inplace=True)
247/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
247/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
247/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
247/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
247/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
247/6:
# checking for duplicate values in the data
df.duplicated().sum()
247/7:
# checking the names of the columns in the data
print(df.columns)
247/8:
# checking column datatypes and number of non-null values
df.info()
247/9:
# checking for missing values in the data.
df.isnull().sum()
247/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
247/11: df.dropna(subset=["Price"], inplace=True)
247/12:
# checking missing values in rest of the data
df.isnull().sum()
247/13: df.drop(["New_Price"], axis=1, inplace=True)
247/14: df
247/15:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
df
247/16:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            bhp = (
                float(spl[0].replace("bhp ", "")) 
            )  # taking numeric part and converting hours to minutes
            
            return bhp 
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
247/17:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
247/18:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(
                spl[0].replace("bhp ", "")
            )  # taking numeric part 

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
247/19:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
247/20:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
247/21:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
247/22:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km/kg = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km/kg
        
        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        
        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/23:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmkg = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km / kg

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/24:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/25:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmkg = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return kmkg

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/26:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/27:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/28:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/29:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km",""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl",""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl",""))  # taking numeric part of Engine

        else:
            return float(var.replace("km",""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/30:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/31:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/32:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/33:
def mileage_to_var(var):
#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        else "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/35:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/36:
def mileage_to_var(var):
#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/38:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/39:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        else:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
    
    
#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
247/40:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
        return np.nan  # will return NaN if value is not string
    
    
#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
247/41:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
            return np.nan  # will return NaN if value is not string
    
    
#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
247/42:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
247/43: df.drop(["New_Price"], axis=1, inplace=True)
248/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
248/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
248/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
248/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
248/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
248/6:
# checking for duplicate values in the data
df.duplicated().sum()
248/7:
# checking the names of the columns in the data
print(df.columns)
248/8:
# checking column datatypes and number of non-null values
df.info()
248/9:
# checking for missing values in the data.
df.isnull().sum()
248/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
248/11: df.dropna(subset=["Price"], inplace=True)
248/12:
# checking missing values in rest of the data
df.isnull().sum()
248/13: df.drop(["New_Price"], axis=1, inplace=True)
248/14: df
248/15: df.drop(["New_Price"], axis=1, inplace=False)
249/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
249/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
249/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
249/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
249/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
249/6:
# checking for duplicate values in the data
df.duplicated().sum()
249/7:
# checking the names of the columns in the data
print(df.columns)
249/8:
# checking column datatypes and number of non-null values
df.info()
249/9:
# checking for missing values in the data.
df.isnull().sum()
249/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
249/11: df.dropna(subset=["Price"], inplace=True)
249/12:
# checking missing values in rest of the data
df.isnull().sum()
249/13: df.drop(["New_Price"], axis=1, inplace=False)
249/14: df.dropna(subset=["Price"], inplace=False)
249/15:
# checking missing values in rest of the data
df.isnull().sum()
249/16: df.drop(["Price"], axis=1, inplace=False)
250/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
250/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
250/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
250/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
250/5:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
250/6:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
250/7:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
250/8:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
250/9:
# checking for duplicate values in the data
df.duplicated().sum()
250/10:
# checking the names of the columns in the data
print(df.columns)
250/11:
# checking column datatypes and number of non-null values
df.info()
251/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
251/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
251/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
251/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
251/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
251/6:
# checking for duplicate values in the data
df.duplicated().sum()
252/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
252/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
252/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
252/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
252/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
252/6:
# checking for duplicate values in the data
df.duplicated().sum()
252/7:
# checking the names of the columns in the data
print(df.columns)
252/8:
# checking column datatypes and number of non-null values
df.info()
252/9:
# checking for missing values in the data.
df.isnull().sum()
252/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
252/11: df.drop(["Price"], axis=1, inplace=False)
252/12: df.drop(["Price"], axis=1, inplace= True)
252/13:
# checking missing values in rest of the data
df.isnull().sum()
252/14: df.drop(["New_Price"], axis=1, inplace=False)
252/15: df
252/16:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
252/17:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
252/18:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
252/19:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
252/20:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
            return np.nan  # will return NaN if value is not string


#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
252/21:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
252/22:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
252/23: df.Location.value_counts()
252/24: labeled_barplot(df, "Location")
252/25: df.groupby("Location")["Price"].mean()
252/26: data.groupby("Location")["Price"].mean()
252/27:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df, y="Price", x="Company")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df, y="Price", x="Company")
plt.xticks(rotation=90)

plt.show()
252/28:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
252/29:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=data, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=data, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
252/30: data.groupby("Year")["Price"].mean()
252/31:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=data, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=data, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
252/32:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = (spl[0].replace("kmpl", "")).astype(float)  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
            return np.nan  # will return NaN if value is not string
        # removing the units and converting to float
#         df1["Weight_kg"] = df1["Weight"].str.replace("kg", "").astype(float)
#         df1.drop("Weight", axis=1, inplace=True)
#         df1["Weight_kg"].describe()


#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
252/33:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
252/34:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
252/35: histogram_boxplot(df3, "Price")
252/36: histogram_boxplot(df, "Price")
252/37: histogram_boxplot(data, "Price")
252/38: histogram_boxplot(df, "Kilometers_Driven", bins=100)
252/39: histogram_boxplot(df, "Engine")
252/40: histogram_boxplot(df, "Power")
252/41: df1.groupby("Location")["Price"].mean()
252/42:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
252/43: df1.groupby("Location")["Price"].mean()
252/44:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
252/45: df1.groupby("Year")["Price"].mean()
252/46:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
252/47:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
252/48: histogram_boxplot(df1, "Price")
252/49: histogram_boxplot(df1, "Kilometers_Driven", bins=100)
252/50: histogram_boxplot(df1, "Engine")
252/51: histogram_boxplot(df, "Engine")
252/52: histogram_boxplot(df1, "Engine")
252/53: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
252/54: histogram_boxplot(df1, "Price", bins=10)
252/55: histogram_boxplot(df1, "Price", bins=50)
252/56: histogram_boxplot(df1, "Kilometers_Driven", bins=40)
252/57: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
252/58: histogram_boxplot(df1, "Engine")
254/1:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
254/2: histogram_boxplot(df1, "Price", bins=50)
254/3:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
255/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
255/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
255/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
255/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
255/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
255/6:
# checking for duplicate values in the data
df.duplicated().sum()
255/7:
# checking the names of the columns in the data
print(df.columns)
255/8:
# checking column datatypes and number of non-null values
df.info()
255/9:
# checking for missing values in the data.
df.isnull().sum()
255/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
255/11: df.drop(["Price"], axis=1, inplace=True)
255/12:
# checking missing values in rest of the data
df.isnull().sum()
255/13: df.drop(["New_Price"], axis=1, inplace=False)
255/14: df
255/15:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
255/16:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
255/17:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
255/18:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
255/19:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = (spl[0].replace("kmpl", "")).astype(float)  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
            return np.nan  # will return NaN if value is not string
        # removing the units and converting to float


#         df1["Weight_kg"] = df1["Weight"].str.replace("kg", "").astype(float)
#         df1.drop("Weight", axis=1, inplace=True)
#         df1["Weight_kg"].describe()


#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
255/20:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
255/21:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
255/22: df.Location.value_counts()
255/23: labeled_barplot(df, "Location")
255/24:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
255/25: df1.groupby("Location")["Price"].mean()
255/26:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
255/27: df1.groupby("Year")["Price"].mean()
255/28:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
255/29:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
255/30: histogram_boxplot(df1, "Price", bins=50)
255/31: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
255/32: histogram_boxplot(df1, "Engine")
255/33: histogram_boxplot(df1, "Power")
255/34: histogram_boxplot(df, "Engine")
255/35: histogram_boxplot(df, "Power")
255/36: histogram_boxplot(df1, "Seats", bins=50)
255/37:
# creating a list of non-tag columns
corr_cols = [item for item in df.columns]
print(corr_cols)
255/38:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
255/39:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df1[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
255/40:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
255/41:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Power", y="Price", data=df3)
plt.show()
255/42:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Power", y="Price", data=df1)
plt.show()
255/43:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Kilometers_Driven", y="Price", data=df1)
plt.show()
255/44:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
255/45:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df1)
plt.show()
255/46:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Name", y="Price", data=df1)
plt.show()
255/47:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df1)
plt.show()
255/48:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df1)
plt.show()
255/49:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df1)
plt.show()
255/50:
# creating a list of non-tag columns
dist_cols = [
    item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
255/51:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 4, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
255/52:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
255/53:
# creating a copy of the dataframe
df2 = df1.copy()
255/54:
# using log transforms on some columns

for col in dist_cols:
    df2[col + "_log"] = np.log(df2[col] + 1)

# dropping the original columns
df2.drop(dist_cols, axis=1, inplace=True)
df2.head()
255/55:
# creating a list of non-tag columns
dist_cols = [
    item for item in df2.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
255/56:
X = df2.drop(["Price"], axis=1)
y = df2["Price"]
255/57:
X = df.drop(["Price"], axis=1)
y = df["Price"]
255/58:
X = data.drop(["Price"], axis=1)
y = data["Price"]
255/59:
X = df.drop(["Price"], axis=1)
y = df["Price"]
255/60: df3=data.copy()
255/61:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
255/62:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
255/63:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
255/64:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
255/65:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
255/66: df3 = data.copy()
255/67:
X = df.drop(["Price"], axis=1)
y = df["Price"]
255/68:
X = df1.drop(["Price"], axis=1)
y = df1["Price"]
255/69:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
255/70:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
255/71:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
255/72:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
255/73: df1 = df1.drop(["Mileage"], axis=1)
255/74:
X = df1.drop(["Price"], axis=1)
y = df1["Price"]
255/75:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
255/76:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
255/77:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
255/78:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
255/79:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
255/80:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
255/81:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
255/82:
df1 = df1.drop(["Mileage"], axis=1)
df1
255/83:
# checking missing values in rest of the data
df.isnull().sum()
255/84:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
255/85:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
255/86:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
255/87:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
255/88:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
255/89:
# checking for duplicate values in the data
df.duplicated().sum()
255/90:
# checking the names of the columns in the data
print(df.columns)
255/91:
# checking column datatypes and number of non-null values
df.info()
255/92:
# checking for missing values in the data.
df.isnull().sum()
255/93:
# Let's look at the statistical summary of the data
df.describe(include="all").T
255/94:
# checking missing values in rest of the data
df.isnull().sum()
255/95: df.drop(["New_Price"], axis=1, inplace=False)
255/96: df
255/97:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
255/98:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
255/99:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
255/100:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
255/101:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = (spl[0].replace("kmpl", "")).astype(float)  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        else:
            return np.nan  # will return NaN if value is not string
        # removing the units and converting to float


#         df1["Weight_kg"] = df1["Weight"].str.replace("kg", "").astype(float)
#         df1.drop("Weight", axis=1, inplace=True)
#         df1["Weight_kg"].describe()


#     if isinstance(var, str):  # checking if the value is string or not
#         if "km/kg" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             km = float(spl[0].replace("km/kg", ""))  # taking numeric part

#             return km

#         if "kmpl" in var:  # checking for the presence of CC in the Engine
#             spl = var.split(" ")  # splitting the value by space
#             kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

#             return kmpl
#         elif "kmpl" in var:
#             return float(var.replace("kmpl", ""))  # taking numeric part of Engine

#         else:
#             return float(var.replace("km/kg", ""))  # taking numeric part of Engine
255/102:
df["Mileage"] = df["Mileage"].apply(engine_to_var)
df.head()
256/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
256/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
256/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
256/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
256/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
256/6:
# checking for duplicate values in the data
df.duplicated().sum()
256/7:
# checking the names of the columns in the data
print(df.columns)
256/8:
# checking column datatypes and number of non-null values
df.info()
256/9:
# checking for missing values in the data.
df.isnull().sum()
256/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
256/11:
# checking missing values in rest of the data
df.isnull().sum()
256/12: df.drop(["New_Price"], axis=1, inplace=False)
256/13: df
256/14:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
256/15:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
256/16:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/17:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
256/18:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float((spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/19:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            cc = float((spl[0].replace("CC", ""))  # taking numeric part

            return cc
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/20:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
256/21:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            cc = float((spl[0].replace("CC", ""))  # taking numeric part

            return cc
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/22:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float((spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/23:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
256/24:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float((spl[0].replace("CC", ""))  # taking numeric part
            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/25:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part
            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
256/26:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
256/27:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC ", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
256/28:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
256/29:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
256/30:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
256/31:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
256/32:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
256/33:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
257/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
257/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
257/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
257/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
257/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
257/6:
# checking for duplicate values in the data
df.duplicated().sum()
257/7:
# checking the names of the columns in the data
print(df.columns)
257/8:
# checking column datatypes and number of non-null values
df.info()
257/9:
# checking for missing values in the data.
df.isnull().sum()
257/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
257/11:
# checking missing values in rest of the data
df.isnull().sum()
257/12: df.drop(["New_Price"], axis=1, inplace=False)
257/13: df
257/14:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/15:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/16:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/17:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
257/18: df.dropna(subset=["Price"], inplace=True)
257/19:
# checking missing values in rest of the data
df.isnull().sum()
257/20: df.drop(["New_Price"], axis=1, inplace=False)
257/21: df
257/22:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/23:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/24:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/25:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
257/26:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
    else:
        return np.nan  # will return NaN if value is not string
257/27:
df["Engine"] = df["Engine"].apply(power_to_var)
df.head()
257/28:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
257/29:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/30:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/31:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/32:
data["Engine"] = data["Engine"].apply(power_to_var)
df.head()
257/33:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
257/34:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/35:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
257/36:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/37:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
257/38: df.Location.value_counts()
257/39:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/40:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
257/41:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        if "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
        
    else:
        return np.nan  # will return NaN if value is not string
257/42:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/43:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km
        
        elif:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine


        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        
        else "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
257/44:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km
        
        

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl
        
        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
257/45:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/46:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine
        

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

        
    else:
        return np.nan  # will return NaN if value is not string
257/47:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/48:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
257/49: df.Location.value_counts()
257/50: labeled_barplot(df, "Location")
257/51:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
257/52: df1.groupby("Location")["Price"].mean()
257/53:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
257/54: df1.groupby("Year")["Price"].mean()
257/55:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
257/56:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
257/57: histogram_boxplot(df1, "Price", bins=50)
257/58: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
257/59: histogram_boxplot(df, "Engine")
257/60: histogram_boxplot(df, "Power")
257/61: histogram_boxplot(df1, "Seats", bins=50)
257/62:
# creating a list of non-tag columns
corr_cols = [item for item in df.columns]
print(corr_cols)
257/63:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
257/64: histogram_boxplot(df1, "Engine")
257/65: histogram_boxplot(df, "Engine")
257/66: histogram_boxplot(data, "Engine")
257/67: histogram_boxplot(df1, "Engine", bins=10)
257/68:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
257/69:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
257/70:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
257/71:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
257/72:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
257/73:
# checking for duplicate values in the data
df.duplicated().sum()
257/74:
# checking the names of the columns in the data
print(df.columns)
257/75:
# checking column datatypes and number of non-null values
df.info()
257/76:
# checking for missing values in the data.
df.isnull().sum()
257/77:
# Let's look at the statistical summary of the data
df.describe(include="all").T
257/78: df.dropna(subset=["Price"], inplace=True)
257/79:
# checking missing values in rest of the data
df.isnull().sum()
257/80: df.drop(["New_Price"], axis=1, inplace=False)
257/81: df
257/82:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/83:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/84:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/85:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
257/86:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
257/87:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/88:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
257/89: df.Location.value_counts()
257/90: labeled_barplot(df, "Location")
257/91:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
257/92: df1.groupby("Location")["Price"].mean()
257/93:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
257/94: df1.groupby("Year")["Price"].mean()
257/95:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
257/96:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
257/97: histogram_boxplot(df1, "Price", bins=50)
257/98: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
257/99: histogram_boxplot(df1, "Engine", bins=10)
257/100: histogram_boxplot(df, "Power")
257/101: histogram_boxplot(df, "Engine", bins=10)
257/102: histogram_boxplot(df, "Seats", bins=50)
257/103: histogram_boxplot(df, "Kilometers_Driven", bins=10)
257/104: histogram_boxplot(df, "Price", bins=50)
257/105: histogram_boxplot(df, "Power")
257/106: histogram_boxplot(df, "Seats", bins=50)
257/107:
# creating a list of non-tag columns
corr_cols = [item for item in df.columns]
print(corr_cols)
257/108:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
257/109:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df1)
plt.show()
257/110:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df1)
plt.show()
257/111:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df1)
plt.show()
257/112:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df1)
plt.show()
257/113:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/114:
# creating a copy of the dataframe
df2 = df1.copy()
257/115:
# using log transforms on some columns

for col in dist_cols:
    df2[col + "_log"] = np.log(df2[col] + 1)

# dropping the original columns
df2.drop(dist_cols, axis=1, inplace=True)
df2.head()
257/116:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/117:
df1 = df1.drop(["Mileage"], axis=1)
df1
257/118:
X = df1.drop(["Price"], axis=1)
y = df1["Price"]
257/119:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/120:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/121:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/122: df1.drop(["Name"], axis=1)
257/123: df1.drop(["Name"], axis=1,inplace= False)
257/124: df1.drop(["Name"], axis=1, inplace=True)
257/125:
X = df1.drop(["Price"], axis=1)
y = df1["Price"]
257/126:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/127:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/128:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/129: df1.drop(["Name"], axis=1, inplace=False)
257/130: df.drop(["Name"], axis=1, inplace=False)
257/131:
df3 = df.drop(["Name"], axis=1, inplace=False)
df3
257/132:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/133:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/134:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/135:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/136:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/137:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
257/138:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/139:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/140:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/141:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/142:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/143:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/144:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/145:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/146:
X = df.drop(["Price"], axis=1)
y = df["Price"]
257/147:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/148:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/149:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/150:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
257/151:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/152:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/153:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/154: df=df.drop(["Seats"], axis=1)
257/155:
X = df.drop(["Price"], axis=1)
y = df["Price"]
257/156:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
257/157:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/158:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/159:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/160:
X = df.drop(["Price"], axis=1)
y = df["Price"]
257/161:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/162:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/163:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/164: df = df.drop(["Name"], axis=1)
257/165:
X = df.drop(["Price"], axis=1)
y = df["Price"]
257/166:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/167:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/168:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/169: df3 = df3.drop(["Seats"], axis=1)
257/170: df3 = df3.drop(["Name"], axis=1)
257/171:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/172: df3 = df3.drop(["Seats"], axis=1)
257/173:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/174: df3=df2.copy
257/175: df3 = df3.drop(["Seats"], axis=1)
257/176:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/177:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/178:
# creating a copy of the dataframe
df2 = df1.copy()
257/179:
# using log transforms on some columns

for col in dist_cols:
    df2[col + "_log"] = np.log(df2[col] + 1)

# dropping the original columns
df2.drop(dist_cols, axis=1, inplace=True)
df2.head()
257/180:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/181: df3 = df2.copy
257/182:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/183: df3 = df2.copy()
257/184:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/185:
df3 = df2.copy()
df3
257/186:
df3 = df1.copy()
df3
257/187:
X = df1.drop(["Price"], axis=1)
y = df1["Price"]
257/188:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/189:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/190:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/191:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/192:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/193:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/194:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
257/195:
df3 = df.copy()
df3
257/196: df3= df.drop(["Price"])
257/197: df
257/198: df3 = df3.drop(["Price"])
257/199: df
257/200:
df3 = df.copy()
df3
257/201: df3 = df3.drop(["Price"])
257/202: df3 = df3.drop(["Price"], axis=1)
257/203:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/204: df3 = df3.drop(["Location","Owner_Type","Transmission","Owner_Type "], axis=1)
257/205: df3 = df3.drop(["Location", "Owner_Type", "Transmission", "Owner_Type"], axis=1)
257/206:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/207:
X = df3.drop(["Price"], axis=1)
y = df3["Price",axis=1]
257/208:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/209:
df3 = df3.drop(["Location", "Owner_Type", "Transmission", "Owner_Type"], axis=1)
df3
257/210:
df3 = df3.drop(["Location", "Owner_Type", "Transmission", "Owner_Type"])
df3
257/211: df3.describe()
257/212:
year = year.astype('category') # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
257/213:
df.year = df.year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
257/214:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
257/215:
# Let's look at the statistical summary of the data
df.describe(include="all").T
257/216: df.dropna(subset=["Price"], inplace=True)
257/217:
# checking missing values in rest of the data
df.isnull().sum()
257/218: df.drop(["New_Price"], axis=1, inplace=False)
257/219: df
257/220:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/221:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
257/222:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
257/223:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
257/224:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
257/225:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
257/226:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
257/227: df.Location.value_counts()
257/228: labeled_barplot(df, "Location")
257/229:
# creating a copy of the data so that original data remains unchanged
df1 = data.copy()
257/230: df1.groupby("Location")["Price"].mean()
257/231:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
257/232: df1.groupby("Year")["Price"].mean()
257/233:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
257/234:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
257/235: histogram_boxplot(df, "Price", bins=50)
257/236: histogram_boxplot(df, "Kilometers_Driven", bins=10)
257/237: histogram_boxplot(df, "Engine", bins=10)
257/238: histogram_boxplot(df1, "Price", bins=50)
257/239: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
257/240: histogram_boxplot(df1, "Engine", bins=10)
257/241: histogram_boxplot(df, "Engine", bins=10)
257/242:
# creating a copy of the data so that original data remains unchanged
df1 = df.copy()
257/243: df1.groupby("Location")["Price"].mean()
257/244:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
257/245: df1.groupby("Year")["Price"].mean()
257/246:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
257/247:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
257/248: histogram_boxplot(df1, "Price", bins=50)
257/249: histogram_boxplot(df1, "Engine", bins=10)
257/250: histogram_boxplot(df1, "Power")
257/251: histogram_boxplot(df, "Engine", bins=10)
257/252: histogram_boxplot(df, "Power")
257/253:
# creating a list of non-tag columns
corr_cols = [item for item in df.columns]
print(corr_cols)
257/254:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
257/255:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df1[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
257/256:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df1)
plt.show()
257/257:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df1)
plt.show()
257/258:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df1)
plt.show()
257/259:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df1)
plt.show()
257/260:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/261:
# creating a copy of the dataframe
df2 = df1.copy()
257/262:
# creating a copy of the data so that original data remains unchanged
df1 = df.copy()
257/263: df1.groupby("Location")["Price"].mean()
257/264:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
257/265: df1.groupby("Year")["Price"].mean()
257/266:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
257/267:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
257/268: histogram_boxplot(df1, "Price", bins=50)
257/269: histogram_boxplot(df1, "Kilometers_Driven", bins=10)
257/270: histogram_boxplot(df1, "Seats", bins=50)
257/271: histogram_boxplot(df, "Seats", bins=50)
257/272: histogram_boxplot(df, "Seats", bins=10)
257/273: histogram_boxplot(df1, "Seats", bins=10)
257/274: histogram_boxplot(df1, "Mileage", bins=50)
257/275: histogram_boxplot(df1, "Mileage", bins=10)
257/276:
# creating a list of non-tag columns
corr_cols = [item for item in df.columns]
print(corr_cols)
257/277:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df1[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
257/278:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df1)
plt.show()
257/279:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df1)
plt.show()
257/280:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df1)
plt.show()
257/281:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df1)
plt.show()
257/282:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/283:
# creating a list of non-tag columns
dist_cols = [item for item in df1.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/284:
# creating a copy of the dataframe
df2 = df1.copy()
257/285:
# using log transforms on some columns

for col in dist_cols:
    df2[col + "_log"] = np.log(df2[col] + 1)

# dropping the original columns
df2.drop(dist_cols, axis=1, inplace=True)
df2.head()
257/286:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/287:
# creating a copy of the dataframe
df2 = df.copy()
257/288:
# using log transforms on some columns

for col in dist_cols:
    df2[col + "_log"] = np.log(df2[col] + 1)

# dropping the original columns
df2.drop(dist_cols, axis=1, inplace=True)
df2.head()
257/289:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
257/290: df
257/291:
df3 = df.copy()
df3
257/292: df3.describe()
257/293:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
257/294:
X = df3.drop(["Price"], axis=1)
y = df3["Price"]
257/295:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
257/296:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
257/297:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
269/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
269/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
269/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
269/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
269/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
269/6:
# checking for duplicate values in the data
df.duplicated().sum()
269/7:
# checking the names of the columns in the data
print(df.columns)
269/8:
# checking column datatypes and number of non-null values
df.info()
269/9:
# checking for missing values in the data.
df.isnull().sum()
269/10:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
269/11:
# Let's look at the statistical summary of the data
df.describe(include="all").T
269/12:
# Let's look at the statistical summary of the data
df.describe(include="all").T
269/13: df.dropna(subset=["Price"], inplace=True)
269/14:
# checking missing values in rest of the data
df.isnull().sum()
269/15: df.drop(["New_Price"], axis=1, inplace=False)
269/16: df
269/17:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
269/18:
df["Power"] = df["Power"].apply(power_to_var)
df.head()
269/19:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
269/20:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
269/21:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
269/22:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
269/23: df1 = df.drop(["New_Price"], axis=1, inplace=False)
269/24: df1
269/25:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
269/26:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
269/27:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
269/28:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
269/29:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine

    else:
        return np.nan  # will return NaN if value is not string
269/30:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
269/31:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head(sample(10),random_state=4)
269/32:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
269/33:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
269/34:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
269/35:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
269/36:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
269/37:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power

    else:
        return np.nan  # will return NaN if value is not string
269/38:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
269/39:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
269/40:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
269/41:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
269/42: df.Location.value_counts()
269/43: df1['Mileage'].sum()
269/44: df1 = df.drop(["New_Price"], axis=1, inplace=False)
269/45: df1
269/46:
df["Power"] = df["Power"].apply(power_to_var)
df1.head()
269/47:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power

    else:
        return np.nan  # will return NaN if value is not string
269/48:
df["Engine"] = df["Engine"].apply(engine_to_var)
df.head()
269/49:
df["Mileage"] = df["Mileage"].apply(mileage_to_var)
df.head()
269/50:
# df["Mileage"] = df["Mileage"].apply(mileage_to_var)
# df.head()
269/51: df1["Mileage"].sum()
270/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
270/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
270/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
270/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
270/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
270/6:
# checking for duplicate values in the data
df.duplicated().sum()
270/7:
# checking the names of the columns in the data
print(df.columns)
270/8:
# checking column datatypes and number of non-null values
df.info()
270/9:
# checking for missing values in the data.
df.isnull().sum()
270/10:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
270/11:
# Let's look at the statistical summary of the data
df.describe(include="all").T
270/12: df.dropna(subset=["Price"], inplace=True)
270/13:
# checking missing values in rest of the data
df.isnull().sum()
270/14: df1 = df.drop(["New_Price"], axis=1, inplace=False)
270/15: df1
270/16:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
270/17:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
270/18:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power

    else:
        return np.nan  # will return NaN if value is not string
270/19:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
270/20:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
270/21:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
270/22: df1["Mileage"].sum()
270/23: df1["Mileage"].value_counts()
270/24:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
270/25: df1.Location.value_counts()
270/26: df2=df1.copy()
270/27: df2.Location.value_counts()
270/28: labeled_barplot(df2, "Location")
270/29: df2.groupby("Location")["Price"].mean()
270/30:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
270/31: df2.groupby("Year")["Price"].mean()
270/32:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
270/33:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
270/34: histogram_boxplot(df2, "Price", bins=50)
270/35: histogram_boxplot(df2, "Kilometers_Driven", bins=10)
270/36: histogram_boxplot(df1, "Power", bins=10)
270/37: histogram_boxplot(df1, "Mileage", bins=10)
270/38: histogram_boxplot(df2, "Power", bins=10)
270/39: histogram_boxplot(df2, "Mileage", bins=10)
270/40:
# creating a list of non-tag columns
corr_cols = [item for item in df2.columns]
print(corr_cols)
270/41:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df2[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
270/42:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df2)
plt.show()
270/43:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df2)
plt.show()
270/44:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df2)
plt.show()
270/45:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df2)
plt.show()
270/46:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/47:
# creating a copy of the dataframe
df3 = df2.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("Engine")
dist_cols.remove("Seats")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("Mileage")
270/48:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
270/49:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df2, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/50:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/51:
# creating a copy of the dataframe
df3 = df2.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("Mileage")
270/52:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
270/53:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/54:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/55:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/56:
# creating a copy of the dataframe
df3 = df2.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("Mileage")
270/57:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
270/58:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
270/59:
df4 = df3.copy()
df4
270/60:
df4 = df3.copy()
df4.head()
270/61: df4.describe()
270/62:
df4 = df2.copy()
df4.head()
270/63:
X = df4.drop(["Price"], axis=1)
y = df4["Price"]
270/64:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
270/65:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
270/66:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
270/67:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
270/68:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
270/69:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
270/70:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
270/71: df4.isnull.sum()
270/72: df4.isnull().sum()
270/73:
df4["Mileage"].fillna(df4.Mileage.median(), inplace=True)
df4.isnull().sum()
270/74:
df4["Engine"].fillna(df4.Engine.median(), inplace=True)
df4.isnull().sum()
270/75:
df4["Power"].fillna(df4.Power.median(), inplace=True)
df4.isnull().sum()
270/76:
df4["Seats"].fillna(df4.Seats.median(), inplace=True)
df4.isnull().sum()
270/77:
X = df4.drop(["Price"], axis=1)
y = df4["Price"]
270/78: df4.isnull().sum()
270/79:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
270/80:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
270/81:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
270/82:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
270/83:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
270/84:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df.round(1)
270/85:
We will be using metric functions defined in sklearn for RMSE, MAE, and 2

.

We will define functions to calculate adjusted 2

and MAPE.

    The mean absolute percentage error (MAPE) measures the accuracy of predictions as a percentage, and can be calculated as the average absolute percent error for each predicted value minus actual values divided by actual values. It works best if there are no extreme values in the data and none of the actual values are 0.

We will create a function that will print out all the above metrics in one go.
270/86:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
270/87:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
270/88:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
270/89:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf.round(1)
270/90:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
270/91:
print("Test performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model,x_test,y_test)
lin_reg_model_test_perf
259/1:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
259/2:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
259/3:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
271/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
271/2:
# loading the dataset
data = pd.read_csv("anime_data_raw.csv")
271/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
271/4:
# let's view a sample of the data
data.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
271/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
271/6:
# checking for duplicate values in the data
df.duplicated().sum()
271/7:
# checking the names of the columns in the data
print(df.columns)
271/8:
# checking column datatypes and number of non-null values
df.info()
271/9:
# checking for missing values in the data.
df.isnull().sum()
271/10:
# Let's look at the statistical summary of the data
df.describe(include="all").T
271/11: df.dropna(subset=["rating"], inplace=True)
271/12:
# checking missing values in rest of the data
df.isnull().sum()
271/13: df[df.startYr.isnull()]
271/14: df.dropna(subset=["startYr"], inplace=True)
271/15:
# let us reset the dataframe index
df.reset_index(inplace=True, drop=True)
271/16:
# checking missing values in rest of the data
df.isnull().sum()
271/17: df[df.finishYr.isnull()]
271/18:
# checking the summary of the data with missing values in finishYr
df[df.finishYr.isnull()].describe(include="all").T
271/19:
df["finishYr"].fillna(2020, inplace=True)

# checking missing values in rest of the data
df.isnull().sum()
271/20:
df["years_running"] = df["finishYr"] - df["startYr"]
df.drop(["startYr", "finishYr"], axis=1, inplace=True)
df.head()
271/21:
# we define a function to convert the duration column to numeric


def time_to_minutes(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "hr" in var:  # checking for the presence of hours in the duration
            spl = var.split(" ")  # splitting the value by space
            hr = (
                float(spl[0].replace("hr", "")) * 60
            )  # taking numeric part and converting hours to minutes
            mt = float(spl[1].replace("min", ""))  # taking numeric part of minutes
            return hr + mt
        else:
            return float(var.replace("min", ""))  # taking numeric part of minutes
    else:
        return np.nan  # will return NaN if value is not string
271/22:
# let's apply the function to the duration column and overwrite the column
df["duration"] = df["duration"].apply(time_to_minutes)
df.head()
271/23:
# let's check the summary of the duration column
df["duration"].describe()
271/24:
df["sznOfRelease"].fillna("is_missing", inplace=True)
df.isnull().sum()
271/25: df.mediaType.value_counts()
271/26:
df.mediaType.fillna("Other", inplace=True)

# checking the number of unique values and the number of times they occur
df.mediaType.value_counts()
271/27:
cols_with_list_vals = ["studios", "tags", "contentWarn"]

for col in cols_with_list_vals:
    df[col] = (
        df[col].str.lstrip("[").str.rstrip("]")
    )  # remove the leading and trailing square braces
    df[col] = df[col].replace("", np.nan)  # mark as NaN if the value is a blank string

df.head()
271/28:
# checking missing values in rest of the data
df.isnull().sum()
271/29:
df.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
271/30:
studio_df = pd.DataFrame(
    df.studios.str.split(", ", expand=True).values.flatten(), columns=["Studios"]
)
val_c = studio_df.Studios.value_counts()
val_c
271/31:
# we take 100 as threshold
threshold = 100
val_c[val_c.values >= threshold]
271/32:
# list of studios
studios_list = val_c[val_c.values >= threshold].index.tolist()
print("Studio names taken into consideration:", len(studios_list), studios_list)
271/33:
# let us create a copy of our dataframe
df1 = df.copy()
271/34:
# first we will fill missing values in the columns by 'Others'
df1.studios.fillna("'Others'", inplace=True)
df1.studios.isnull().sum()
271/35:
studio_val = []

for i in range(df1.shape[0]):  # iterate over all rows in data
    txt = df1.studios.values[i]  # getting the values in studios column
    flag = 0  # flag variable
    for item in studios_list:  # iterate over the list of studios considered
        if item in txt and flag == 0:  # checking if studio name is in the row
            studio_val.append(item)
            flag = 1
    if flag == 0:  # if the row values is different from the list of studios considered
        studio_val.append("'Others'")

# we will strip the leading and trailing ', and assign the values to a column
df1["studio_primary"] = [item.strip("'") for item in studio_val]
df1.tail()
271/36:
# we will create a list defining whether there is a collaboration between studios
# we will check if the second split has None values, which will mean no collaboration between studios
studio_val2 = [
    0 if item is None else 1
    for item in df1.studios.str.split(", ", expand=True).iloc[:, 1]
]

df1["studios_colab"] = studio_val2
df1.tail()
271/37: df1.drop("studios", axis=1, inplace=True)
271/38:
df1.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
271/39:
tag_df = pd.DataFrame(
    df1.tags.str.split(", ", expand=True).values.flatten(), columns=["Tags"]
)
val_c = tag_df.Tags.value_counts()
val_c
271/40:
# we take 500 as threshold
threshold = 500
val_c[val_c.values >= threshold]
271/41:
# list of tags
tags_list = val_c[val_c.values >= threshold].index.tolist()
print("Tags taken into consideration:", len(tags_list), tags_list)
271/42:
# let us create a copy of our dataframe
df2 = df1.copy()
271/43:
# first we will fill missing values in the columns by 'Others'
df2.tags.fillna("Others", inplace=True)
df2.tags.isnull().sum()
271/44:
tags_df = df2.loc[:, ["title", "tags"]].copy()

for item in tags_list:
    tags_df["tag_" + item] = 0

# creating a column to denote tags other than the ones in the list
tags_df["tag_Others"] = 0

tags_df.head()
271/45: tags_df.shape
271/46:
for i in range(tags_df.shape[0]):  # iterate over all rows in data
    txt = tags_df.tags.values[i]  # getting the values in tags column
    flag = 0  # flag variable
    for item in tags_list:  # iterate over the list of tags considered
        if item in txt:  # checking if tag is in the row
            tags_df.loc[i, "tag_" + item] = 1
            flag = 1
    if flag == 0:  # if the row values is different from the list of tags considered
        tags_df.loc[i, "tag_Others"] = 1

tags_df.head()
271/47:
# concatenating the tags dataframe (except the tags and title columns) to the original data
df2 = pd.concat([df2, tags_df.iloc[:, 2:]], axis=1)
df2.head()
271/48:
df2.drop("tags", axis=1, inplace=True)
df2.shape
271/49:
df2.sample(
    10, random_state=2
)  # setting the random_state will ensure we get the same results every time
271/50:
cw_df = pd.DataFrame(
    df2.contentWarn.str.split(", ", expand=True).values.flatten(), columns=["CW"]
)
val_c = cw_df.CW.value_counts()
val_c
271/51:
df2["contentWarn"].fillna(0, inplace=True)
df2["contentWarn"] = [1 if item != 0 else 0 for item in df2.contentWarn.values]

df2["contentWarn"].value_counts()
271/52:
# checking missing values in rest of the data
df2.isnull().sum()
271/53:
df3 = df2.copy()

df3[["duration", "watched"]] = df3.groupby(["studio_primary", "mediaType"])[
    ["duration", "watched"]
].transform(lambda x: x.fillna(x.median()))
df3.isnull().sum()
271/54:
df3["duration"].fillna(df3.duration.median(), inplace=True)
df3.isnull().sum()
271/55:
df3.drop(["description", "title"], axis=1, inplace=True)

# let's check the summary of our data
df3.describe(include="all").T
271/56:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
271/57: histogram_boxplot(df3, "rating")
271/58: histogram_boxplot(df3, "eps", bins=100)
271/59: histogram_boxplot(df3, "duration")
271/60: histogram_boxplot(df3, "watched", bins=50)
271/61: histogram_boxplot(df3, "years_running")
271/62:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
271/63: labeled_barplot(df3, "mediaType", perc=True)
271/64: labeled_barplot(df3, "ongoing", perc=True)
271/65: labeled_barplot(df3, "sznOfRelease", perc=True)
271/66: labeled_barplot(df3, "studio_primary", perc=True)
271/67:
# creating a list of tag columns
tag_cols = [item for item in df3.columns if "tag" in item]
271/68:
# checking the values in tag columns
for column in tag_cols:
    print(df3[column].value_counts())
    print("-" * 50)
271/69:
# creating a list of non-tag columns
corr_cols = [item for item in df3.columns if "tag" not in item]
print(corr_cols)
271/70:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df3[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
271/71:
plt.figure(figsize=(10, 5))
sns.boxplot(x="mediaType", y="rating", data=df3)
plt.show()
271/72:
plt.figure(figsize=(10, 5))
sns.boxplot(x="sznOfRelease", y="rating", data=df3)
plt.show()
271/73:
plt.figure(figsize=(15, 5))
sns.boxplot(x="studio_primary", y="rating", data=df3)
plt.xticks(rotation=90)
plt.show()
271/74:
# creating a list of non-tag columns
dist_cols = [
    item for item in df3.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
271/75:
# creating a copy of the dataframe
df4 = df3.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("contentWarn")
dist_cols.remove("studios_colab")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("rating")
271/76:
# using log transforms on some columns

for col in dist_cols:
    df4[col + "_log"] = np.log(df4[col] + 1)

# dropping the original columns
df4.drop(dist_cols, axis=1, inplace=True)
df4.head()
271/77:
# creating a list of non-tag columns
dist_cols = [
    item for item in df4.select_dtypes(include=np.number).columns if "tag" not in item
]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 3, i + 1)
    plt.hist(df4[dist_cols[i]], bins=50)
    # sns.histplot(data=df4, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
271/78:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df4[dist_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
271/79:
df4.drop(["votes_log"], axis=1, inplace=True)
df4.shape
271/80:
X = df4.drop(["rating"], axis=1)
y = df4["rating"]
271/81:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
271/82:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
271/83:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
271/84:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
271/85:
coef_df = pd.DataFrame(
    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),
    index=x_train.columns.tolist() + ["Intercept"],
    columns=["Coefficients"],
)
coef_df
271/86:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
271/87:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
271/88:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
271/89:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
270/92:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
271/90:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
272/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
272/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
272/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
272/4:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
272/5:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
272/6:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
272/7:
# checking for duplicate values in the data
df.duplicated().sum()
272/8:
# checking the names of the columns in the data
print(df.columns)
272/9:
# checking column datatypes and number of non-null values
df.info()
272/10:
# checking for missing values in the data.
df.isnull().sum()
272/11:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
272/12:
# Let's look at the statistical summary of the data
df.describe(include="all").T
272/13: df.dropna(subset=["Price"], inplace=True)
272/14:
# checking missing values in rest of the data
df.isnull().sum()
272/15: df1 = df.drop(["New_Price"], axis=1, inplace=False)
272/16: df1
272/17:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
272/18:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
272/19:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of Power

    else:
        return np.nan  # will return NaN if value is not string
272/20:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
272/21:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of CC in the Engine
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of Engine

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
272/22:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
272/23:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
272/24: df2 = df1.copy()
272/25: df2.Location.value_counts()
272/26: labeled_barplot(df2, "Location")
272/27: df2.groupby("Location")["Price"].mean()
272/28:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
272/29: df2.groupby("Year")["Price"].mean()
272/30:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
272/31:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
272/32: histogram_boxplot(df2, "Price", bins=50)
272/33: histogram_boxplot(df2, "Kilometers_Driven", bins=10)
272/34: histogram_boxplot(df2, "Power", bins=10)
272/35: histogram_boxplot(df2, "Mileage", bins=10)
272/36:
# creating a list of non-tag columns
corr_cols = [item for item in df2.columns]
print(corr_cols)
272/37:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df2[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
272/38:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df2)
plt.show()
272/39:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df2)
plt.show()
272/40:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df2)
plt.show()
272/41:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df2)
plt.show()
272/42:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
272/43:
# creating a copy of the dataframe
df3 = df2.copy()

# removing contentWarn and studios_colab columns as they have only 0 and 1 values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the rating column as it is almost normally distributed
dist_cols.remove("Mileage")
272/44:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
272/45:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
272/46:
df4 = df2.copy()
df4.head()
272/47:
df4["Mileage"].fillna(df4.Mileage.median(), inplace=True)
df4.isnull().sum()
272/48:
df4["Engine"].fillna(df4.Engine.median(), inplace=True)
df4.isnull().sum()
272/49:
df4["Power"].fillna(df4.Power.median(), inplace=True)
df4.isnull().sum()
272/50:
df4["Seats"].fillna(df4.Seats.median(), inplace=True)
df4.isnull().sum()
272/51:
X = df4.drop(["Price"], axis=1)
y = df4["Price"]
272/52: df4.isnull().sum()
272/53:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
272/54:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
272/55:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
272/56:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
272/57:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
272/58: coef_df
272/59:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
272/60:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
272/61:
print("Test performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
272/62:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
272/63:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
272/64:
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

# Build step forward feature selection
sfs = SFS(
    reg,
    k_features=x_train.shape[1],
    forward=True,  # k_features denotes "Number of features to select"
    floating=False,
    scoring="r2",
    n_jobs=-1,
    verbose=2,
    cv=5,
)

# Perform SFFS
sfs = sfs.fit(x_train, y_train)
272/65:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(df.astype)
272/66:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
272/67:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
274/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
274/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
274/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
274/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
274/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
274/6:
# checking for duplicate values in the data
df.duplicated().sum()
274/7:
# checking the names of the columns in the data
print(df.columns)
274/8:
# checking column datatypes and number of non-null values
df.info()
274/9:
# checking for missing values in the data.
df.isnull().sum()
274/10:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(honeyprod.astype)
274/11:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(df.astype)
274/12:
# Let's look at the statistical summary of the data
df.describe(include="all").T
274/13: df.dropna(subset=["Price"], inplace=True)
274/14:
# checking missing values in rest of the data
df.isnull().sum()
274/15: df1 = df.drop(["New_Price"], axis=1, inplace=False)
274/16: df1
274/17:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
274/18:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
274/19:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of engine

    else:
        return np.nan  # will return NaN if value is not string
274/20:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
274/21:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of km/kg in the mileage
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of kmpl in the mileage
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of mileage

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of Engine
    else:
        return np.nan  # will return NaN if value is not string
274/22:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
274/23:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
274/24: df2 = df1.copy()
274/25: df2.Location.value_counts()
274/26:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
274/27:
#crating a copy of the dataframe
df2 = df1.copy()
274/28:
# creating a copy of the dataframe
df2 = df1.copy()
274/29: df2.Location.value_counts()
274/30: df2.groupby("Location")["Price"].mean()
274/31: df2.groupby("Year")["Price"].mean()
274/32:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
274/33:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
274/34: Now, we are heading forward to bivariate analysis
274/35:
# creating a list of non-tag columns
corr_cols = [item for item in df2.columns]
print(corr_cols)
274/36: Observations:
274/37:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df2)
plt.show()
274/38: Observation: In terms of mean price,'Coimbatore' holds the top position; while 'Kolkata' and 'Jaipur' are at the bottom line.
274/39:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df2)
plt.show()
274/40:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df2)
plt.show()
274/41:
# creating a copy of the dataframe
df3 = df2.copy()

# removing Kilometers_Driven and Seats columns as they have only around two or three values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the Mileage column as it is almost normally distributed
dist_cols.remove("Mileage")
274/42:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df2)
plt.show()
274/43:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df1[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
274/44:
# creating a copy of the dataframe
df3 = df2.copy()

# removing Kilometers_Driven and Seats columns as they have only around two or three values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the Mileage column as it is almost normally distributed
dist_cols.remove("Mileage")
274/45:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
274/46:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # anyone can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
274/47:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
274/48:
# creating a copy of the dataframe
df3 = df2.copy()

# removing Kilometers_Driven and Seats columns as they have only around two or three values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the Mileage column as it is almost normally distributed
dist_cols.remove("Mileage")
274/49:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
274/50:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # anyone can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
274/51:
df4 = df2.copy()
df4.head()
274/52: df4.isnull().sum()
274/53:
df4["Mileage"].fillna(df4.Mileage.median(), inplace=True)
df4.isnull().sum()
274/54:
df4["Engine"].fillna(df4.Engine.median(), inplace=True)
df4.isnull().sum()
274/55:
df4["Power"].fillna(df4.Power.median(), inplace=True)
df4.isnull().sum()
274/56:
df4["Seats"].fillna(df4.Seats.median(), inplace=True)
df4.isnull().sum()
274/57:
X = df4.drop(["Price"], axis=1)
y = df4["Price"]
274/58:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
274/59:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
274/60:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
274/61:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
274/62:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)
274/63:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
274/64:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
274/65:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100000


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
274/66:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
274/67:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
274/68:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 10000


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
274/69:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
274/70:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
274/71:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 1000


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
274/72:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
274/73:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
274/74:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 1000


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
274/75:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
274/76:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
274/77:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
274/78:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
274/79:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
276/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
276/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
276/3:
# checking the shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")  # f-string
276/4:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
276/5:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
276/6:
# checking for duplicate values in the data
df.duplicated().sum()
276/7:
# checking the names of the columns in the data
print(df.columns)
276/8:
# checking column datatypes and number of non-null values
df.info()
276/9:
# checking for missing values in the data.
df.isnull().sum()
276/10:
df.Year = df.Year.astype("category")  # To convert year into categories
# Uncomment the following code to learn more about the astype function and its attribtes
# help(df.astype)
276/11:
# Let's look at the statistical summary of the data
df.describe(include="all").T
276/12: df.dropna(subset=["Price"], inplace=True)
276/13:
# checking missing values in rest of the data
df.isnull().sum()
276/14: df1 = df.drop(["New_Price"], axis=1, inplace=False)
276/15: df1
276/16:
def power_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "bhp" in var:  # checking for the presence of bhp in the Power
            spl = var.split(" ")  # splitting the value by space
            bhp = float(spl[0].replace("bhp ", ""))  # taking numeric part

            return bhp
        else:
            return float(var.replace("bhp", ""))  # taking numeric part of Power
    else:
        return np.nan  # will return NaN if value is not string
276/17:
df1["Power"] = df1["Power"].apply(power_to_var)
df1.head()
276/18:
def engine_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "CC" in var:  # checking for the presence of CC in the engine
            spl = var.split(" ")  # splitting the value by space
            CC = float(spl[0].replace("CC", ""))  # taking numeric part

            return CC
        else:
            return float(var.replace("CC", ""))  # taking numeric part of engine

    else:
        return np.nan  # will return NaN if value is not string
276/19:
df1["Engine"] = df1["Engine"].apply(engine_to_var)
df1.head()
276/20:
def mileage_to_var(var):
    if isinstance(var, str):  # checking if the value is string or not
        if "km/kg" in var:  # checking for the presence of km/kg in the mileage
            spl = var.split(" ")  # splitting the value by space
            km = float(spl[0].replace("km/kg", ""))  # taking numeric part

            return km

        elif "kmpl" in var:  # checking for the presence of kmpl in the mileage
            spl = var.split(" ")  # splitting the value by space
            kmpl = float(spl[0].replace("kmpl", ""))  # taking numeric part

            return kmpl

        elif "kmpl" in var:
            return float(var.replace("kmpl", ""))  # taking numeric part of mileage

        else:
            return float(var.replace("km/kg", ""))  # taking numeric part of mileage
    else:
        return np.nan  # will return NaN if value is not string
276/21:
df1["Mileage"] = df1["Mileage"].apply(mileage_to_var)
df1.head()
276/22:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
276/23:
# creating a copy of the dataframe
df2 = df1.copy()
276/24: df2.Location.value_counts()
276/25: labeled_barplot(df2, "Location")
276/26: df2.groupby("Location")["Price"].mean()
276/27:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Location")
plt.xticks(rotation=90)

plt.show()
276/28: df2.groupby("Year")["Price"].mean()
276/29:
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.barplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
sns.boxplot(data=df1, y="Price", x="Year")
plt.xticks(rotation=45)

plt.show()
276/30:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
276/31: histogram_boxplot(df2, "Price", bins=50)
276/32: histogram_boxplot(df2, "Kilometers_Driven", bins=10)
276/33: histogram_boxplot(df2, "Power", bins=10)
276/34: histogram_boxplot(df2, "Mileage", bins=10)
276/35:
# creating a list of non-tag columns
corr_cols = [item for item in df2.columns]
print(corr_cols)
276/36:
plt.figure(figsize=(12, 7))
sns.heatmap(
    df2[corr_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()
276/37:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Location", y="Price", data=df2)
plt.show()
276/38:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Fuel_Type", y="Price", data=df2)
plt.show()
276/39:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Transmission", y="Price", data=df2)
plt.show()
276/40:
plt.figure(figsize=(10, 5))
sns.boxplot(x="Owner_Type", y="Price", data=df2)
plt.show()
276/41:
# creating a list of non-tag columns
dist_cols = [item for item in df2.select_dtypes(include=np.number)]

# let's plot a histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df2[dist_cols[i]], bins=50)
    # sns.histplot(data=df1, x=dist_cols[i], kde=True)  # you can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
276/42:
# creating a copy of the dataframe
df3 = df2.copy()

# removing Kilometers_Driven and Seats columns as they have only around two or three values
dist_cols.remove("Kilometers_Driven")
dist_cols.remove("Seats")

# also dropping the Mileage column as it is almost normally distributed
dist_cols.remove("Mileage")
276/43:
# using log transforms on some columns

for col in dist_cols:
    df3[col + "_log"] = np.log(df3[col] + 1)

# dropping the original columns
df3.drop(dist_cols, axis=1, inplace=True)
df3.head()
276/44:
# creating a list of non-tag columns
dist_cols = [item for item in df3.select_dtypes(include=np.number)]

# let's plot histogram of all non-tag columns

plt.figure(figsize=(15, 45))

for i in range(len(dist_cols)):
    plt.subplot(12, 2, i + 1)
    plt.hist(df3[dist_cols[i]], bins=50)
    # sns.histplot(data=df3, x=dist_cols[i], kde=True)  # anyone can comment the previous line and run this one to get distribution curves
    plt.tight_layout()
    plt.title(dist_cols[i], fontsize=25)

plt.show()
276/45:
df4 = df2.copy()
df4.head()
276/46: df4.isnull().sum()
276/47:
df4["Mileage"].fillna(df4.Mileage.median(), inplace=True)
df4.isnull().sum()
276/48:
df4["Engine"].fillna(df4.Engine.median(), inplace=True)
df4.isnull().sum()
276/49:
df4["Power"].fillna(df4.Power.median(), inplace=True)
df4.isnull().sum()
276/50:
df4["Seats"].fillna(df4.Seats.median(), inplace=True)
df4.isnull().sum()
276/51:
X = df4.drop(["Price"], axis=1)
y = df4["Price"]
276/52:
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True,
)

X.head()
276/53:
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)
276/54:
print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])
276/55:
lin_reg_model = LinearRegression()
lin_reg_model.fit(x_train, y_train)
276/56:
# function to compute adjusted R-squared
def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


# function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


# function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    """
    Function to compute different metrics to check regression model performance

    model: regressor
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)  # to compute R-squared
    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared
    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE
    mae = mean_absolute_error(target, pred)  # to compute MAE
    mape = mape_score(target, pred)  # to compute MAPE

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf
276/57:
# Checking model performance on train set
print("Training Performance\n")
lin_reg_model_train_perf = model_performance_regression(lin_reg_model, x_train, y_train)
lin_reg_model_train_perf
276/58:
# Checking model performance on test set
print("Test Performance\n")
lin_reg_model_test_perf = model_performance_regression(lin_reg_model, x_test, y_test)
lin_reg_model_test_perf
277/1:
### TODO:
#    - insantiate a shirt object with the following characteristics:
#        - color red, size S, style long-sleeve, and price 25
#    - store the object in a variable called shirt_one
#
#
###

new_shirt = Shirt('red','S','long-sleeve','25')
new_shirt
277/2:
class Shirt:

    def __init__(self, shirt_color, shirt_size, shirt_style, shirt_price):
        self.color = shirt_color
        self.size = shirt_size
        self.style = shirt_style
        self.price = shirt_price
    
    def change_price(self, new_price):
    
        self.price = new_price
        
    def discount(self, discount):

        return self.price * (1 - discount)
277/3:
### TODO:
#    - insantiate a shirt object with the following characteristics:
#        - color red, size S, style long-sleeve, and price 25
#    - store the object in a variable called shirt_one
#
#
###

new_shirt = Shirt('red','S','long-sleeve','25')
new_shirt
277/4:
### TODO:
#    - insantiate a shirt object with the following characteristics:
#        - color red, size S, style long-sleeve, and price 25
#    - store the object in a variable called shirt_one
#
#
###

new_shirt = Shirt('red','S','long-sleeve','25')
new_shirt()
277/5:
### TODO:
#    - insantiate a shirt object with the following characteristics:
#        - color red, size S, style long-sleeve, and price 25
#    - store the object in a variable called shirt_one
#
#
###

new_shirt = Shirt('red','S','long-sleeve','25')
new_shirt
277/6:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

new_shirt.price()
277/7:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(new_shirt.price
277/8:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(new_shirt.price)
277/9:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(new_shirt.price)
new_shirt.change_price(10)
print(new_shirt.price)
277/10:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(new_shirt.price)
new_shirt.change_price(10)
print(new_shirt.price)
new_shirt.discount(0.12)
print(new_shirt.price)
277/11:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(new_shirt.price)
new_shirt.change_price(10)
print(new_shirt.price)

print(new_shirt.discount(0.12))
277/12:
### TODO:
#    - insantiate a shirt object with the following characteristics:
#        - color red, size S, style long-sleeve, and price 25
#    - store the object in a variable called shirt_one
#
#
###

shirt_one = Shirt('red','S','long-sleeve','25')
shirt_one
277/13:
### TODO:
#     - print the price of the shirt using the price attribute
#     - use the change_price method to change the price of the shirt to 10
#     - print the price of the shirt using the price attribute
#     - use the discount method to print the price of the shirt with a 12% discount
#
###

print(shirt_one.price)
shirt_one.change_price(10)
print(shirt_one.price)

print(shirt_one.discount(0.12))
277/14:
### TODO:
#
#    - instantiate another object with the following characteristics:
# .       - color orange, size L, style short-sleeve, and price 10
#    - store the object in a variable called shirt_two
#
###

shirt_two = Shirt('orange','L','short-sleeve','10')
shirt_two
277/15:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

print(shirt_two.price)
277/16:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

shirt_two.price

total = shirt_one.price + shirt_two.price
277/17:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

shirt_two.price

total = 'shirt_one.price' + 'shirt_two.price'
277/18:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

shirt_two.price

total = 'shirt_one.price' + 'shirt_two.price'
print(total)
277/19:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

print(shirt_two.price)

total = 'shirt_one.price' + 'shirt_two.price'
print(total)
277/20:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

print(shirt_two.price)

total = float(shirt_one.price) + float(shirt_two.price)
print(total)
277/21:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###

print(shirt_two.price)

total = float(shirt_one.price) + float(shirt_two.price)
print(total)
277/22:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###



total = float(shirt_one.price) + float(shirt_two.price)
print(total)
277/23:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
print(shirt_one.discount(0.14))
print(shirt_one.discount(0.06))
277/24:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
print(shirt_one.discount(0.14))
print(shirt_two.discount(0.06))

total_discount = float(shirt_one.discount(0.14) + shirt_two.discount(0.06))
print(total_discount)
277/25:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
a = shirt_one.discount(0.14)
b = shirt_two.discount(0.06)

total_discount = float(a + b)
print(total_discount)
277/26:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
a = shirt_one.discount(0.14)
b = shirt_two.discount(0.06)

total_discount = float('a' + 'b')
print(total_discount)
277/27:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
a = shirt_one.discount(0.14)
b = shirt_two.discount(0.06)

total_discount = float(float(a)+float(b))
print(total_discount)
277/28:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
a = shirt_one.discount(0.14)
b = shirt_two.discount(0.06)

total_discount = float(a)+float(b)
print(total_discount)
277/29:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###


total_discount =  shirt_one.discount(.14) + shirt_two.discount(.06)
277/30:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###



total = shirt_one.price + shirt_two.price
print(total)
277/31:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###



total = float(shirt_one.price + shirt_two.price)
print(total)
277/32:
### TODO:
#
#    - calculate the total cost of shirt_one and shirt_two
#    - store the results in a variable called total
#    
###



total = float('shirt_one.price') + float('shirt_two.price')
print(total)
277/33:
# Unit tests to check your solution
from tests import run_tests

run_tests(shirt_one, shirt_two, total, total_discount)
277/34:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###
a = shirt_one.discount(0.14)
b = shirt_two.discount(0.06)

total_discount = float(a)+float(b)
print(total_discount)
277/35:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###


total_discount =total_discount =  shirt_one.discount(.14) + shirt_two.discount(.06)
277/36:
# Unit tests to check your solution
from tests import run_tests

run_tests(shirt_one, shirt_two, total, total_discount)
277/37:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###


total_discount =  shirt_one.discount(.14) + shirt_two.discount(.06)
277/38:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###


total_discount =  float('shirt_one.discount(.14)') + float('shirt_two.discount(.06)')
277/39:
### TODO:
#
#    - use the shirt discount method to calculate the total cost if
#       shirt_one has a discount of 14% and shirt_two has a discount
#       of 6%
#    - store the results in a variable called total_discount
###


total_discount =  float(shirt_one.discount(.14)) + float(shirt_two.discount(.06))
278/1:
### TODO:
#   - code a Pants class with the following attributes
#   - color (string) eg 'red', 'yellow', 'orange'
#   - waist_size (integer) eg 8, 9, 10, 32, 33, 34
#   - length (integer) eg 27, 28, 29, 30, 31
#   - price (float) eg 9.28


class Pants:

    def __init__(self, Pants_color, Pants_size, Pants_style, Pants_price):
        
        self.color = Pants_color
        self.size = Pants_size
        self.style = Pants_style
        self.price = Pants_price
    
    def change_price(self, new_price):
    
        self.price = new_price
        
        return None
        
    def discount(self, discount):

        return self.price * (1 - discount)
    
### TODO: Declare the Pants Class 

### TODO: write an __init__ function to initialize the attributes

### TODO: write a change_price method:
#    Args:
#        new_price (float): the new price of the shirt
#    Returns:
#        None

### TODO: write a discount method:
#    Args:
#        discount (float): a decimal value for the discount. 
#            For example 0.05 for a 5% discount.
#
#    Returns:
#        float: the discounted price
278/2:
def check_results():
    pants = Pants('red', 35, 36, 15.12)
    assert pants.color == 'red'
    assert pants.waist_size == 35
    assert pants.length == 36
    assert pants.price == 15.12
    
    pants.change_price(10) == 10
    assert pants.price == 10 
    
    assert pants.discount(.1) == 9
    
    print('You made it to the end of the check. Nice job!')

check_results()
278/3:
### TODO:
#   - code a Pants class with the following attributes
#   - color (string) eg 'red', 'yellow', 'orange'
#   - waist_size (integer) eg 8, 9, 10, 32, 33, 34
#   - length (integer) eg 27, 28, 29, 30, 31
#   - price (float) eg 9.28


class Pants:

    def __init__(self, Pants_color, waist_size, length, price):
        
        self.color = Pants_color
        self.size = waist_size
        self.length = length
        self.price = price
    
    def change_price(self, new_price):
    
        self.price = new_price
        
        return None
        
    def discount(self, discount):

        return self.price * (1 - discount)
    
### TODO: Declare the Pants Class 

### TODO: write an __init__ function to initialize the attributes

### TODO: write a change_price method:
#    Args:
#        new_price (float): the new price of the shirt
#    Returns:
#        None

### TODO: write a discount method:
#    Args:
#        discount (float): a decimal value for the discount. 
#            For example 0.05 for a 5% discount.
#
#    Returns:
#        float: the discounted price
278/4:
def check_results():
    pants = Pants('red', 35, 36, 15.12)
    assert pants.color == 'red'
    assert pants.waist_size == 35
    assert pants.length == 36
    assert pants.price == 15.12
    
    pants.change_price(10) == 10
    assert pants.price == 10 
    
    assert pants.discount(.1) == 9
    
    print('You made it to the end of the check. Nice job!')

check_results()
278/5:
### TODO:
#   - code a Pants class with the following attributes
#   - color (string) eg 'red', 'yellow', 'orange'
#   - waist_size (integer) eg 8, 9, 10, 32, 33, 34
#   - length (integer) eg 27, 28, 29, 30, 31
#   - price (float) eg 9.28


class Pants:

    def __init__(self, Pants_color, waist_size, length, price):
        
        self.color = Pants_color
        self.waist_size = waist_size
        self.length = length
        self.price = price
    
    def change_price(self, new_price):
    
        self.price = new_price
        
        return None
        
    def discount(self, discount):

        return self.price * (1 - discount)
    
### TODO: Declare the Pants Class 

### TODO: write an __init__ function to initialize the attributes

### TODO: write a change_price method:
#    Args:
#        new_price (float): the new price of the shirt
#    Returns:
#        None

### TODO: write a discount method:
#    Args:
#        discount (float): a decimal value for the discount. 
#            For example 0.05 for a 5% discount.
#
#    Returns:
#        float: the discounted price
278/6:
def check_results():
    pants = Pants('red', 35, 36, 15.12)
    assert pants.color == 'red'
    assert pants.waist_size == 35
    assert pants.length == 36
    assert pants.price == 15.12
    
    pants.change_price(10) == 10
    assert pants.price == 10 
    
    assert pants.discount(.1) == 9
    
    print('You made it to the end of the check. Nice job!')

check_results()
278/7:
### TODO:
#   - code a Pants class with the following attributes
#   - color (string) eg 'red', 'yellow', 'orange'
#   - waist_size (integer) eg 8, 9, 10, 32, 33, 34
#   - length (integer) eg 27, 28, 29, 30, 31
#   - price (float) eg 9.28


class Pants:

    def __init__(self, Pants_color, waist_size, length, price):
        
        self.color = Pants_color
        self.waist_size = waist_size
        self.length = length
        self.price = price
    
    def change_price(self, new_price):
    
        self.price = new_price
        
        return None
        
    def discount(self, discount):

        return self.price * (1 - discount)
    
### TODO: Declare the Pants Class 

### TODO: write an __init__ function to initialize the attributes

### TODO: write a change_price method:
#    Args:
#        new_price (float): the new price of the shirt
#    Returns:
#        None

### TODO: write a discount method:
#    Args:
#        discount (float): a decimal value for the discount. 
#            For example 0.05 for a 5% discount.
#
#    Returns:
#        float: the discounted price
278/8:
def check_results():
    pants = Pants('red', 35, 36, 15.12)
    assert pants.color == 'red'
    assert pants.waist_size == 35
    assert pants.length == 36
    assert pants.price == 15.12
    
    pants.change_price(10) == 10
    assert pants.price == 10 
    
    assert pants.discount(.1) == 9
    
    print('You made it to the end of the check. Nice job!')

check_results()
280/1:
import math
import matplotlib.pyplot as plt

class Gaussian():
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu = 0, sigma = 1):
        
        self.mean = mu
        self.stdev = sigma
        self.data = []


    
    def calculate_mean(self):
    
        """Method to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
        
        #TODO: Calculate the mean of the data set. Remember that the data set is stored in self.data
        # Change the value of the mean attribute to be the mean of the data set
        # Return the mean of the data set           
        pass
                


    def calculate_stdev(self, sample=True):

        """Method to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        # TODO:
        #   Calculate the standard deviation of the data set
        #   
        #   The sample variable determines if the data set contains a sample or a population
        #   If sample = True, this means the data is a sample. 
        #   Keep the value of sample in mind for calculating the standard deviation
        #
        #   Make sure to update self.stdev and return the standard deviation as well    
            
        pass
        

    def read_data_file(self, file_name, sample=True):
    
        """Method to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute. 
        After reading in the file, the mean and standard deviation are calculated
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
        
        # This code opens a data file and appends the data to a list called data_list
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        # TODO: 
        #   Update the self.data attribute with the data_list
        #   Update self.mean with the mean of the data_list. 
        #       You can use the calculate_mean() method with self.calculate_mean()
        #   Update self.stdev with the standard deviation of the data_list. Use the 
        #       calcaulte_stdev() method.
                
        
    def plot_histogram(self):
        """Method to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        
        # TODO: Plot a histogram of the data_list using the matplotlib package.
        #       Be sure to label the x and y axes and also give the chart a title
        
                
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        # TODO: Calculate the probability density function of the Gaussian distribution
        #       at the value x. You'll need to use self.stdev and self.mean to do the calculation
        pass        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Method to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        #TODO: Nothing to do for this method. Try it out and see how it works.
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
280/2:
import math
import matplotlib.pyplot as plt

class Gaussian():
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu = 0, sigma = 1):
        
        self.mean = mu
        self.stdev = sigma
        self.data = []


    
    def calculate_mean(self):
    
        """Method to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
        
        #TODO: Calculate the mean of the data set. Remember that the data set is stored in self.data
        # Change the value of the mean attribute to be the mean of the data set
        # Return the mean of the data set           
        pass
                


    def calculate_stdev(self, sample=True):

        """Method to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        # TODO:
        #   Calculate the standard deviation of the data set
        #   
        #   The sample variable determines if the data set contains a sample or a population
        #   If sample = True, this means the data is a sample. 
        #   Keep the value of sample in mind for calculating the standard deviation
        #
        #   Make sure to update self.stdev and return the standard deviation as well    
            
        pass
        

    def read_data_file(self, file_name, sample=True):
    
        """Method to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute. 
        After reading in the file, the mean and standard deviation are calculated
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
        
        # This code opens a data file and appends the data to a list called data_list
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        # TODO: 
        #   Update the self.data attribute with the data_list
        #   Update self.mean with the mean of the data_list. 
        #       You can use the calculate_mean() method with self.calculate_mean()
        #   Update self.stdev with the standard deviation of the data_list. Use the 
        #       calcaulte_stdev() method.
                
        
    def plot_histogram(self):
        """Method to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        
        # TODO: Plot a histogram of the data_list using the matplotlib package.
        #       Be sure to label the x and y axes and also give the chart a title
        
                
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        # TODO: Calculate the probability density function of the Gaussian distribution
        #       at the value x. You'll need to use self.stdev and self.mean to do the calculation
        pass        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Method to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        #TODO: Nothing to do for this method. Try it out and see how it works.
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
280/3:
# Unit tests to check your solution

import unittest

class TestGaussianClass(unittest.TestCase):
    def setUp(self):
        self.gaussian = Gaussian(25, 2)

    def test_initialization(self): 
        self.assertEqual(self.gaussian.mean, 25, 'incorrect mean')
        self.assertEqual(self.gaussian.stdev, 2, 'incorrect standard deviation')

    def test_pdf(self):
        self.assertEqual(round(self.gaussian.pdf(25), 5), 0.19947,\
         'pdf function does not give expected result') 

    def test_meancalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(self.gaussian.calculate_mean(),\
         sum(self.gaussian.data) / float(len(self.gaussian.data)), 'calculated mean not as expected')

    def test_stdevcalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(round(self.gaussian.stdev, 2), 92.87, 'sample standard deviation incorrect')
        self.gaussian.read_data_file('numbers.txt', False)
        self.assertEqual(round(self.gaussian.stdev, 2), 88.55, 'population standard deviation incorrect')
                
tests = TestGaussianClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
280/4:
import math
import matplotlib.pyplot as plt

class Gaussian():
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu = 0, sigma = 1):
        
        self.mean = mu
        self.stdev = sigma
        self.data = []

    
    def calculate_mean(self):
    
        """Function to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
                    
        avg = 1.0 * sum(self.data) / len(self.data)
        
        self.mean = avg
        
        return self.mean



    def calculate_stdev(self, sample=True):

        """Function to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        if sample:
            n = len(self.data) - 1
        else:
            n = len(self.data)
    
        mean = self.mean
    
        sigma = 0
    
        for d in self.data:
            sigma += (d - mean) ** 2
        
        sigma = math.sqrt(sigma / n)
    
        self.stdev = sigma
        
        return self.stdev
        

    def read_data_file(self, file_name, sample=True):
    
        """Function to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute. 
        After reading in the file, the mean and standard deviation are calculated
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
            
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        self.data = data_list
        self.mean = self.calculate_mean()
        self.stdev = self.calculate_stdev(sample)
        
        
    def plot_histogram(self):
        """Function to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        plt.hist(self.data)
        plt.title('Histogram of Data')
        plt.xlabel('data')
        plt.ylabel('count')
        
        
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        return (1.0 / (self.stdev * math.sqrt(2*math.pi))) * math.exp(-0.5*((x - self.mean) / self.stdev) ** 2)
        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Function to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
280/5:
# Unit tests to check your solution

import unittest

class TestGaussianClass(unittest.TestCase):
    def setUp(self):
        self.gaussian = Gaussian(25, 2)

    def test_initialization(self): 
        self.assertEqual(self.gaussian.mean, 25, 'incorrect mean')
        self.assertEqual(self.gaussian.stdev, 2, 'incorrect standard deviation')

    def test_pdf(self):
        self.assertEqual(round(self.gaussian.pdf(25), 5), 0.19947,\
         'pdf function does not give expected result') 

    def test_meancalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(self.gaussian.calculate_mean(),\
         sum(self.gaussian.data) / float(len(self.gaussian.data)), 'calculated mean not as expected')

    def test_stdevcalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(round(self.gaussian.stdev, 2), 92.87, 'sample standard deviation incorrect')
        self.gaussian.read_data_file('numbers.txt', False)
        self.assertEqual(round(self.gaussian.stdev, 2), 88.55, 'population standard deviation incorrect')
                
tests = TestGaussianClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
281/1:
import math
import matplotlib.pyplot as plt

class Gaussian():
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu = 0, sigma = 1):
        
        self.mean = mu
        self.stdev = sigma
        self.data = []

    
    def calculate_mean(self):
    
        """Function to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
                    
        avg = 1.0 * sum(self.data) / len(self.data)
        
        self.mean = avg
        
        return self.mean



    def calculate_stdev(self, sample=True):

        """Function to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        if sample:
            n = len(self.data) - 1
        else:
            n = len(self.data)
    
        mean = self.mean
    
        sigma = 0
    
        for d in self.data:
            sigma += (d - mean) ** 2
        
        sigma = math.sqrt(sigma / n)
    
        self.stdev = sigma
        
        return self.stdev
        

    def read_data_file(self, file_name, sample=True):
    
        """Function to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute. 
        After reading in the file, the mean and standard deviation are calculated
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
            
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        self.data = data_list
        self.mean = self.calculate_mean()
        self.stdev = self.calculate_stdev(sample)
        
        
    def plot_histogram(self):
        """Function to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        plt.hist(self.data)
        plt.title('Histogram of Data')
        plt.xlabel('data')
        plt.ylabel('count')
        
        
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        return (1.0 / (self.stdev * math.sqrt(2*math.pi))) * math.exp(-0.5*((x - self.mean) / self.stdev) ** 2)
        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Function to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
        
    def __add__(self, other):
        
        """Function to add together two Gaussian distributions
        
        Args:
            other (Gaussian): Gaussian instance
            
        Returns:
            Gaussian: Gaussian distribution
            
        """
        
        result = Gaussian()
        result.mean = self.mean + other.mean
        result.stdev = math.sqrt(self.stdev ** 2 + other.stdev ** 2)
        
        return result
        
        
    def __repr__(self):
    
        """Function to output the characteristics of the Gaussian instance
        
        Args:
            None
        
        Returns:
            string: characteristics of the Gaussian
        
        """
        
        return "mean {}, standard deviation {}".format(self.mean, self.stdev)
281/2:
# Unit tests to check your solution

import unittest

class TestGaussianClass(unittest.TestCase):
    def setUp(self):
        self.gaussian = Gaussian(25, 2)

    def test_initialization(self): 
        self.assertEqual(self.gaussian.mean, 25, 'incorrect mean')
        self.assertEqual(self.gaussian.stdev, 2, 'incorrect standard deviation')

    def test_pdf(self):
        self.assertEqual(round(self.gaussian.pdf(25), 5), 0.19947,\
         'pdf function does not give expected result') 

    def test_meancalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(self.gaussian.calculate_mean(),\
         sum(self.gaussian.data) / float(len(self.gaussian.data)), 'calculated mean not as expected')

    def test_stdevcalculation(self):
        self.gaussian.read_data_file('numbers.txt', True)
        self.assertEqual(round(self.gaussian.stdev, 2), 92.87, 'sample standard deviation incorrect')
        self.gaussian.read_data_file('numbers.txt', False)
        self.assertEqual(round(self.gaussian.stdev, 2), 88.55, 'population standard deviation incorrect')

    def test_add(self):
        gaussian_one = Gaussian(25, 3)
        gaussian_two = Gaussian(30, 4)
        gaussian_sum = gaussian_one + gaussian_two
        
        self.assertEqual(gaussian_sum.mean, 55)
        self.assertEqual(gaussian_sum.stdev, 5)

    def test_repr(self):
        gaussian_one = Gaussian(25, 3)
        
        self.assertEqual(str(gaussian_one), "mean 25, standard deviation 3")
        
tests = TestGaussianClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/1:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Pants(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price, country_of_origin)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/2:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/3:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price, country_of_origin)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/4:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/5:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price):
        
        Clothing.__init__(self, color, size, style, price)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/6:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/7:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price, country_of_origin)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/8:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/9:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
        
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price, country_of_origin)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
   
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/10:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/11:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
    def calculate_shipping(self, weight, rate):
        self.weight = weight
        self.rate = rate
        return wetight*rate
        
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
   
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/12:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/13:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        self.weight = weight
        self.rate = rate
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
    
    def calculate_shipping(self, weight, rate):
        return weight * rate
        
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
   
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/14:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
282/15:
class Clothing:

    def __init__(self, color, size, style, price):
        self.color = color
        self.size = size
        self.style = style
        self.price = price
        
        
    def change_price(self, price):
        self.price = price
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount)
    
    def calculate_shipping(self, weight, rate):
        
        self.weight = weight
        self.rate = rate
        
        return weight * rate
        
        
class Shirt(Clothing):
    
    def __init__(self, color, size, style, price, long_or_short):
        
        Clothing.__init__(self, color, size, style, price)
        self.long_or_short = long_or_short
    
    def double_price(self):
        self.price = 2*self.price
    
class Pants(Clothing):

    def __init__(self, color, size, style, price, waist):
        
        Clothing.__init__(self, color, size, style, price)
        self.waist = waist
        
    def calculate_discount(self, discount):
        return self.price * (1 - discount / 2)
    
# TODO: Write a class called Blouse, that inherits from the Clothing class
# and has the the following attributes and methods:
#   attributes: color, size, style, price, country_of_origin
#     where country_of_origin is a string that holds the name of a 
#     country
#
#   methods: triple_price, which has no inputs and returns three times
#     the price of the blouse
#
#
class Blouse(Clothing):

    def __init__(self, color, size, style, price, country_of_origin):
        
        Clothing.__init__(self, color, size, style, price)
        self.country_of_origin = country_of_origin
        
    def triple_price(self):
        self.price = 3*self.price
        
   
    
# TODO: Add a method to the clothing class called calculate_shipping.
#   The method has two inputs: weight and rate. Weight is a float
#   representing the weight of the article of clothing. Rate is a float
#   representing the shipping weight. The method returns weight * rate
282/16:
# Unit tests to check your solution

import unittest

class TestClothingClass(unittest.TestCase):
    def setUp(self):
        self.clothing = Clothing('orange', 'M', 'stripes', 35)
        self.blouse = Blouse('blue', 'M', 'luxury', 40, 'Brazil')
        self.pants = Pants('black', 32, 'baggy', 60, 30)
        
    def test_initialization(self): 
        self.assertEqual(self.clothing.color, 'orange', 'color should be orange')
        self.assertEqual(self.clothing.price, 35, 'incorrect price')
        
        self.assertEqual(self.blouse.color, 'blue', 'color should be blue')
        self.assertEqual(self.blouse.size, 'M', 'incorrect size')
        self.assertEqual(self.blouse.style, 'luxury', 'incorrect style')
        self.assertEqual(self.blouse.price, 40, 'incorrect price')
        self.assertEqual(self.blouse.country_of_origin, 'Brazil', 'incorrect country of origin')

    def test_calculateshipping(self):
        self.assertEqual(self.clothing.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 

        self.assertEqual(self.blouse.calculate_shipping(.5, 3), .5 * 3,\
         'Clothing shipping calculation not as expected') 
    
tests = TestClothingClass()

tests_loaded = unittest.TestLoader().loadTestsFromModule(tests)

unittest.TextTestRunner().run(tests_loaded)
283/1:
class Distribution:
    
    def __init__(self, mu=0, sigma=1):
    
        """ Generic distribution class for calculating and 
        visualizing a probability distribution.
    
        Attributes:
            mean (float) representing the mean value of the distribution
            stdev (float) representing the standard deviation of the distribution
            data_list (list of floats) a list of floats extracted from the data file
            """
        
        self.mean = mu
        self.stdev = sigma
        self.data = []


    def read_data_file(self, file_name):
    
        """Function to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute.
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
            
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        self.data = data_list
283/2:
import math
import matplotlib.pyplot as plt

class Gaussian(Distribution):
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu=0, sigma=1):
        
        Distribution.__init__(self, mu, sigma)
    
        
    
    def calculate_mean(self):
    
        """Function to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
                    
        avg = 1.0 * sum(self.data) / len(self.data)
        
        self.mean = avg
        
        return self.mean



    def calculate_stdev(self, sample=True):

        """Function to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        if sample:
            n = len(self.data) - 1
        else:
            n = len(self.data)
    
        mean = self.calculate_mean()
    
        sigma = 0
    
        for d in self.data:
            sigma += (d - mean) ** 2
        
        sigma = math.sqrt(sigma / n)
    
        self.stdev = sigma
        
        return self.stdev
        
        
        
    def plot_histogram(self):
        """Function to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        plt.hist(self.data)
        plt.title('Histogram of Data')
        plt.xlabel('data')
        plt.ylabel('count')
        
        
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        return (1.0 / (self.stdev * math.sqrt(2*math.pi))) * math.exp(-0.5*((x - self.mean) / self.stdev) ** 2)
        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Function to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
        
    def __add__(self, other):
        
        """Function to add together two Gaussian distributions
        
        Args:
            other (Gaussian): Gaussian instance
            
        Returns:
            Gaussian: Gaussian distribution
            
        """
        
        result = Gaussian()
        result.mean = self.mean + other.mean
        result.stdev = math.sqrt(self.stdev ** 2 + other.stdev ** 2)
        
        return result
        
        
    def __repr__(self):
    
        """Function to output the characteristics of the Gaussian instance
        
        Args:
            None
        
        Returns:
            string: characteristics of the Gaussian
        
        """
        
        return "mean {}, standard deviation {}".format(self.mean, self.stdev)
283/3:
# initialize two gaussian distributions
gaussian_one = Gaussian(25, 3)
gaussian_two = Gaussian(30, 2)

# initialize a third gaussian distribution reading in a data efile
gaussian_three = Gaussian()
gaussian_three.read_data_file('numbers.txt')
gaussian_three.calculate_mean()
gaussian_three.calculate_stdev()
283/4:
# print out the mean and standard deviations
print(gaussian_one.mean)
print(gaussian_two.mean)

print(gaussian_one.stdev)
print(gaussian_two.stdev)

print(gaussian_three.mean)
print(gaussian_three.stdev)
283/5:
# plot histogram of gaussian three
gaussian_three.plot_histogram_pdf()
283/6:
# add gaussian_one and gaussian_two together
gaussian_one + gaussian_two
284/1:
import math
import matplotlib.pyplot as plt

class Gaussian():
    """ Gaussian distribution class for calculating and 
    visualizing a Gaussian distribution.
    
    Attributes:
        mean (float) representing the mean value of the distribution
        stdev (float) representing the standard deviation of the distribution
        data_list (list of floats) a list of floats extracted from the data file
            
    """
    def __init__(self, mu = 0, sigma = 1):
        
        self.mean = mu
        self.stdev = sigma
        self.data = []

    
    def calculate_mean(self):
    
        """Function to calculate the mean of the data set.
        
        Args: 
            None
        
        Returns: 
            float: mean of the data set
    
        """
                    
        avg = 1.0 * sum(self.data) / len(self.data)
        
        self.mean = avg
        
        return self.mean



    def calculate_stdev(self, sample=True):

        """Function to calculate the standard deviation of the data set.
        
        Args: 
            sample (bool): whether the data represents a sample or population
        
        Returns: 
            float: standard deviation of the data set
    
        """

        if sample:
            n = len(self.data) - 1
        else:
            n = len(self.data)
    
        mean = self.mean
    
        sigma = 0
    
        for d in self.data:
            sigma += (d - mean) ** 2
        
        sigma = math.sqrt(sigma / n)
    
        self.stdev = sigma
        
        return self.stdev
        

    def read_data_file(self, file_name, sample=True):
    
        """Function to read in data from a txt file. The txt file should have
        one number (float) per line. The numbers are stored in the data attribute. 
        After reading in the file, the mean and standard deviation are calculated
                
        Args:
            file_name (string): name of a file to read from
        
        Returns:
            None
        
        """
            
        with open(file_name) as file:
            data_list = []
            line = file.readline()
            while line:
                data_list.append(int(line))
                line = file.readline()
        file.close()
    
        self.data = data_list
        self.mean = self.calculate_mean()
        self.stdev = self.calculate_stdev(sample)
        
        
    def plot_histogram(self):
        """Function to output a histogram of the instance variable data using 
        matplotlib pyplot library.
        
        Args:
            None
            
        Returns:
            None
        """
        plt.hist(self.data)
        plt.title('Histogram of Data')
        plt.xlabel('data')
        plt.ylabel('count')
        
        
        
    def pdf(self, x):
        """Probability density function calculator for the gaussian distribution.
        
        Args:
            x (float): point for calculating the probability density function
            
        
        Returns:
            float: probability density function output
        """
        
        return (1.0 / (self.stdev * math.sqrt(2*math.pi))) * math.exp(-0.5*((x - self.mean) / self.stdev) ** 2)
        

    def plot_histogram_pdf(self, n_spaces = 50):

        """Function to plot the normalized histogram of the data and a plot of the 
        probability density function along the same range
        
        Args:
            n_spaces (int): number of data points 
        
        Returns:
            list: x values for the pdf plot
            list: y values for the pdf plot
            
        """
        
        mu = self.mean
        sigma = self.stdev

        min_range = min(self.data)
        max_range = max(self.data)
        
         # calculates the interval between x values
        interval = 1.0 * (max_range - min_range) / n_spaces

        x = []
        y = []
        
        # calculate the x values to visualize
        for i in range(n_spaces):
            tmp = min_range + interval*i
            x.append(tmp)
            y.append(self.pdf(tmp))

        # make the plots
        fig, axes = plt.subplots(2,sharex=True)
        fig.subplots_adjust(hspace=.5)
        axes[0].hist(self.data, density=True)
        axes[0].set_title('Normed Histogram of Data')
        axes[0].set_ylabel('Density')

        axes[1].plot(x, y)
        axes[1].set_title('Normal Distribution for \n Sample Mean and Sample Standard Deviation')
        axes[0].set_ylabel('Density')
        plt.show()

        return x, y
288/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
288/2:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
288/3:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
286/1:
cData = pd.read_csv("Inc_Exp_Data.csv")  
cData.shape
286/2:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
286/3:
cData = pd.read_csv("Inc_Exp_Data.csv")  
cData.shape
286/4:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
289/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
289/2:
cData = pd.read_csv("auto-mpg.csv")  
cData.shape
289/3:
# 8 variables: 
#
# MPG (miles per gallon), 
# cylinders, 
# engine displacement (cu. inches), 
# horsepower,
# vehicle weight (lbs.), 
# time to accelerate from O to 60 mph (sec.),
# model year (modulo 100), and 
# origin of car (1. American, 2. European,3. Japanese).
#
# Also provided are the car labels (types) 
# Missing data values are marked by series of question marks.


cData.head()
288/4:
cData = pd.read_csv("Inc_Exp_Data.csv")  
cData.shape
288/5:
df = pd.read_csv("Inc_Exp_Data.csv") 
df.shape
288/6:
df = pd.read_csv("Inc_Exp_Data.csv") 
df.head()
288/7: df.describe()
288/8: df['Mthly_HH_Expense'].mean()
288/9: df['Mthly_HH_Expense'].median()
288/10: df['Mthly_HH_Expense'].mode()
291/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
291/2:
# loading the dataset
data = pd.read_csv("used_cars_data.csv", index_col=0)
291/3:
# let's view a sample of the data
data.sample(
    5, random_state=2
)  # setting the random_state will ensure we get the same results every time
291/4:
# creating a copy of the data so that original data remains unchanged
df = data.copy()
291/5:
# checking for duplicate values in the data
df.duplicated().sum()
291/6:
# checking the names of the columns in the data
print(df.columns)
291/7:
# loading the dataset
df = pd.read_csv("used_cars_data.csv", index_col=0)
292/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
pd.set_option('display.float_format', lambda x: '%.5f' % x) # To supress numerical display in scientific notations
292/2: cardiofit = pd.read_csv("CardioGoodFitness.csv") #To read the csv file
292/3: cardiofit.head() #To get the top 5 data printed
292/4: cardiofit.shape
292/5: cardiofit.dtypes
292/6: cardiofit.info()
292/7: cardiofit.isnull().sum()
292/8: goodfit=cardiofit.copy() # creating a copy of the provided dataset is a good practice
292/9: cardiofit.describe().round(2)
292/10:
data= goodfit.groupby(['Age'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data
292/11:
data2= goodfit.groupby(['Fitness'])['Income'].mean().sort_values(ascending = False).reset_index().head()
data2
292/12:
data3= goodfit.groupby(['Income'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data3
292/13:
data4= goodfit.groupby(['Age'])['Miles'].mean().sort_values(ascending = False).reset_index().head()
data4
292/14: ### To take some subsets of the given data [data, data2, data3,data4]that will be used later
292/15:
cardiofit.groupby(by=['MaritalStatus'])['Income'].sum().reset_index().sort_values(['Income']).tail(10).plot(x='MaritalStatus',
                                                                                                           y='Income',
                                                                                                           kind='bar',
                                                                                                           figsize=(15,5))
plt.show()
292/16:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=goodfit, palette='muted')  # barplot
292/17:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Fitness', y='Income', data=data2, palette='muted')  # barplot
292/18:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='Income', y='Miles', data=data3, palette='muted')  # barplot
292/19:
plt.figure(figsize=(10,5))  # setting the figure size
ax = sns.barplot(x='MaritalStatus', y='Income', data=cardiofit, palette='muted')  # barplot
292/20:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Usage'])
plt.show()
292/21:
plt.figure(figsize=(10,5))
sns.violinplot(x=goodfit.Product, y=goodfit['Income'])
plt.show()
292/22:
plt.figure(figsize=(15, 7))
sns.histplot(goodfit.Income)
plt.show()
292/23:
sns.boxplot(data = goodfit, x = 'Income')
plt.show()
288/11:
plt.figure(figsize=(15, 7))
sns.histplot(df.Highest_Qualified_Member)
plt.show()
288/12:
plt.figure(figsize=(15, 7))
sns.histplot(df.Highest_Qualified_Member, kind='kde')
plt.show()
288/13:
plt.figure(figsize=(15, 7))
sns.histplot(df.Highest_Qualified_Member, kde= True)
plt.show()
288/14: df['Highest_Qualified_Member'].value_counts().plot(kind='bar')
288/15:
mth_exp_tmp = pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp[mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()]
288/16: pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
288/17: mth_exp_tmp.reset_index(inplace=True)
288/18:
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp
288/19: mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()
288/20: mth_exp_tmp[mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()]
288/21: mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()
288/22:
mth_exp_tmp = pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()
288/23:
mth_exp_tmp = pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp[mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()]
288/24:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()
288/25:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()
df = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
288/26:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()
#df = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
288/27:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()
288/28:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = inc_exp["Mthly_HH_Income"].quantile(0.75) - inc_exp["Mthly_HH_Income"].quantile(0.25)
288/29:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
288/30:
df = pd.read_csv("Inc_Exp_Data.csv") 
df.head()
288/31: df.describe()
288/32: df['Mthly_HH_Expense'].mean()
288/33: df['Mthly_HH_Expense'].median()
288/34: pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
288/35:
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp
288/36: mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()
288/37:
mth_exp_tmp = pd.crosstab(index=df["Mthly_HH_Expense"], columns="count")
mth_exp_tmp.reset_index(inplace=True)
mth_exp_tmp[mth_exp_tmp['count'] == df.Mthly_HH_Expense.value_counts().max()]
288/38:
plt.figure(figsize=(15, 7))
sns.histplot(df.Highest_Qualified_Member, kde= True)
plt.show()
288/39: df['Highest_Qualified_Member'].value_counts().plot(kind='bar')
288/40:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = inc_exp["Mthly_HH_Income"].quantile(0.75) - inc_exp["Mthly_HH_Income"].quantile(0.25)
288/41:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = inc_exp["Mthly_HH_Income"].quantile(0.75) - inc_exp["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
288/42:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
288/43:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
print()
288/44:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
print()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
IQR_e = inc_exp["Mthly_HH_Expense"].quantile(0.75) - inc_exp["Mthly_HH_Expense"].quantile(0.25)
288/45:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
print()



IQR_e = df["Mthly_HH_Expense"].quantile(0.75) - df["Mthly_HH_expense"].quantile(0.25)
print("IQR for Monthly Household Expense",IQR_e)
288/46:
df.plot(x="Mthly_HH_Income", y="Mthly_HH_Expense")
plt.show()

IQR_i = df["Mthly_HH_Income"].quantile(0.75) - df["Mthly_HH_Income"].quantile(0.25)
print("IQR for Monthly Household Income",IQR_i)
print()



IQR_e = df["Mthly_HH_Expense"].quantile(0.75) - df["Mthly_HH_Expense"].quantile(0.25)
print("IQR for Monthly Household Expense",IQR_e)
288/47: df.iloc[:,0:4]
288/48: df.iloc[:,0:4].std()
288/49: df.iloc[:,0:4].std().to_frame()
288/50: pd.DataFrame(df.iloc[:,0:4].std().to_frame()).T
288/51: df.iloc[:,0:4]
288/52: df.iloc[:,0:4].std()
288/53: df.iloc[:,0:4].std().to_frame()
288/54: df.iloc[:,0:4].std().to_frame().T
288/55: pd.DataFrame(df.iloc[:,0:4].std().to_frame()).T
288/56: df.iloc[:,0:3].var()
288/57: df.iloc[:,0:3].var().round(3)
288/58: df.iloc[:,0:3].var().to_frame()
288/59:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
pd.options.display.float_format = '{:,.2f}'.format
288/60: df.iloc[:,0:3].var().to_frame()
288/61: df.iloc[:,0:3].var().to_frame().T
288/62: pd.DataFrame(df.iloc[:,0:3].var().to_frame()).T
288/63: df["Highest_Qualified_Member"].value_counts()
288/64: df["Highest_Qualified_Member"].value_counts().to_frame()
288/65: pd.DataFrame(df["Highest_Qualified_Member"].value_counts().to_frame())
288/66: pd.DataFrame(df["Highest_Qualified_Member"].value_counts().to_frame()).T
288/67: df["No_of_Earning_Members"].value_counts()
288/68: df["No_of_Earning_Members"].value_counts().plot(kind="bar")
288/69:
df["No_of_Earning_Members"].value_counts().plot(kind="bar")
plt.show()
295/1:
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt       # matplotlib.pyplot plots data
%matplotlib inline 
import seaborn as sns
pd.set_option('mode.chained_assignment', None)
295/2: pdata = pd.read_csv("pima-indians-diabetes.csv")
295/3: pdata.shape # Check number of columns and rows in data frame
295/4: pdata.head() # To check first 5 rows of data set
295/5: pdata.isnull().values.any() # If there are any null values in data set
295/6:
columns = list(pdata)[0:-1] # Excluding Outcome column which has only 
pdata[columns].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2)); 
# Histogram of first 8 columns
295/7: pdata.corr() # It will show correlation matrix
295/8:
# However we want to see correlation in graphical representation so below is function for that
def plot_corr(df, size=11):
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)
    plt.xticks(range(len(corr.columns)), corr.columns)
    plt.yticks(range(len(corr.columns)), corr.columns)
    for (i, j), z in np.ndenumerate(corr):
        ax.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')
295/9: plot_corr(pdata)
295/10: sns.pairplot(pdata,diag_kind='kde')
295/11:
n_true = len(pdata.loc[pdata['class'] == True])
n_false = len(pdata.loc[pdata['class'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
295/12:
from sklearn.model_selection import train_test_split

X = pdata.drop('class',axis=1)     # Predictor feature columns (8 X m)
Y = pdata['class']   # Predicted class (1=True, 0=False) (1 X m)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
# 1 is just any random seed number

x_train.head()
295/13:
print("{0:0.2f}% data is in training set".format((len(x_train)/len(pdata.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(x_test)/len(pdata.index)) * 100))
295/14:
print("Original Diabetes True Values    : {0} ({1:0.2f}%)".format(len(pdata.loc[pdata['class'] == 1]), (len(pdata.loc[pdata['class'] == 1])/len(pdata.index)) * 100))
print("Original Diabetes False Values   : {0} ({1:0.2f}%)".format(len(pdata.loc[pdata['class'] == 0]), (len(pdata.loc[pdata['class'] == 0])/len(pdata.index)) * 100))
print("")
print("Training Diabetes True Values    : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))
print("Training Diabetes False Values   : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))
print("")
print("Test Diabetes True Values        : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))
print("Test Diabetes False Values       : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))
print("")
295/15: x_train.head(10)
295/16:
# note that the in the video of this explanation, we have imputed all the zereos. However, this might not be a good approach
# as we have other columns where we might need to keep the zeroes as they are. Ex.-pregnancy column

# Hence, we will get the imputer to run on the relevant columns only.

from sklearn.impute import SimpleImputer
rep_0 = SimpleImputer(missing_values=0, strategy="mean")
cols=['Plas','Pres','skin','test','mass','pedi']

x_train[cols] = rep_0.fit_transform(x_train[cols])
x_test[cols] = rep_0.transform(x_test[cols])

x_test.head(10)
295/17:
from sklearn import metrics

from sklearn.linear_model import LogisticRegression

# Fit the model on train
model = LogisticRegression(solver="liblinear", random_state=1)
model.fit(x_train, y_train)
#predict on test
y_predict = model.predict(x_test)


coef_df = pd.DataFrame(model.coef_)
coef_df['intercept'] = model.intercept_
print(coef_df)
295/18:
model_score = model.score(x_test, y_test)
print(model_score)
295/19:
cm=metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])

df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 1"," Actual 0"]],
                  columns = [i for i in ["Predict 1","Predict 0"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True,fmt='g')
plt.show()
298/1:
data.native_country = data.native_country.str.replace(' ','') # remove empty spaces from strings
data.native_country.head()
298/2:
import warnings
warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# Libraries to help with model building
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
pd.set_option("display.max_columns", None)
# pd.set_option('display.max_rows', None)
pd.set_option("display.max_rows", 200)

# for statistical analysis 
# import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant


# To get diferent metric scores
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, f1_score
from sklearn.model_selection import GridSearchCV
298/3: who = pd.read_csv("who_data.csv")
298/4:
# copying data to another variable to avoid any changes to original data
data = who.copy()
298/5: data.head()
298/6: data.tail()
298/7: data.shape
298/8: data.info()
298/9:
# fixing the names of columns as there are dots, spaces  in columns names
data.columns =[col.replace(' ','_') for col in data.columns]
data.columns =[col.replace('-','_') for col in data.columns]
data.columns =[col.replace('.','') for col in data.columns]
298/10: print(data.columns)
298/11: data.describe(include='all')
298/12:
# filtering object type columns
cat_columns = data.describe(include=['object']).columns
cat_columns
298/13:
for i in cat_columns:
    print('Unique values in',i, 'are :')
    print(data[i].value_counts())
    print('*'*50)
298/14:
data.workclass = data.workclass.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.native_country = data.native_country.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.occupation = data.occupation.apply(lambda x: 'Unknown' if x == ' ?' else x)
298/15: print(f'There are a {data.native_country.nunique()} distinct countries in the data set, we can reduce them to their respective continents.')
298/16:
data.native_country = data.native_country.str.replace(' ','') # remove empty spaces from strings
data.native_country.head()
298/17:
data.native_country = data.native_country.str.replace(' ','') # remove empty spaces from strings
data.native_country.head(15)
298/18:
data['native_country'] = data['native_country'].apply(region_combining)
data['native_country']
298/19:
# filtering object type columns
cat_columns = data.describe(include=['object']).columns
cat_columns
298/20:
for i in cat_columns:
    print('Unique values in',i, 'are :')
    print(data[i].value_counts())
    print('*'*50)
298/21:
data.workclass = data.workclass.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.native_country = data.native_country.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.occupation = data.occupation.apply(lambda x: 'Unknown' if x == ' ?' else x)
298/22: print(f'There are a {data.native_country.nunique()} distinct countries in the data set, we can reduce them to their respective continents.')
298/23:
data.native_country = data.native_country.str.replace(' ','') # remove empty spaces from strings
data.native_country.head(15)
298/24:
north_america  = ["Canada", "Cuba", "Dominican-Republic", "El-Salvador", "Guatemala",
                   "Haiti", "Honduras", "Jamaica", "Mexico", "Nicaragua",
                   "Outlying-US(Guam-USVI-etc)", "Puerto-Rico", "Trinadad&Tobago",
                   "United-States"]
asia  = ["Cambodia", "China", "Hong", "India", "Iran", "Japan", "Laos",
          "Philippines", "Taiwan", "Thailand", "Vietnam"]
south_america = ["Columbia", "Ecuador", "Peru"]
europe = ["England", "France", "Germany", "Greece", "Holand-Netherlands",
            "Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland",
            "Yugoslavia"]
other  = ["South", "Unknown","?"]
298/25:
def region_combining(x):
    if x in north_america:
        return 'north_america'
    elif x in asia:
        return 'asia'
    elif x in south_america:
        return 'south_america'
    elif x in europe:
        return 'europe'
    elif x in other:
        return 'other'
    else:
        return x
298/26:
data['native_country'] = data['native_country'].apply(region_combining)
data['native_country']
298/27: print('Distinct values in Native Country column:', data.native_country.nunique())
298/28: data.marrital_status.unique()
298/29:
data.marrital_status = data.marrital_status.str.replace(' ','') # remove empty spaces from strings

married = ['Married-civ-spouse','Married-AF-spouse','Married-spouse-absent']
not_married = ['Divorced','Separated','Widowed']

def reduce_marital_status(x):
    if x in married:
        return 'married'
    elif x in not_married:
        return 'not_married'
    else:
        return x
298/30: data.marrital_status = data.marrital_status.apply(reduce_marital_status)
298/31: data.marrital_status.unique()
298/32:
# While doing uni-variate analysis of numerical variables we want to study their central tendency and dispersion.
# Let us write a function that will help us create boxplot and histogram for any input numerical 
# variable.
# This function takes the numerical column as the input and returns the boxplots 
# and histograms for the variable.
# Let us see if this help us write faster and cleaner code.
def histogram_boxplot(feature, figsize=(15,10), bins = None):
    """ Boxplot and histogram combined
    feature: 1-d feature array
    figsize: size of fig (default (9,8))
    bins: number of bins (default None / auto)
    """
    sns.set(font_scale=2) # setting the font scale  of the seaborn
    f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2
                                           sharex = True, # x-axis will be shared among all subplots
                                           gridspec_kw = {"height_ratios": (.25, .75)}, 
                                           figsize = figsize 
                                           ) # creating the 2 subplots
    sns.boxplot(feature, ax=ax_box2, showmeans=True, color='red') # boxplot will be created and a star will indicate the mean value of the column
    sns.distplot(feature, kde=False, ax=ax_hist2, bins=bins) if bins else sns.distplot(feature, kde=False, ax=ax_hist2) # For histogram
    ax_hist2.axvline(np.mean(feature), color='g', linestyle='--') # Add mean to the histogram
    ax_hist2.axvline(np.median(feature), color='black', linestyle='-') # Add median to the histogram
298/33: histogram_boxplot(data.age,bins=25);
298/34:
def treat_outliers(data,col):
    '''
    treats outliers in a varaible
    col: str, name of the numerical varaible
    data: data frame
    col: name of the column
    '''
    Q1=data[col].quantile(0.25) # 25th quantile
    Q3=data[col].quantile(0.75)  # 75th quantile
    IQR=Q3-Q1
    Lower_Whisker = Q1 - 1.5*IQR 
    Upper_Whisker = Q3 + 1.5*IQR
    data[col] = np.clip(data[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker 
                                                            # and all the values above upper_whishker will be assigned value of upper_Whisker 
    return data

def treat_outliers_all(data, col_list):
    '''
    treat outlier in all numerical varaibles
    col_list: list of numerical varaibles
    data: data frame
    '''
    for c in col_list:
        data = treat_outliers(data,c)
        
    return data
298/35:
numerical_col = data.select_dtypes(include=np.number).columns.tolist()# getting list of numerical columns

  
# items to be removed 
unwanted= {'capital_gain', 'capital_loss'} # these column have very few non zero observation , doing outlier treatment would remove those observation so we are keeping it as it is
  
numerical_col = [ele for ele in numerical_col if ele not in unwanted] 
data = treat_outliers_all(data,numerical_col)
298/36:
## converting object type columns to category
data[cat_columns] = data[cat_columns].astype('category')

data['salary'] = data['salary'].apply(lambda x: 1 if x==' <=50K' else 0)

X = data.drop(['salary'], axis=1)
Y = data['salary']

X = pd.get_dummies(X,drop_first=True)

#Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state = 1)
298/37:
## converting object type columns to category
data[cat_columns] = data[cat_columns].astype('category')

data['salary'] = data['salary'].apply(lambda x: 1 if x==' <=50K' else 0)

X = data.drop(['salary'], axis=1)
Y = data['salary']

X = pd.get_dummies(X,drop_first=True)

#Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state = 1)
298/38:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,train,test,train_y,test_y,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    
    score_list=[] 
    
    pred_train = model.predict(train)
    pred_test = model.predict(test)
    
    train_acc = accuracy_score(pred_train,train_y)
    test_acc = accuracy_score(pred_test,test_y)
    
    train_recall = recall_score(train_y,pred_train)
    test_recall = recall_score(test_y,pred_test)
    
    train_precision = precision_score(train_y,pred_train)
    test_precision = precision_score(test_y,pred_test)
    
    train_f1 = f1_score(train_y,pred_train)
    test_f1 = f1_score(test_y,pred_test)
    
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))
        
     # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",accuracy_score(pred_train,train_y))
        print("Accuracy on test set : ",accuracy_score(pred_test,test_y))
        print("Recall on training set : ",recall_score(train_y,pred_train))
        print("Recall on test set : ",recall_score(test_y,pred_test))
        print("Precision on training set : ",precision_score(train_y,pred_train))
        print("Precision on test set : ",precision_score(test_y,pred_test))
        print("F1 on training set : ",f1_score(train_y,pred_train))
        print("F1 on test set : ",f1_score(test_y,pred_test))
    
    return score_list # returning the list with train and test scores
298/39:
def make_confusion_matrix(model,test_X,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - >50K","Actual - <=50K"]],
                  columns = [i for i in ['Predicted - >50K','Predicted - <=50k']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
298/40:
# dataframe with numerical column only
num_feature_set = X.copy()
num_feature_set = add_constant(num_feature_set)
num_feature_set = num_feature_set.astype(float)
299/1:
import warnings
warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# Libraries to help with model building
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
pd.set_option("display.max_columns", None)
# pd.set_option('display.max_rows', None)
pd.set_option("display.max_rows", 200)

# for statistical analysis 
# import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant


# To get diferent metric scores
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, f1_score
from sklearn.model_selection import GridSearchCV
299/2: who = pd.read_csv("who_data.csv")
299/3:
# copying data to another variable to avoid any changes to original data
data = who.copy()
299/4: data.head()
299/5: data.tail()
299/6: data.shape
299/7: data.info()
299/8:
# fixing the names of columns as there are dots, spaces  in columns names
data.columns =[col.replace(' ','_') for col in data.columns]
data.columns =[col.replace('-','_') for col in data.columns]
data.columns =[col.replace('.','') for col in data.columns]
299/9: print(data.columns)
299/10: data.describe(include='all')
299/11:
# filtering object type columns
cat_columns = data.describe(include=['object']).columns
cat_columns
299/12:
for i in cat_columns:
    print('Unique values in',i, 'are :')
    print(data[i].value_counts())
    print('*'*50)
299/13:
data.workclass = data.workclass.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.native_country = data.native_country.apply(lambda x: 'Unknown' if x == ' ?' else x)
data.occupation = data.occupation.apply(lambda x: 'Unknown' if x == ' ?' else x)
299/14: print(f'There are a {data.native_country.nunique()} distinct countries in the data set, we can reduce them to their respective continents.')
299/15:
data.native_country = data.native_country.str.replace(' ','') # remove empty spaces from strings
data.native_country.head(15)
299/16:
north_america  = ["Canada", "Cuba", "Dominican-Republic", "El-Salvador", "Guatemala",
                   "Haiti", "Honduras", "Jamaica", "Mexico", "Nicaragua",
                   "Outlying-US(Guam-USVI-etc)", "Puerto-Rico", "Trinadad&Tobago",
                   "United-States"]
asia  = ["Cambodia", "China", "Hong", "India", "Iran", "Japan", "Laos",
          "Philippines", "Taiwan", "Thailand", "Vietnam"]
south_america = ["Columbia", "Ecuador", "Peru"]
europe = ["England", "France", "Germany", "Greece", "Holand-Netherlands",
            "Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland",
            "Yugoslavia"]
other  = ["South", "Unknown","?"]
299/17:
def region_combining(x):
    if x in north_america:
        return 'north_america'
    elif x in asia:
        return 'asia'
    elif x in south_america:
        return 'south_america'
    elif x in europe:
        return 'europe'
    elif x in other:
        return 'other'
    else:
        return x
299/18:
data['native_country'] = data['native_country'].apply(region_combining)
data['native_country']
299/19: print('Distinct values in Native Country column:', data.native_country.nunique())
299/20: data.marrital_status.unique()
299/21:
data.marrital_status = data.marrital_status.str.replace(' ','') # remove empty spaces from strings

married = ['Married-civ-spouse','Married-AF-spouse','Married-spouse-absent']
not_married = ['Divorced','Separated','Widowed']

def reduce_marital_status(x):
    if x in married:
        return 'married'
    elif x in not_married:
        return 'not_married'
    else:
        return x
299/22: data.marrital_status = data.marrital_status.apply(reduce_marital_status)
299/23: data.marrital_status.unique()
299/24:
# While doing uni-variate analysis of numerical variables we want to study their central tendency and dispersion.
# Let us write a function that will help us create boxplot and histogram for any input numerical 
# variable.
# This function takes the numerical column as the input and returns the boxplots 
# and histograms for the variable.
# Let us see if this help us write faster and cleaner code.
def histogram_boxplot(feature, figsize=(15,10), bins = None):
    """ Boxplot and histogram combined
    feature: 1-d feature array
    figsize: size of fig (default (9,8))
    bins: number of bins (default None / auto)
    """
    sns.set(font_scale=2) # setting the font scale  of the seaborn
    f2, (ax_box2, ax_hist2) = plt.subplots(nrows = 2, # Number of rows of the subplot grid= 2
                                           sharex = True, # x-axis will be shared among all subplots
                                           gridspec_kw = {"height_ratios": (.25, .75)}, 
                                           figsize = figsize 
                                           ) # creating the 2 subplots
    sns.boxplot(feature, ax=ax_box2, showmeans=True, color='red') # boxplot will be created and a star will indicate the mean value of the column
    sns.distplot(feature, kde=False, ax=ax_hist2, bins=bins) if bins else sns.distplot(feature, kde=False, ax=ax_hist2) # For histogram
    ax_hist2.axvline(np.mean(feature), color='g', linestyle='--') # Add mean to the histogram
    ax_hist2.axvline(np.median(feature), color='black', linestyle='-') # Add median to the histogram
299/25: histogram_boxplot(data.age,bins=25);
299/26: histogram_boxplot(data.education_no_of_years)
299/27: histogram_boxplot(data.fnlwgt)
299/28: histogram_boxplot(data.capital_gain)
299/29:
# Lets look at quantiles of capital gain
data.capital_gain.quantile([.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.99,1])
299/30: histogram_boxplot(data.capital_loss)
299/31:
# Lets us look at quantile of capital loss
data.capital_loss.quantile([.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.98,.99,1])
299/32: histogram_boxplot(data.working_hours_per_week)
299/33:
# Function to create barplots that indicate percentage for each category.

def perc_on_bar(z):
    '''
    plot
    feature: categorical feature
    the function won't work if a column is passed in hue parameter
    '''

    total = len(data[z]) # length of the column
    plt.figure(figsize=(15,5))
    plt.xticks(rotation=45)
    ax = sns.countplot(data[z],palette='Paired')
    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # percentage of each class of the category
        x = p.get_x() + p.get_width() / 2 - 0.05 # width of the plot
        y = p.get_y() + p.get_height()           # hieght of the plot
        
        ax.annotate(percentage, (x, y), size = 15) # annotate the percantage 
    plt.show() # show the plot
299/34: perc_on_bar('workclass')
299/35: perc_on_bar('marrital_status')
299/36: perc_on_bar('native_country')
299/37: perc_on_bar('salary')
299/38:
plt.figure(figsize=(10,5))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.2f',cmap='Spectral')
plt.show()
299/39:
### Function to plot stacked bar charts for categorical columns
def stacked_plot(x):
    sns.set()
    ## crosstab 
    tab1 = pd.crosstab(x,data['salary'],margins=True).sort_values(by=' >50K',ascending=False)
    print(tab1)
    print('-'*120)
    ## visualising the cross tab
    tab = pd.crosstab(x,data['salary'],normalize='index').sort_values(by=' >50K',ascending=False)
    tab.plot(kind='bar',stacked=True,figsize=(17,7))
    plt.legend(loc='lower left', frameon=False,)
    plt.legend(loc="upper left", bbox_to_anchor=(1,1))
    plt.show()
299/40: stacked_plot(data['sex'])
299/41: stacked_plot(data['education'])
299/42: stacked_plot(data['occupation'])
299/43: stacked_plot(data['race'])
299/44:
plt.figure(figsize=(17,8))
sns.barplot(x=data.salary,y=data.education_no_of_years)
plt.ylabel('average no education years');
299/45: stacked_plot(data['workclass'])
299/46: stacked_plot(data['marrital_status'])
299/47: stacked_plot(data['relationship'])
299/48: stacked_plot(data['education_no_of_years'])
299/49:
### Function to plot distributions and Boxplots of customers
def plot(x,target='salary'):
    fig,axs = plt.subplots(2,2,figsize=(12,10))
    axs[0, 0].set_title('Distribution of people with <=50K salary')
    sns.distplot(data[(data[target] == ' <=50K')][x],ax=axs[0,0],color='teal')
    axs[0, 1].set_title("Distribution of people with >50K salary")
    sns.distplot(data[(data[target] == ' >50K')][x],ax=axs[0,1],color='orange')
    axs[1,0].set_title('Boxplot w.r.t salary')
    sns.boxplot(data[target],data[x],ax=axs[1,0],palette='gist_rainbow')
    axs[1,1].set_title('Boxplot w.r.t salary - Without outliers')
    sns.boxplot(data[target],data[x],ax=axs[1,1],showfliers=False,palette='gist_rainbow') #turning off outliers from boxplot
    plt.tight_layout()
    plt.show()
299/50: plot('age')
299/51: plot('working_hours_per_week')
299/52:
# let us look at mean working hours per week
plt.figure(figsize=(10,5))
sns.barplot(x='salary',y="working_hours_per_week",data=data)
plt.ylabel('mean working hours per week');
299/53:
plt.figure(figsize=(10,5))
sns.scatterplot(x=data.working_hours_per_week,y=data.capital_gain,hue=data.salary)
plt.legend(loc="upper left", bbox_to_anchor=(1,1));
299/54: sns.displot(data=data, x="capital_gain", hue="salary",height=5,aspect=2);
299/55:
plt.figure(figsize=(12,7))
sns.histplot(data = data, x="age", hue="marrital_status",stat='density',kde=True,discrete=True)
plt.show()
299/56:
# lets us look at normalized chart of age vs marital status
sns.displot(data=data,x="age", hue="marrital_status",kind="kde",multiple='fill',height=5,aspect=1.5);
299/57:
plt.figure(figsize=(10,7))
sns.boxplot(y="occupation", x="working_hours_per_week", data=data);
299/58:
# outlier detection using boxplot
numerical_col = data.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(20,30))

for i, variable in enumerate(numerical_col):
                     plt.subplot(5,4,i+1)
                     plt.boxplot(data[variable],whis=1.5)
                     plt.tight_layout()
                     plt.title(variable)

plt.show()
299/59:
def treat_outliers(data,col):
    '''
    treats outliers in a varaible
    col: str, name of the numerical varaible
    data: data frame
    col: name of the column
    '''
    Q1=data[col].quantile(0.25) # 25th quantile
    Q3=data[col].quantile(0.75)  # 75th quantile
    IQR=Q3-Q1
    Lower_Whisker = Q1 - 1.5*IQR 
    Upper_Whisker = Q3 + 1.5*IQR
    data[col] = np.clip(data[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker 
                                                            # and all the values above upper_whishker will be assigned value of upper_Whisker 
    return data

def treat_outliers_all(data, col_list):
    '''
    treat outlier in all numerical varaibles
    col_list: list of numerical varaibles
    data: data frame
    '''
    for c in col_list:
        data = treat_outliers(data,c)
        
    return data
299/60:
numerical_col = data.select_dtypes(include=np.number).columns.tolist()# getting list of numerical columns

  
# items to be removed 
unwanted= {'capital_gain', 'capital_loss'} # these column have very few non zero observation , doing outlier treatment would remove those observation so we are keeping it as it is
  
numerical_col = [ele for ele in numerical_col if ele not in unwanted] 
data = treat_outliers_all(data,numerical_col)
299/61:
## converting object type columns to category
data[cat_columns] = data[cat_columns].astype('category')

data['salary'] = data['salary'].apply(lambda x: 1 if x==' <=50K' else 0)

X = data.drop(['salary'], axis=1)
Y = data['salary']

X = pd.get_dummies(X,drop_first=True)

#Splitting data in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state = 1)
299/62:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,train,test,train_y,test_y,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    
    score_list=[] 
    
    pred_train = model.predict(train)
    pred_test = model.predict(test)
    
    train_acc = accuracy_score(pred_train,train_y)
    test_acc = accuracy_score(pred_test,test_y)
    
    train_recall = recall_score(train_y,pred_train)
    test_recall = recall_score(test_y,pred_test)
    
    train_precision = precision_score(train_y,pred_train)
    test_precision = precision_score(test_y,pred_test)
    
    train_f1 = f1_score(train_y,pred_train)
    test_f1 = f1_score(test_y,pred_test)
    
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))
        
     # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",accuracy_score(pred_train,train_y))
        print("Accuracy on test set : ",accuracy_score(pred_test,test_y))
        print("Recall on training set : ",recall_score(train_y,pred_train))
        print("Recall on test set : ",recall_score(test_y,pred_test))
        print("Precision on training set : ",precision_score(train_y,pred_train))
        print("Precision on test set : ",precision_score(test_y,pred_test))
        print("F1 on training set : ",f1_score(train_y,pred_train))
        print("F1 on test set : ",f1_score(test_y,pred_test))
    
    return score_list # returning the list with train and test scores
299/63:
def make_confusion_matrix(model,test_X,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - >50K","Actual - <=50K"]],
                  columns = [i for i in ['Predicted - >50K','Predicted - <=50k']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
299/64:
# dataframe with numerical column only
num_feature_set = X.copy()
num_feature_set = add_constant(num_feature_set)
num_feature_set = num_feature_set.astype(float)
299/65:
vif_series1 = pd.Series([variance_inflation_factor(num_feature_set.values,i) for i in range(num_feature_set.shape[1])],index=num_feature_set.columns, dtype = float)
print('Series before feature selection: \n\n{}\n'.format(vif_series1))
299/66:
# dataframe with numerical column only
num_feature_set = X.copy()
num_feature_set = add_constant(num_feature_set)
num_feature_set = num_feature_set.astype(float)
299/67:
vif_series1 = pd.Series([variance_inflation_factor(num_feature_set.values,i) for i in range(num_feature_set.shape[1])],index=num_feature_set.columns, dtype = float)
print('Series before feature selection: \n\n{}\n'.format(vif_series1))
299/68:
# droping variables of perfect collinearity
variables_with_prefect_collinearity = vif_series1[vif_series1.values==np.inf].index.tolist()
num_feature_set = num_feature_set.drop(variables_with_prefect_collinearity,axis=1)
299/69: X_train, X_test, y_train, y_test = train_test_split(num_feature_set, Y, test_size=0.30, random_state = 1)
299/70: y_test.value_counts()
299/71:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
299/72:
y_predict = model.predict(X_test)
cm=metrics.confusion_matrix( y_test, y_predict, labels=[0, 1])
cm
299/73: roc_auc_score(y_train, lg.predict(X_train))
299/74: roc_auc_score(y_train, lg.predict_proba(X_train)[:,1])
299/75: pd.DataFrame(lg.predict_proba(X_train)[:,1])
299/76: pd.DataFrame(lg.predict(X_train))
299/77:
logit_roc_auc_train = roc_auc_score(y_train, lg.predict_proba(X_train)[:,1])
fpr, tpr, thresholds = roc_curve(y_train, lg.predict_proba(X_train)[:,1])
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_train)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()
299/78:
a,b,c = roc_curve(y_train, lg.predict_proba(X_train)[:,1])
len(c)
299/79:
a,b,c = roc_curve(y_train, lg.predict(X_train))
len(c)
299/80:
logit_roc_auc_test = roc_auc_score(y_test, lg.predict_proba(X_test)[:,1])
fpr, tpr, thresholds = roc_curve(y_test, lg.predict_proba(X_test)[:,1])
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_test)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()
299/81:
log_odds = lg.coef_[0]
pd.DataFrame(log_odds, X_train.columns, columns=['coef']).T
301/1:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
301/2:
url = "credit.csv"
creditData = pd.read_csv(url)

#creditData = pd.read_csv("credit.csv")
creditData.head(10) #several missing values!
301/3: creditData.shape
301/4: creditData['default'].value_counts()
301/5: creditData.describe()
301/6: creditData.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type
301/7:
for feature in creditData.columns: # Loop through all columns in the dataframe
    if creditData[feature].dtype == 'object': # Only apply for columns with categorical strings
        creditData[feature] = pd.Categorical(creditData[feature])# Replace strings with an integer
creditData.head(10)
301/8:
print(creditData.checking_balance.value_counts())
print(creditData.credit_history.value_counts())
print(creditData.purpose.value_counts())
print(creditData.savings_balance.value_counts())
print(creditData.employment_duration.value_counts())
print(creditData.other_credit.value_counts())
print(creditData.housing.value_counts())
print(creditData.job.value_counts())
print(creditData.phone.value_counts())
301/9:
print(creditData.checking_balance.value_counts())
print(creditData.credit_history.value_counts())
print(creditData.purpose.value_counts())
print(creditData.savings_balance.value_counts())
print(creditData.employment_duration.value_counts())
print(creditData.other_credit.value_counts())
print(creditData.housing.value_counts())
print(creditData.job.value_counts())
print(creditData.phone.value_counts())
301/10:
replaceStruct = {
                "checking_balance":     {"< 0 DM": 1, "1 - 200 DM": 2 ,"> 200 DM": 3 ,"unknown":-1},
                "credit_history": {"critical": 1, "poor":2 , "good": 3, "very good": 4,"perfect": 5},
                 "savings_balance": {"< 100 DM": 1, "100 - 500 DM":2 , "500 - 1000 DM": 3, "> 1000 DM": 4,"unknown": -1},
                 "employment_duration":     {"unemployed": 1, "< 1 year": 2 ,"1 - 4 years": 3 ,"4 - 7 years": 4 ,"> 7 years": 5},
                "phone":     {"no": 1, "yes": 2 },
                #"job":     {"unemployed": 1, "unskilled": 2, "skilled": 3, "management": 4 },
                "default":     {"no": 0, "yes": 1 } 
                    }
oneHotCols=["purpose","housing","other_credit","job"]
301/11:
creditData=creditData.replace(replaceStruct)
creditData=pd.get_dummies(creditData, columns=oneHotCols)
creditData.head(10)
301/12: creditData.info()
301/13:
X = creditData.drop("default" , axis=1)
y = creditData.pop("default")
301/14: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)
301/15:
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)
301/16:
print("Accuracy on training set : ",dTree.score(X_train, y_train))
print("Accuracy on test set : ",dTree.score(X_test, y_test))
301/17:
#Checking number of positives
y.sum(axis = 0)
301/18:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
301/19:
##  Function to calculate recall score
def get_recall_score(model):
    '''
    model : classifier to predict values of X

    '''
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
    print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
301/20: make_confusion_matrix(dTree,y_test)
301/21:
# Recall on train and test
get_recall_score(dTree)
301/22:
feature_names = list(X.columns)
print(feature_names)
301/23:
plt.figure(figsize=(20,30))
tree.plot_tree(dTree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
301/24:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree,feature_names=feature_names,show_weights=True))
301/25:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
301/26:
importances = dTree.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
301/27:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)
dTree1.fit(X_train, y_train)
301/28: make_confusion_matrix(dTree1, y_test)
301/29:
# Accuracy on train and test
print("Accuracy on training set : ",dTree1.score(X_train, y_train))
print("Accuracy on test set : ",dTree1.score(X_test, y_test))
# Recall on train and test
get_recall_score(dTree1)
301/30:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree1,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
301/31:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree1,feature_names=feature_names,show_weights=True))
301/32:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree1.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
301/33:
importances = dTree1.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10,10))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
301/34: from sklearn.model_selection import GridSearchCV
301/35:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(1,10), 
              'min_samples_leaf': [1, 2, 5, 7, 10,15,20],
              'max_leaf_nodes' : [2, 3, 5, 10],
              'min_impurity_decrease': [0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
301/36: make_confusion_matrix(estimator,y_test)
301/37:
# Accuracy on train and test
print("Accuracy on training set : ",estimator.score(X_train, y_train))
print("Accuracy on test set : ",estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
301/38:
plt.figure(figsize=(15,10))

tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
301/39:
# Text report showing the rules of a decision tree -

print(tree.export_text(estimator,feature_names=feature_names,show_weights=True))
301/40:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(estimator.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

#Here we will see that importance of features has increased
301/41:
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
310/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
310/2:
df_train = pd.read_csv('Train dataset.csv')
df_train.head()
311/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
311/2:
df_train = pd.read_csv('Train dataset.csv')
df_train.head()
311/3:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
311/4:
## Checking for the null values

df_train.isnull().sum()
311/5:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
311/6:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
311/7:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient']])
311/8:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient']])
IPQ
311/9:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
311/10:
## Checking for the null values

df1.isnull().sum()
311/11:
# drop the S.No. column as it does not add any value to the analysis
df1.drop("Registration Number", axis=1, inplace=True)
311/12:
# drop the Registration Number column as it does not add any value to the analysis
df1.drop("Registration Number", axis=1, inplace=True)
311/13:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
311/14:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
311/15:
## Checking for the null values

df1.isnull().sum()
311/16:
# drop the Registration Number column as it does not add any value to the analysis
df1.drop("Registration Number", axis=1, inplace=True)
311/17:
# checking column datatypes and number of non-null values
df1.info()
311/18:
# checking for duplicate values
df1.duplicated().sum()
311/19:
# viewing the column values
df["Cuisine"].head(10)
311/20:
# viewing the column values
df1["Cuisine"].head(10)
311/21:
## Checking for the null values

df1.isnull().sum()
311/22:
# checking column datatypes and number of non-null values
df1.info()
311/23:
# checking for duplicate values
df1.duplicated().sum()
311/24:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
311/25:
## Checking for the null values

df1.isnull().sum()
311/26:
# checking column datatypes and number of non-null values
df1.info()
311/27:
# checking for duplicate values
df1.duplicated().sum()
311/28:
# viewing the column values
df1["Cuisine"].head(10)
311/29:
df1_cuisine = df1["Cuisine"].str.split(" ", expand=True)
df1_cuisine.head()
311/30:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head()
311/31:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head(15)
311/32:
# we will create two new columns for mileage values and units

df_col1 = df1_cuisine[0]
df_col2 = df1_cuisine[1]


# Checking the new dataframe
df.head()
311/33:
# we will create two new columns for mileage values and units

df_col1 = df1_cuisine[0]
df_col2 = df1_cuisine[1]


# Checking the new dataframe
df1.head()
311/34:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
311/35:
df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))

# we will create two new columns for mileage values and units
df["mileage_num"] = df_mileage[0]
df["mileage_unit"] = df_mileage[1]
311/36:
df1["df_col1"].merge(df1["df_col2"], left_on='lkey', right_on='rkey', suffixes=(False, False))

df1.head()
311/37:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
311/38:
df1["df_col1"].merge(df1["df_col2"], left_on='lkey', right_on='rkey', suffixes=(False, False))

df1.head()
311/39:
df2 = pd.concat([df1["df_col1"], df1["df_col2"] ], axis=1)

df2.head()
311/40: df3 = pd.concat([df1["df_col1"], df1["df_col2"]], axis=1, join="inner")
311/41:
df3 = pd.concat([df1["df_col1"], df1["df_col2"]], axis=1, join="inner")
df3.head()
311/42:
df3 = pd.concat([df1["df_col1"], df1["df_col2"]], axis=1, join="outer")
df3.head()
311/43: df2 = pd.merge(df1["df_col1"], df1["df_col2"], on="col1", how="outer", indicator=True)
311/44: df2 = pd.merge(df1["df_col1"], df1["df_col2"], on="df_col1" how="outer", indicator=True)
311/45: df2 = pd.merge(df1["df_col1"], df1["df_col2"], on="df_col1", how="outer", indicator=True)
311/46: df2 = pd.merge(df1["df_col1"], df1["df_col2"], how="outer", indicator=True)
311/47: df2 = pd.merge(df1["df_col1"], df1["df_col2"])
311/48: df1['df_col1'].append(df['df_col2']).reset_index(drop=True)
311/49: df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
311/50: df1.head()
311/51:
df1["cuisine_new"] = df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
df1.head()
311/52:
df1.drop(columns=["Cuisine", "df_col1", "df_col2"], inplace=True)
df1.head()
311/53:
df1_near = df1["Restaurant Location"].str.split(" ")
df1_near.head(15)
311/54:
df1_near = df1["Restaurant Location"].str.split(" ", expand=True)
df1_near.head(15)
311/55: df1["Restaurant_Location"] = df1_near[1]
311/56:
df1["Restaurant_Location"] = df1_near[1]
df1.head()
311/57:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
311/58: df1.info()
311/59:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df['Joined']))
df['Opened'].head()
311/60:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Joined']))
df1['Opened'].head()
311/61:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
311/62:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Joined'])]
311/63:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
311/64:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
311/65:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
311/66: df1["Restaurant Type"].head()
311/67: df1["Restaurant Type"].head(19)
311/68: df1["Restaurant Theme"].head(19)
311/69:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
311/70:
df1["Restaurant Theme"].head(19)
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
311/71:
df1["Restaurant Theme"].head(19)
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
311/72:
df1["Restaurant Theme"]
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
311/73:

df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
311/74:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
311/75:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
311/76:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
311/77:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
311/78:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
311/79: df1.info()
311/80:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
311/81:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
311/82:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
311/83:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
314/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
314/2:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
314/3:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
314/4:
## Checking for the null values

df1.isnull().sum()
314/5:
# checking column datatypes and number of non-null values
df1.info()
314/6:
# checking for duplicate values
df1.duplicated().sum()
314/7:
# viewing the column values
df1["Cuisine"].head(10)
314/8:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head(15)
314/9:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
314/10:
df2 = pd.concat([df1["df_col1"], df1["df_col2"] ], axis=1)

df2.head()
314/11:
df1["cuisine_new"] = df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
df1.head()
314/12:
df1.drop(columns=["Cuisine", "df_col1", "df_col2"], inplace=True)
df1.head()
314/13:
df1_near = df1["Restaurant Location"].str.split(" ", expand=True)
df1_near.head(15)
314/14:
df1["Restaurant_Location"] = df1_near[1]
df1.head()
314/15:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
314/16: df1.info()
314/17:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
314/18:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
314/19:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
314/20:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(15)
314/21:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head(5)
314/22:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
314/23:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
314/24:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
315/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
315/2:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
315/3:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
315/4:
## Checking for the null values

df1.isnull().sum()
315/5:
# checking column datatypes and number of non-null values
df1.info()
315/6:
# checking for duplicate values
df1.duplicated().sum()
315/7:
# viewing the column values
df1["Cuisine"].head(10)
315/8:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head(15)
315/9:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
315/10:
df2 = pd.concat([df1["df_col1"], df1["df_col2"] ], axis=1)

df2.head()
315/11:
df1["cuisine_new"] = df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
df1.head()
315/12:
df1.drop(columns=["Cuisine", "df_col1", "df_col2"], inplace=True)
df1.head()
315/13:
df1_near = df1["Restaurant Location"].str.split(" ", expand=True)
df1_near.head(15)
315/14:
df1["Restaurant_Location"] = df1_near[1]
df1.head()
315/15:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
315/16: df1.info()
315/17:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
315/18:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
315/19:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
315/20:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
315/21:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
315/22:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
315/23:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient']])
315/24:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient']])
IPQ
315/25:
## Declaring the Linear Regression function

LR = LinearRegression()
315/26:
## Fitting the Linear Regression function

model = LR.fit(IPQ,df_train['Annual Turnover'])
315/27:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
315/28:
df_test = pd.read_csv('D:/Hackathon/Test dataset.csv')
df_test.head()
315/29:
# let's check again for missing values
df1.isnull().sum()
315/30:
# let's check again for missing values
df1.isnull()
315/31:
# let's check again for missing values
df1.isnull().sum()
315/32:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
df1 = df1.apply(medianFiller,axis=0)

df1['Instagram Popularity Quotient'] = df1['Instagram Popularity Quotient'].astype('float64')  # converting the hp column from object / string type to float
315/33:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
df1 = df1.apply(medianFiller,axis=0)

df1['Instagram Popularity Quotient'] = df1['Instagram Popularity Quotient']  # converting the hp column from object / string type to float
315/34:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
315/35:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
315/36:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
315/37:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient']])
315/38:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient']])
IPQ
315/39:
# let's check again for missing values
df1.isnull().sum()
315/40:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
df1 = df1.apply(medianFiller,axis=0)

df1['Instagram Popularity Quotient'] = df1['Instagram Popularity Quotient']  # converting the hp column from object / string type to float
317/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
317/2:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
317/3:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
317/4:
## Checking for the null values

df1.isnull().sum()
317/5:
# checking column datatypes and number of non-null values
df1.info()
317/6:
# checking for duplicate values
df1.duplicated().sum()
317/7:
# viewing the column values
df1["Cuisine"].head(10)
317/8:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head(15)
317/9:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
317/10:
df2 = pd.concat([df1["df_col1"], df1["df_col2"] ], axis=1)

df2.head()
317/11:
df1["cuisine_new"] = df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
df1.head()
317/12:
df1.drop(columns=["Cuisine", "df_col1", "df_col2"], inplace=True)
df1.head()
317/13:
df1_near = df1["Restaurant Location"].str.split(" ", expand=True)
df1_near.head(15)
317/14:
df1["Restaurant_Location"] = df1_near[1]
df1.head()
317/15:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
317/16: df1.info()
317/17:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
317/18:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
317/19:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
317/20:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
317/21:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
317/22:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
317/23:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient']])
317/24:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient']])
IPQ
317/25:
# let's check again for missing values
df1.isnull().sum()
317/26:
# replace the missing values with median value.
# Note, we do not need to specify the column names below
# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)
#cData = cData.fillna(cData.median())

medianFiller = lambda x: x.fillna(x.median())
df1 = df1.apply(medianFiller,axis=0)

df1['Instagram Popularity Quotient'] = df1['Instagram Popularity Quotient']  # converting the hp column from object / string type to float
317/27: CI.fit(df_train[['Comedy Gigs Rating']])
317/28: SI.fit(df_train[['Comedy Gigs Rating']])
317/29:
CGR = SI.transform(df_train[['Comedy Gigs Rating']])
CGR
317/30:
# let's check again for missing values
df1.isnull().sum()
317/31:
# let's check again for missing values
df1.isnull().sum().info()
317/32:
# let's check again for missing values
df1.isnull().sum()
317/33:
# let's check again for missing values
df1.isnull().sum()
317/34:
## Declaring the Linear Regression function

LR = LinearRegression()
317/35:
## Fitting the Linear Regression function

model = LR.fit(IPQ,df_train['Annual Turnover'])
317/36:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
317/37:
## Checking the score of the function on the training data

model.score(CGR,df_train['Annual Turnover'])
317/38:
df_test = pd.read_csv('D:/Hackathon/Test dataset.csv')
df_test.head()
317/39:
df_test = pd.read_csv('Test_dataset.csv')
df_test.head()
317/40:
## Checking for the null values

df_test.isnull().sum()
317/41:
## Using the model built on the Training set to predict on the Test Set

prediction = model.predict(df_test[['Instagram Popularity Quotient']])
prediction
317/42:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating']])
317/43:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating']])

IPQ
317/44:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient']])
317/45:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient']])

IPQ
317/46:
# let's check again for missing values
df1.isnull().sum().sort_values(ascending=False)
317/47:
# let's check again for missing values
df2= df1.isnull().sum().sort_values(ascending=False)
df2.head(10)
317/48:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
317/49:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
317/50:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient','Live Sports Rating','Value Deals Rating','Live Music Rating','Overall Restaurant Rating','Resturant Tier','Ambience']])
317/51:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient','Live Sports Rating','Value Deals Rating','Live Music Rating','Overall Restaurant Rating','Resturant Tier','Ambience']])

IPQ
317/52:
# let's check again for missing values
df2= df1.isnull().sum().sort_values(ascending=False)
df2.head(10)
317/53:
## Declaring the Linear Regression function

LR = LinearRegression()
317/54:
## Fitting the Linear Regression function

model = LR.fit(IPQ,df_train['Annual Turnover'])
317/55:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
317/56:
## Checking the score of the function on the training data

model.score(CGR,df_train['Annual Turnover'])
317/57:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
317/58:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
317/59:
df_test = pd.read_csv('Test_dataset.csv')
df_test.head()
317/60:
## Checking for the null values

df_test.isnull().sum()
317/61:
df_test = pd.read_csv('Test_dataset.csv')
df_test.head()
317/62:
## Checking for the null values

df_test.isnull().sum()
317/63:
## Using the model built on the Training set to predict on the Test Set

prediction = model.predict(df_test[['Instagram Popularity Quotient']])
prediction
317/64:
solution_df = pd.DataFrame(df_test['Registration Number'])
solution_df
317/65:
solution_df['Annual Turnover'] = prediction
solution_df
317/66:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('D:/Hackathon')
317/67:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('Hackathon')
320/1:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
320/2:
url = "credit.csv"
creditData = pd.read_csv(url)

#creditData = pd.read_csv("credit.csv")
creditData.head(10) #several missing values!
320/3: creditData.shape
320/4: creditData['default'].value_counts()
320/5: creditData.describe()
320/6: creditData.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type
320/7:
for feature in creditData.columns: # Loop through all columns in the dataframe
    if creditData[feature].dtype == 'object': # Only apply for columns with categorical strings
        creditData[feature] = pd.Categorical(creditData[feature])# Replace strings with an integer
creditData.head(10)
320/8:
print(creditData.checking_balance.value_counts())
print(creditData.credit_history.value_counts())
print(creditData.purpose.value_counts())
print(creditData.savings_balance.value_counts())
print(creditData.employment_duration.value_counts())
print(creditData.other_credit.value_counts())
print(creditData.housing.value_counts())
print(creditData.job.value_counts())
print(creditData.phone.value_counts())
320/9:
replaceStruct = {
                "checking_balance":     {"< 0 DM": 1, "1 - 200 DM": 2 ,"> 200 DM": 3 ,"unknown":-1},
                "credit_history": {"critical": 1, "poor":2 , "good": 3, "very good": 4,"perfect": 5},
                 "savings_balance": {"< 100 DM": 1, "100 - 500 DM":2 , "500 - 1000 DM": 3, "> 1000 DM": 4,"unknown": -1},
                 "employment_duration":     {"unemployed": 1, "< 1 year": 2 ,"1 - 4 years": 3 ,"4 - 7 years": 4 ,"> 7 years": 5},
                "phone":     {"no": 1, "yes": 2 },
                #"job":     {"unemployed": 1, "unskilled": 2, "skilled": 3, "management": 4 },
                "default":     {"no": 0, "yes": 1 } 
                    }
oneHotCols=["purpose","housing","other_credit","job"]
320/10:
creditData=creditData.replace(replaceStruct)
creditData=pd.get_dummies(creditData, columns=oneHotCols)
creditData.head(10)
320/11: creditData.info()
320/12:
X = creditData.drop("default" , axis=1)
y = creditData.pop("default")
320/13: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)
320/14:
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)
320/15:
print("Accuracy on training set : ",dTree.score(X_train, y_train))
print("Accuracy on test set : ",dTree.score(X_test, y_test))
320/16:
#Checking number of positives
y.sum(axis = 0)
320/17:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
320/18:
##  Function to calculate recall score
def get_recall_score(model):
    '''
    model : classifier to predict values of X

    '''
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
    print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
320/19: make_confusion_matrix(dTree,y_test)
320/20:
# Recall on train and test
get_recall_score(dTree)
320/21:
feature_names = list(X.columns)
print(feature_names)
320/22:
plt.figure(figsize=(20,30))
tree.plot_tree(dTree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
320/23:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree,feature_names=feature_names,show_weights=True))
320/24:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
320/25:
importances = dTree.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
320/26:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)
dTree1.fit(X_train, y_train)
320/27: make_confusion_matrix(dTree1, y_test)
320/28:
# Accuracy on train and test
print("Accuracy on training set : ",dTree1.score(X_train, y_train))
print("Accuracy on test set : ",dTree1.score(X_test, y_test))
# Recall on train and test
get_recall_score(dTree1)
320/29:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree1,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
320/30:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree1,feature_names=feature_names,show_weights=True))
320/31:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree1.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
320/32:
importances = dTree1.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10,10))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
320/33: from sklearn.model_selection import GridSearchCV
320/34:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(1,10), 
              'min_samples_leaf': [1, 2, 5, 7, 10,15,20],
              'max_leaf_nodes' : [2, 3, 5, 10],
              'min_impurity_decrease': [0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
320/35: make_confusion_matrix(estimator,y_test)
320/36:
# Accuracy on train and test
print("Accuracy on training set : ",estimator.score(X_train, y_train))
print("Accuracy on test set : ",estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
320/37:
plt.figure(figsize=(15,10))

tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
320/38:
# Text report showing the rules of a decision tree -

print(tree.export_text(estimator,feature_names=feature_names,show_weights=True))
320/39:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(estimator.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

#Here we will see that importance of features has increased
320/40:
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
320/41:
clf = DecisionTreeClassifier(random_state=1)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
320/42: pd.DataFrame(path)
320/43:
fig, ax = plt.subplots(figsize=(10,5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
320/44:
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))
320/45:
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1,figsize=(10,7))
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
320/46:
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
320/47:
fig, ax = plt.subplots(figsize=(10,5))
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
320/48:
index_best_model = np.argmax(test_scores)
best_model = clfs[index_best_model]
print(best_model)
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('Test accuracy of best model: ',best_model.score(X_test, y_test))
320/49:
recall_train=[]
for clf in clfs:
    pred_train3=clf.predict(X_train)
    values_train=metrics.recall_score(y_train,pred_train3)
    recall_train.append(values_train)
320/50:
recall_test=[]
for clf in clfs:
    pred_test3=clf.predict(X_test)
    values_test=metrics.recall_score(y_test,pred_test3)
    recall_test.append(values_test)
320/51:
fig, ax = plt.subplots(figsize=(15,5))
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(ccp_alphas, recall_train, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, recall_test, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
320/52:
# creating the model where we get highest train and test recall
index_best_model = np.argmax(recall_test)
best_model = clfs[index_best_model]
print(best_model)
320/53: make_confusion_matrix(best_model,y_test)
320/54:
# Recall on train and test
get_recall_score(best_model)
320/55:
plt.figure(figsize=(17,15))

tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
320/56:
# Text report showing the rules of a decision tree -

print(tree.export_text(best_model,feature_names=feature_names,show_weights=True))
320/57:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(best_model.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
320/58:
importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
320/59:
comparison_frame = pd.DataFrame({'Model':['Initial decision tree model','Decision tree with restricted maximum depth','Decision treee with hyperparameter tuning',
                                         'Decision tree with post-pruning'], 'Train_Recall':[1,0.53,0.56,0.63], 'Test_Recall':[0.46,0.46,0.52,0.56]}) 
comparison_frame
321/1:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
321/2:
url = "Loan_Delinquent_Dataset.csv"
creditData = pd.read_csv(url)

#creditData = pd.read_csv("credit.csv")
creditData.head(10) #several missing values!
321/3:
url = "Loan_Delinquent_Dataset.csv"
creditData = pd.read_csv(url)

#creditData = pd.read_csv("credit.csv")
creditData.head(10) #several missing values!
321/4:
url = "Loan_Delinquent_Dataset.csv"
df = pd.read_csv(url)

#creditData = pd.read_csv("credit.csv")
df.head(10) #several missing values!
321/5:
df1 = df.copy()
df1.head()
321/6: df1.shape
321/7: df1['isDelinquent'].value_counts()
321/8: df1.describe()
321/9: df1.describe().T
321/10: df1.info()
321/11:
for feature in df1.columns: # Loop through all columns in the dataframe
    if df1[feature].dtype == 'object': # Only apply for columns with categorical strings
        df1[feature] = pd.Categorical(df1feature])# Replace strings with an integer
df1.head(10)
321/12:
for feature in df1.columns: # Loop through all columns in the dataframe
    if df1[feature].dtype == 'object': # Only apply for columns with categorical strings
        df1[feature] = pd.Categorical(df1[feature])# Replace strings with an integer
df1.head(10)
321/13:
df1.drop["ID"]
df1
321/14:
df1.drop("ID")
df1
321/15:
df.drop(['ID'], axis=1)
df1
321/16:
df1.drop(['ID'], axis=1)
df1
321/17:
df2 = df1.drop(['ID'], axis=1)
df2
321/18: df2.info()
321/19:
for feature in df2.columns: # Loop through all columns in the dataframe
    if df2[feature].dtype == 'object': # Only apply for columns with categorical strings
        df2[feature] = pd.Categorical(df2[feature])# Replace strings with an integer
df2.head(10)
321/20:
print(df2.term.value_counts())
print(df2.gender.value_counts())
print(df2.purpose.value_counts())
print(df2.home_ownership.value_counts())
print(df2.age.value_counts())
print(df2.FICO.value_counts())
print(df2.isDelinquent.value_counts())
321/21:
df2["term"]=df2["term"].split(" ")
df2
321/22:
df2_term = df2["term"].str.split(" ", expand=True)
df2_term.head()
321/23:
# let's verify that there is only one unit
df2_term[1].value_counts()
321/24:
# we will create a new column for engine values
df2["term"] = df2_term[0].astype(float)

# Checking the new dataframe
df2.head()
321/25:
# we will create a new column for engine values
df2["term"] = df2_term[0].astype(int)

# Checking the new dataframe
df2.head()
321/26:
print(df2.term.value_counts())
print(df2.gender.value_counts())
print(df2.purpose.value_counts())
print(df2.home_ownership.value_counts())
print(df2.age.value_counts())
print(df2.FICO.value_counts())
print(df2.isDelinquent.value_counts())
321/27:
replaceStruct = {
                "age":     {">25": 1, "20-25": 2 },
                "FICO": {"300-500": 1, ">500":2} } 
                    }
oneHotCols=["purpose","gender","home_ownership"]
321/28:
replaceStruct = {
                "age":     {">25": 1, "20-25": 2 },
                "FICO": {"300-500": 1, ">500":2} } 
                }
oneHotCols=["purpose","gender","home_ownership"]
321/29:
replaceStruct = {
                "age":     {">25": 1, "20-25": 2 },
                "FICO": {"300-500": 1, ">500":2}  
                }
oneHotCols=["purpose","gender","home_ownership"]
321/30:
df2=df2.replace(replaceStruct)
df2=pd.get_dummies(df2, columns=oneHotCols)
df2.head(10)
321/31: df2.shape
321/32: df2.info()
321/33:
X = df2.drop("isDelinquent" , axis=1)
y = df2.pop("isDelinquent")
321/34: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)
321/35:
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)
321/36:
print("Accuracy on training set : ",dTree.score(X_train, y_train))
print("Accuracy on test set : ",dTree.score(X_test, y_test))
321/37:
#Checking number of positives
y.sum(axis = 0)
321/38:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
321/39:
##  Function to calculate recall score
def get_recall_score(model):
    '''
    model : classifier to predict values of X

    '''
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
    print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
321/40: make_confusion_matrix(dTree,y_test)
321/41:
# Recall on train and test
get_recall_score(dTree)
321/42:
feature_names = list(X.columns)
print(feature_names)
321/43:
plt.figure(figsize=(20,30))
tree.plot_tree(dTree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/44:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree,feature_names=feature_names,show_weights=True))
321/45:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
321/46:
importances = dTree.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/47:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)
dTree1.fit(X_train, y_train)
321/48: make_confusion_matrix(dTree1, y_test)
321/49:
# Accuracy on train and test
print("Accuracy on training set : ",dTree1.score(X_train, y_train))
print("Accuracy on test set : ",dTree1.score(X_test, y_test))
# Recall on train and test
get_recall_score(dTree1)
321/50:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree1,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/51:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree1,feature_names=feature_names,show_weights=True))
321/52:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree1.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
321/53:
importances = dTree1.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10,10))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/54: from sklearn.model_selection import GridSearchCV
321/55:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(1,10), 
              'min_samples_leaf': [1, 2, 5, 7, 10,15,20],
              'max_leaf_nodes' : [2, 3, 5, 10],
              'min_impurity_decrease': [0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
321/56: make_confusion_matrix(estimator,y_test)
321/57:
# Accuracy on train and test
print("Accuracy on training set : ",estimator.score(X_train, y_train))
print("Accuracy on test set : ",estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
321/58:
plt.figure(figsize=(15,10))

tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/59:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(1,20), 
              'min_samples_leaf': [1, 2, 3, 5, 10,15,20],
              'max_leaf_nodes' : [2, 3, 5, 10],
              'min_impurity_decrease': [0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
321/60: make_confusion_matrix(estimator,y_test)
321/61:
# Accuracy on train and test
print("Accuracy on training set : ",estimator.score(X_train, y_train))
print("Accuracy on test set : ",estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
321/62:
plt.figure(figsize=(15,10))

tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/63:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(estimator.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

#Here we will see that importance of features has increased
321/64:
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/65:
clf = DecisionTreeClassifier(random_state=1)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
321/66: pd.DataFrame(path)
321/67:
fig, ax = plt.subplots(figsize=(10,5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
321/68:
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))
321/69:
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1,figsize=(10,7))
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
321/70:
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
321/71:
fig, ax = plt.subplots(figsize=(10,5))
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
321/72:
index_best_model = np.argmax(test_scores)
best_model = clfs[index_best_model]
print(best_model)
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('Test accuracy of best model: ',best_model.score(X_test, y_test))
321/73:
recall_train=[]
for clf in clfs:
    pred_train3=clf.predict(X_train)
    values_train=metrics.recall_score(y_train,pred_train3)
    recall_train.append(values_train)
321/74:
recall_test=[]
for clf in clfs:
    pred_test3=clf.predict(X_test)
    values_test=metrics.recall_score(y_test,pred_test3)
    recall_test.append(values_test)
321/75:
fig, ax = plt.subplots(figsize=(15,5))
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(ccp_alphas, recall_train, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, recall_test, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
321/76:
# creating the model where we get highest train and test recall
index_best_model = np.argmax(recall_test)
best_model = clfs[index_best_model]
print(best_model)
321/77: make_confusion_matrix(best_model,y_test)
321/78:
# Recall on train and test
get_recall_score(best_model)
321/79:
plt.figure(figsize=(17,15))

tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/80:
# Text report showing the rules of a decision tree -

print(tree.export_text(best_model,feature_names=feature_names,show_weights=True))
321/81:
# Text report showing the rules of a decision tree -

print(tree.export_text(best_model,feature_names=feature_names,show_weights=True))
321/82:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(best_model.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
321/83:
importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/84:
comparison_frame = pd.DataFrame({'Model':['Initial decision tree model','Decision tree with restricted maximum depth','Decision treee with hyperparameter tuning',
                                         'Decision tree with post-pruning'], 'Train_Recall':[1,0.53,0.56,0.63], 'Test_Recall':[0.46,0.46,0.52,0.56]}) 
comparison_frame
321/85:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Library to suppress warnings or deprecation notes
import warnings

warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data

import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# Libraries to build decision tree classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV

# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    plot_confusion_matrix,
    make_scorer,
)
321/86:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(2, 50, 5), 
              "criterion": ["entropy", "gini"],
              "splitter": ["best", "random"],
              "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
    }
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
321/87:
# Choose the type of classifier. 
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_depth': np.arange(2, 50, 5), 
              "criterion": ["entropy", "gini"],
              "splitter": ["best", "random"],
              "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
estimator.fit(X_train, y_train)
321/88: make_confusion_matrix(estimator,y_test)
321/89:
# Accuracy on train and test
print("Accuracy on training set : ",estimator.score(X_train, y_train))
print("Accuracy on test set : ",estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
321/90:
plt.figure(figsize=(15,10))

tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/91:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(estimator.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))

#Here we will see that importance of features has increased
321/92:
importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/93:
clf = DecisionTreeClassifier(random_state=1)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
321/94: pd.DataFrame(path)
321/95:
fig, ax = plt.subplots(figsize=(10,5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
321/96:
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      clfs[-1].tree_.node_count, ccp_alphas[-1]))
321/97:
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1,figsize=(10,7))
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
321/98:
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
321/99:
fig, ax = plt.subplots(figsize=(10,5))
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
321/100:
index_best_model = np.argmax(test_scores)
best_model = clfs[index_best_model]
print(best_model)
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('Test accuracy of best model: ',best_model.score(X_test, y_test))
321/101:
recall_train=[]
for clf in clfs:
    pred_train3=clf.predict(X_train)
    values_train=metrics.recall_score(y_train,pred_train3)
    recall_train.append(values_train)
321/102:
recall_test=[]
for clf in clfs:
    pred_test3=clf.predict(X_test)
    values_test=metrics.recall_score(y_test,pred_test3)
    recall_test.append(values_test)
321/103:
fig, ax = plt.subplots(figsize=(15,5))
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(ccp_alphas, recall_train, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, recall_test, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()
321/104:
# creating the model where we get highest train and test recall
index_best_model = np.argmax(recall_test)
best_model = clfs[index_best_model]
print(best_model)
321/105: make_confusion_matrix(best_model,y_test)
321/106:
# Recall on train and test
get_recall_score(best_model)
321/107:
plt.figure(figsize=(17,15))

tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
321/108:
# Text report showing the rules of a decision tree -

print(tree.export_text(best_model,feature_names=feature_names,show_weights=True))
321/109:
# Text report showing the rules of a decision tree -

print(tree.export_text(best_model,feature_names=feature_names,show_weights=True))
321/110:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the 'criterion' brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(best_model.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
321/111:
importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
321/112:
comparison_frame = pd.DataFrame({'Model':['Initial decision tree model','Decision tree with restricted maximum depth','Decision treee with hyperparameter tuning',
                                         'Decision tree with post-pruning'], 'Train_Recall':[1,0.53,0.56,0.63], 'Test_Recall':[0.46,0.46,0.52,0.56]}) 
comparison_frame
325/1:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('Hackathon')
325/2:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('Hackathon')
325/3: dir()
325/4: mkdir()
325/5:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('Home/Desktop/texas/Classification/Hackathon')
325/6:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('$Home/Desktop/texas/Classification/Hackathon')
325/7:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('$Home/Desktop/texas/Classification/Hackathon')
325/8:
## Exporting the data frame to a '.csv' file and setting the index = False as we do want the index

solution_df.to_csv('Submission.csv',index=False)
325/9:
solution_df['Annual Turnover'] = prediction
solution_df
325/10:
solution_df = pd.DataFrame(df_test['Registration Number'])
solution_df
325/11:
solution_df['Annual Turnover'] = prediction
solution_df
325/12:
## Setting the directory to export the file as a '.csv'

import os
os.chdir('$Home/Desktop/texas/Classification/Hackathon')
327/1:
import numpy as np   
from sklearn.linear_model import LinearRegression
import pandas as pd    
import matplotlib.pyplot as plt 
%matplotlib inline 
import seaborn as sns
from sklearn.model_selection import train_test_split # Sklearn package's randomized data splitting function
327/2:
df_train = pd.read_csv('Train_dataset.csv')
df_train.head()
327/3:
# let's create a copy of the data to avoid any changes to original data
df1 = df_train.copy()
327/4:
## Checking for the null values

df1.isnull().sum()
327/5:
# checking column datatypes and number of non-null values
df1.info()
327/6:
# checking for duplicate values
df1.duplicated().sum()
327/7:
# viewing the column values
df1["Cuisine"].head(10)
327/8:
df1_cuisine = df1["Cuisine"].str.split(",", expand=True)
df1_cuisine.head(15)
327/9:
# we will create two new columns for mileage values and units

df1["df_col1"] = df1_cuisine[0]
df1["df_col2"] = df1_cuisine[1]


# Checking the new dataframe
df1.head()
327/10:
df2 = pd.concat([df1["df_col1"], df1["df_col2"] ], axis=1)

df2.head()
327/11:
df1["cuisine_new"] = df1['df_col1'].append(df1['df_col2']).reset_index(drop=True)
df1.head()
327/12:
df1.drop(columns=["Cuisine", "df_col1", "df_col2"], inplace=True)
df1.head()
327/13:
df1_near = df1["Restaurant Location"].str.split(" ", expand=True)
df1_near.head(15)
327/14:
df1["Restaurant_Location"] = df1_near[1]
df1.head()
327/15:
df1.drop(columns=["Restaurant Location"], inplace=True)
df1.head()
327/16: df1.info()
327/17:
df1['Opened'] = pd.to_datetime(df1['Opening Day of Restaurant'])
df1['Opened year'] = df1['Opened'].dt.year  # adding in a feature that's just the year
print(min(df1['Opened']), max(df1['Opened']))
df1['Opened'].head()
327/18:
# investigating the players with this earliest Joined date
df1[df1['Opened'] == min(df1['Opened'])]
327/19:
df1["Restaurant Type"]
df1= pd.get_dummies(df1, columns=['Restaurant Type'])
df1.head()
327/20:
df1= pd.get_dummies(df1, columns=['Restaurant Theme'])
df1.head()
327/21:
## Importing Simple Imputer to treat the null values

from sklearn.impute import SimpleImputer
327/22:
## Defning the Simple Imputer funtion to use 'mean' as a strategy of imputation

SI = SimpleImputer(strategy='mean')
327/23:
## Fitting the Simple Imputer function to get the mean of the variable

SI.fit(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient','Live Sports Rating','Value Deals Rating','Live Music Rating','Overall Restaurant Rating','Resturant Tier','Ambience']])
327/24:
## Transforming the variable --> Imputing the variable with its mean

IPQ = SI.transform(df_train[['Instagram Popularity Quotient','Comedy Gigs Rating','Facebook Popularity Quotient','Live Sports Rating','Value Deals Rating','Live Music Rating','Overall Restaurant Rating','Resturant Tier','Ambience']])

IPQ
327/25:
## Declaring the Linear Regression function

LR = LinearRegression()
327/26:
## Fitting the Linear Regression function

model = LR.fit(IPQ,df_train['Annual Turnover'])
327/27:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
327/28:
## Checking the score of the function on the training data

model.score(IPQ,df_train['Annual Turnover'])
327/29:
df_test = pd.read_csv('Test_dataset.csv')
df_test.head()
327/30:
## Checking for the null values

df_test.isnull().sum()
327/31:
solution_df = pd.DataFrame(df_test['Registration Number'])
solution_df
327/32:
solution_df['Annual Turnover'] = prediction
solution_df
334/1:
import warnings
warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# Libraries to help with model building
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit from the number of displayed columns and rows.
pd.set_option("display.max_columns", None)
# pd.set_option('display.max_rows', None)
pd.set_option("display.max_rows", 200)

# for statistical analysis 
# import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant


# To get diferent metric scores
from sklearn import metrics
from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, f1_score
from sklearn.model_selection import GridSearchCV
334/2:
df = pd.read_csv("backpain.csv")
df.head()
334/3:
data=df.copy()
data.head()
334/4:
plt.figure(figsize=(15, 7))
sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, cmap="Spectral")
plt.show()
334/5:
X = loan.drop(["Status"], axis=1)
y = loan["Status"]
334/6:
X = data.drop(["Status"], axis=1)
y = data["Status"]
334/7:
# encoding the categorical variables
X = pd.get_dummies(X, drop_first=True)
X.head()
334/8: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
334/9:
print("Number of rows in train data =", X_train.shape[0])
print("Number of rows in test data =", X_test.shape[0])
334/10:
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))
334/11:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,train,test,train_y,test_y,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    
    score_list=[] 
    
    pred_train = model.predict(train)
    pred_test = model.predict(test)
    
    train_acc = accuracy_score(pred_train,train_y)
    test_acc = accuracy_score(pred_test,test_y)
    
    train_recall = recall_score(train_y,pred_train)
    test_recall = recall_score(test_y,pred_test)
    
    train_precision = precision_score(train_y,pred_train)
    test_precision = precision_score(test_y,pred_test)
    
    train_f1 = f1_score(train_y,pred_train)
    test_f1 = f1_score(test_y,pred_test)
    
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))
        
     # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",accuracy_score(pred_train,train_y))
        print("Accuracy on test set : ",accuracy_score(pred_test,test_y))
        print("Recall on training set : ",recall_score(train_y,pred_train))
        print("Recall on test set : ",recall_score(test_y,pred_test))
        print("Precision on training set : ",precision_score(train_y,pred_train))
        print("Precision on test set : ",precision_score(test_y,pred_test))
        print("F1 on training set : ",f1_score(train_y,pred_train))
        print("F1 on test set : ",f1_score(test_y,pred_test))
    
    return score_list # returning the list with train and test scores
334/12:
def make_confusion_matrix(model,test_X,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - >50K","Actual - <=50K"]],
                  columns = [i for i in ['Predicted - >50K','Predicted - <=50k']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/13:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/14:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/15:
def make_confusion_matrix(model,test_X,y_actual,labels=['Abnormal', 'Normal']):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - >50K","Actual - <=50K"]],
                  columns = [i for i in ['Predicted - >50K','Predicted - <=50k']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/16:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/17:
def make_confusion_matrix(model,test_X,y_actual,labels=['Abnormal', 'Normal']):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=['Abnormal', 'Normal'])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - Abnormal","Actual - Normal"]],
                  columns = [i for i in ['Predicted - Abnormal','Predicted - Normal']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/18:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/19:
from sklearn import metrics

from sklearn.linear_model import LogisticRegression

# Fit the model on train
model = LogisticRegression(solver="liblinear", random_state=1)
model.fit(x_train, y_train)
#predict on test
y_predict = model.predict(x_test)


coef_df = pd.DataFrame(model.coef_)
coef_df['intercept'] = model.intercept_
print(coef_df)
334/20:
from sklearn.model_selection import train_test_split

X = pdata.drop('Status',axis=1)     # Predictor feature columns (8 X m)
Y = pdata['Status']   # Predicted class (1=True, 0=False) (1 X m)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
# 1 is just any random seed number

x_train.head()
334/21:
from sklearn.model_selection import train_test_split

X = data.drop('Status',axis=1)     # Predictor feature columns (8 X m)
Y = data['Status']   # Predicted class (1=True, 0=False) (1 X m)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
# 1 is just any random seed number

x_train.head()
334/22:
print("{0:0.2f}% data is in training set".format((len(x_train)/len(data.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(x_test)/len(data.index)) * 100))
334/23:
print("Original Diabetes True Values    : {0} ({1:0.2f}%)".format(len(data.loc[data['class'] == 1]), (len(data.loc[data['class'] == 1])/len(data.index)) * 100))
print("Original Diabetes False Values   : {0} ({1:0.2f}%)".format(len(data.loc[data['class'] == 0]), (len(data.loc[data['class'] == 0])/len(data.index)) * 100))
print("")
print("Training Diabetes True Values    : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))
print("Training Diabetes False Values   : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))
print("")
print("Test Diabetes True Values        : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))
print("Test Diabetes False Values       : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))
print("")
334/24:
print("Original Diabetes True Values    : {0} ({1:0.2f}%)".format(len(data.loc[data['Status'] == 1]), (len(data.loc[data['Status'] == 1])/len(data.index)) * 100))
print("Original Diabetes False Values   : {0} ({1:0.2f}%)".format(len(data.loc[data['Status'] == 0]), (len(data.loc[data['Status'] == 0])/len(data.index)) * 100))
print("")
print("Training Diabetes True Values    : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))
print("Training Diabetes False Values   : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))
print("")
print("Test Diabetes True Values        : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))
print("Test Diabetes False Values       : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))
print("")
334/25:
n_true = len(data.loc[pdata['Status'] == True])
n_false = len(data.loc[pdata['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/26:
n_true = len(data.loc[data['Status'] == True])
n_false = len(data.loc[data['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/27:
# encoding the categorical variables
X = pd.get_dummies(X, drop_first=True)
X.head()
334/28: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
334/29:
print("Number of rows in train data =", X_train.shape[0])
print("Number of rows in test data =", X_test.shape[0])
334/30:
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))
334/31:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,train,test,train_y,test_y,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    
    score_list=[] 
    
    pred_train = model.predict(train)
    pred_test = model.predict(test)
    
    train_acc = accuracy_score(pred_train,train_y)
    test_acc = accuracy_score(pred_test,test_y)
    
    train_recall = recall_score(train_y,pred_train)
    test_recall = recall_score(test_y,pred_test)
    
    train_precision = precision_score(train_y,pred_train)
    test_precision = precision_score(test_y,pred_test)
    
    train_f1 = f1_score(train_y,pred_train)
    test_f1 = f1_score(test_y,pred_test)
    
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))
        
     # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",accuracy_score(pred_train,train_y))
        print("Accuracy on test set : ",accuracy_score(pred_test,test_y))
        print("Recall on training set : ",recall_score(train_y,pred_train))
        print("Recall on test set : ",recall_score(test_y,pred_test))
        print("Precision on training set : ",precision_score(train_y,pred_train))
        print("Precision on test set : ",precision_score(test_y,pred_test))
        print("F1 on training set : ",f1_score(train_y,pred_train))
        print("F1 on test set : ",f1_score(test_y,pred_test))
    
    return score_list # returning the list with train and test scores
334/32:
def make_confusion_matrix(model,test_X,y_actual,labels=['Abnormal', 'Normal']):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=['Abnormal', 'Normal'])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - Abnormal","Actual - Normal"]],
                  columns = [i for i in ['Predicted - Abnormal','Predicted - Normal']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/33:
n_true = len(data.loc[data['Status'] == True])
n_false = len(data.loc[data['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/34:
n_true = len(data.loc[data['Status'] == True])
n_false = len(data.loc[data['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/35:
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)
334/36:
print("Accuracy on training set : ",dTree.score(X_train, y_train))
print("Accuracy on test set : ",dTree.score(X_test, y_test))
334/37:
#Checking number of positives
y.sum(axis = 0)
334/38:
replaceStruct = {
                "Status":     {"Abnormal": 1, "Normal": 0 }
                }
334/39:
data=data.replace(replaceStruct)

data.head(10)
334/40: data.tail()
334/41:
plt.figure(figsize=(15, 7))
sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, cmap="Spectral")
plt.show()
334/42:
n_true = len(data.loc[data['Status'] == True])
n_false = len(data.loc[data['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/43:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/44:
data=data.replace(replaceStruct)

data.head(10)
334/45: data.tail()
334/46:
plt.figure(figsize=(15, 7))
sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, cmap="Spectral")
plt.show()
334/47:
n_true = len(data.loc[data['Status'] == True])
n_false = len(data.loc[data['Status'] == False])
print("Number of true cases: {0} ({1:2.2f}%)".format(n_true, (n_true / (n_true + n_false)) * 100 ))
print("Number of false cases: {0} ({1:2.2f}%)".format(n_false, (n_false / (n_true + n_false)) * 100))
334/48:
X = data.drop(["Status"], axis=1)
y = data["Status"]
334/49:
# encoding the categorical variables
X = pd.get_dummies(X, drop_first=True)
X.head()
334/50: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
334/51:
print("Number of rows in train data =", X_train.shape[0])
print("Number of rows in test data =", X_test.shape[0])
334/52:
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))
334/53:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,train,test,train_y,test_y,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    
    score_list=[] 
    
    pred_train = model.predict(train)
    pred_test = model.predict(test)
    
    train_acc = accuracy_score(pred_train,train_y)
    test_acc = accuracy_score(pred_test,test_y)
    
    train_recall = recall_score(train_y,pred_train)
    test_recall = recall_score(test_y,pred_test)
    
    train_precision = precision_score(train_y,pred_train)
    test_precision = precision_score(test_y,pred_test)
    
    train_f1 = f1_score(train_y,pred_train)
    test_f1 = f1_score(test_y,pred_test)
    
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))
        
     # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",accuracy_score(pred_train,train_y))
        print("Accuracy on test set : ",accuracy_score(pred_test,test_y))
        print("Recall on training set : ",recall_score(train_y,pred_train))
        print("Recall on test set : ",recall_score(test_y,pred_test))
        print("Precision on training set : ",precision_score(train_y,pred_train))
        print("Precision on test set : ",precision_score(test_y,pred_test))
        print("F1 on training set : ",f1_score(train_y,pred_train))
        print("F1 on test set : ",f1_score(test_y,pred_test))
    
    return score_list # returning the list with train and test scores
334/54:
def make_confusion_matrix(model,test_X,y_actual,labels=['Abnormal', 'Normal']):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(test_X)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=['Abnormal', 'Normal'])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - Abnormal","Actual - Normal"]],
                  columns = [i for i in ['Predicted - Abnormal','Predicted - Normal']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/55:
from sklearn.model_selection import train_test_split

X = data.drop('Status',axis=1)     # Predictor feature columns (8 X m)
Y = data['Status']   # Predicted class (1=True, 0=False) (1 X m)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
# 1 is just any random seed number

x_train.head()
334/56:
print("{0:0.2f}% data is in training set".format((len(x_train)/len(data.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(x_test)/len(data.index)) * 100))
334/57:
print("Original Diabetes True Values    : {0} ({1:0.2f}%)".format(len(data.loc[data['Status'] == 1]), (len(data.loc[data['Status'] == 1])/len(data.index)) * 100))
print("Original Diabetes False Values   : {0} ({1:0.2f}%)".format(len(data.loc[data['Status'] == 0]), (len(data.loc[data['Status'] == 0])/len(data.index)) * 100))
print("")
print("Training Diabetes True Values    : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train)) * 100))
print("Training Diabetes False Values   : {0} ({1:0.2f}%)".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train)) * 100))
print("")
print("Test Diabetes True Values        : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test)) * 100))
print("Test Diabetes False Values       : {0} ({1:0.2f}%)".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test)) * 100))
print("")
334/58:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/59:
from sklearn import metrics

from sklearn.linear_model import LogisticRegression

# Fit the model on train
model = LogisticRegression(solver="liblinear", random_state=1)
model.fit(x_train, y_train)
#predict on test
y_predict = model.predict(x_test)


coef_df = pd.DataFrame(model.coef_)
coef_df['intercept'] = model.intercept_
print(coef_df)
334/60:
dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)
334/61:
print("Accuracy on training set : ",dTree.score(X_train, y_train))
print("Accuracy on test set : ",dTree.score(X_test, y_test))
334/62:
#Checking number of positives
y.sum(axis = 0)
334/63:
model_score = model.score(x_test, y_test)
print(model_score)
334/64:
cm=metrics.confusion_matrix(y_test, y_predict, labels=[1, 0])

df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 1"," Actual 0"]],
                  columns = [i for i in ["Predict 1","Predict 0"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True,fmt='g')
plt.show()
334/65:
# Recall on train and test
get_recall_score(dTree)
334/66:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
334/67:
##  Function to calculate recall score
def get_recall_score(model):
    '''
    model : classifier to predict values of X

    '''
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
    print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
334/68:
# Recall on train and test
get_recall_score(dTree)
334/69: make_confusion_matrix(dTree,y_test)
334/70:
# Recall on train and test
get_recall_score(dTree)
334/71:
feature_names = list(X.columns)
print(feature_names)
334/72:
plt.figure(figsize=(20,30))
tree.plot_tree(dTree,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/73:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree,feature_names=feature_names,show_weights=True))
334/74:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/75:
importances = dTree.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
334/76:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)
dTree1.fit(X_train, y_train)
334/77: make_confusion_matrix(dTree1, y_test)
334/78:
# Accuracy on train and test
print("Accuracy on training set : ",dTree1.score(X_train, y_train))
print("Accuracy on test set : ",dTree1.score(X_test, y_test))
# Recall on train and test
get_recall_score(dTree1)
334/79:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree1,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/80:
# Text report showing the rules of a decision tree -

print(tree.export_text(dTree1,feature_names=feature_names,show_weights=True))
334/81:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree1.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/82:
importances = dTree1.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10,10))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
334/83: from sklearn.model_selection import GridSearchCV
334/84:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/85: make_confusion_matrix(estimator, y_test)
334/86:
# Accuracy on train and test
print("Accuracy on training set : ", estimator.score(X_train, y_train))
print("Accuracy on test set : ", estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
334/87:
plt.figure(figsize=(15, 10))

tree.plot_tree(
    estimator,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=True,
    class_names=True,
)
plt.show()
334/88:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(1, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/89:
# Recall on train and test
get_recall_score(estimator)
334/90:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(3, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/91:
# Recall on train and test
get_recall_score(estimator)
334/92:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(3, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/93:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(4, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/94:
# Recall on train and test
get_recall_score(estimator)
334/95:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(5, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/96:
# Recall on train and test
get_recall_score(estimator)
334/97:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(6|, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/98:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(6, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/99:
# Recall on train and test
get_recall_score(estimator)
334/100:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(7, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/101:
# Recall on train and test
get_recall_score(estimator)
334/102:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(8, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/103:
# Recall on train and test
get_recall_score(estimator)
334/104:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(1, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/105:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/106:
# Recall on train and test
get_recall_score(estimator)
334/107:
# Recall on train and test
get_recall_score(estimator)
334/108:
# Recall on train and test
get_recall_score(estimator)
334/109: from sklearn.model_selection import GridSearchCV
334/110:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(1, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/111:
# Recall on train and test
get_recall_score(estimator)
334/112:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/113: make_confusion_matrix(estimator, y_test)
334/114:
# Accuracy on train and test
print("Accuracy on training set : ", estimator.score(X_train, y_train))
print("Accuracy on test set : ", estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
334/115:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(3, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/116:
# Recall on train and test
get_recall_score(estimator)
334/117:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(4, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/118:
# Recall on train and test
get_recall_score(estimator)
334/119:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(5, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/120:
# Recall on train and test
get_recall_score(estimator)
334/121:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(6, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/122:
# Recall on train and test
get_recall_score(estimator)
334/123:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(7, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/124:
# Recall on train and test
get_recall_score(estimator)
334/125:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(8, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/126:
# Recall on train and test
get_recall_score(estimator)
334/127:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(1, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/128:
# Recall on train and test
get_recall_score(estimator)
334/129:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/130:
# Recall on train and test
get_recall_score(estimator)
334/131:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1, class_weight={0: 0.15, 1: 0.85})

# Grid of parameters to choose from
parameters = {
    "max_depth": [5, 10, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
scorer = make_scorer(recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/132:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1, class_weight={0: 0.15, 1: 0.85})

# Grid of parameters to choose from
parameters = {
    "max_depth": [5, 10, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
scorer = make_scorer(get_recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/133:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/134:
# Recall on train and test
get_recall_score(estimator)
334/135:
# Recall on train and test
get_recall_score(estimator)
334/136:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1, class_weight={0: 0.15, 1: 0.85})

# Grid of parameters to choose from
parameters = {
    "max_depth": [5, 10, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
scorer = make_scorer(get_recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/137:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 10, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/138:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(5, 1, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/139:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(5,1, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/140:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(5,50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/141:
# Recall on train and test
get_recall_score(estimator)
334/142:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": np.arange(2, 50, 5),
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.000001, 0.00001, 0.0001],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/143:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=3,random_state=1)
dTree1.fit(X_train, y_train)
334/144:
# Accuracy on train and test
print("Accuracy on training set : ",dTree1.score(X_train, y_train))
print("Accuracy on test set : ",dTree1.score(X_test, y_test))
# Recall on train and test
get_recall_score(dTree1)
334/145:
model = LogisticRegression(random_state=1)
lg = model.fit(X_train,y_train)
scores_LR = get_metrics_score(lg,X_train,X_test,y_train,y_test,flag=True)

# creating confusion matrix
make_confusion_matrix(lg,X_test,y_test)
334/146:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 1, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/147:
# Accuracy on train and test
print("Accuracy on training set : ", estimator.score(X_train, y_train))
print("Accuracy on test set : ", estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
334/148:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 2, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/149:
# Recall on train and test
get_recall_score(estimator)
334/150:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 2, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/151:
# Recall on train and test
get_recall_score(estimator)
334/152:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/153:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/154:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 2, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/155:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 1, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/156:
# Accuracy on train and test
print("Accuracy on training set : ", estimator.score(X_train, y_train))
print("Accuracy on test set : ", estimator.score(X_test, y_test))
# Recall on train and test
get_recall_score(estimator)
334/157:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 2, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/158:
# Recall on train and test
get_recall_score(estimator)
334/159:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/160:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 4, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/161:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/162:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/163:
# Recall on train and test
get_recall_score(estimator)
334/164:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 4, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/165:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 3, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/166:
# Recall on train and test
get_recall_score(estimator)
334/167:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 4, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/168:
# Recall on train and test
get_recall_score(estimator)
334/169:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 5, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/170:
# Recall on train and test
get_recall_score(estimator)
334/171:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 5, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/172:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 6, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/173:
# Recall on train and test
get_recall_score(estimator)
334/174:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 7, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/175:
 Recall on train and test
get_recall_score(estimator)
334/176:
Recall on train and test
get_recall_score(estimator)
334/177:
# Recall on train and test
get_recall_score(estimator)
334/178:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 7, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/179:
# Recall on train and test
get_recall_score(estimator)
334/180:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 8, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/181:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 7, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/182:
# Recall on train and test
get_recall_score(estimator)
334/183:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 8, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/184:
# Recall on train and test
get_recall_score(estimator)
334/185:

# Recall on train and test
get_recall_score(estimator)
334/186:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 8, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/187:
# Recall on train and test
get_recall_score(estimator)
334/188:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree1.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/189:
dTree1 = DecisionTreeClassifier(criterion = 'gini',max_depth=1,random_state=1)
dTree1.fit(X_train, y_train)
334/190:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=1,random_state=1)
dTree2.fit(X_train, y_train)
334/191:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/192:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/193:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=2,random_state=1)
dTree2.fit(X_train, y_train)
334/194:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/195:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/196:
# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    "max_depth": [5, 8, 15, None],
    "criterion": ["entropy", "gini"],
    "splitter": ["best", "random"],
    "min_impurity_decrease": [0.00001, 0.0001, 0.01],
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=3)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train, y_train)
334/197:
# Recall on train and test
get_recall_score(estimator)
334/198:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=5,random_state=1)
dTree2.fit(X_train, y_train)
334/199:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/200:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/201:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=1,random_state=1)
dTree2.fit(X_train, y_train)
334/202:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/203:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/204:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=2,random_state=1)
dTree2.fit(X_train, y_train)
334/205:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/206:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/207:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=5,random_state=1)
dTree2.fit(X_train, y_train)
334/208:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/209:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/210:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=8,random_state=1)
dTree2.fit(X_train, y_train)
334/211:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/212:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/213:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=5,random_state=1)
dTree2.fit(X_train, y_train)
334/214:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/215:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=2,random_state=1)
dTree2.fit(X_train, y_train)
334/216:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/217:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/218:
dTree2 = DecisionTreeClassifier(criterion = 'gini',max_depth=5,random_state=1)
dTree2.fit(X_train, y_train)
334/219:
plt.figure(figsize=(15,10))

tree.plot_tree(dTree2,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)
plt.show()
334/220:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print (pd.DataFrame(dTree2.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
334/221:
clf = DecisionTreeClassifier(random_state=1, class_weight={0: 0.15, 1: 0.85})
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
334/222: pd.DataFrame(path)
334/223:
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")
plt.show()
334/224:
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=1, ccp_alpha=ccp_alpha, class_weight={0: 0.15, 1: 0.85}
    )
    clf.fit(X_train, y_train)
    clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)
334/225:
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1, figsize=(10, 7))
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()
334/226:
recall_train = []
for clf in clfs:
    pred_train = clf.predict(X_train)
    values_train = recall_score(y_train, pred_train)
    recall_train.append(values_train)
334/227:
recall_test = []
for clf in clfs:
    pred_test = clf.predict(X_test)
    values_test = recall_score(y_test, pred_test)
    recall_test.append(values_test)
334/228:
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]
334/229:
fig, ax = plt.subplots(figsize=(15, 5))
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(
    ccp_alphas, recall_train, marker="o", label="train", drawstyle="steps-post",
)
ax.plot(ccp_alphas, recall_test, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()
335/1:
# Code to read csv file into colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
335/2:
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
335/3:
downloaded = drive.CreateFile({'id':'1CnN9q63xEhvWlFDMDsGrBf1vwOCMHq4D'}) # replace the id with id of file you want to access
downloaded.GetContentFile('used_cars_data.csv')
335/4: !pip install nb_black
335/5: !pip install IPython.core.inputtransformer2
335/6:


# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to split the data into train and test
from sklearn.model_selection import train_test_split

# to build linear regression_model
from sklearn.linear_model import LinearRegression

# to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# to suppress warnings
import warnings

warnings.filterwarnings("ignore")
335/7:
# loading the dataset
data = pd.read_csv("used_cars_data.csv")
335/8:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
335/9:
# let's view a sample of the data
data.sample(n=10, random_state=1)
335/10:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
335/11:
# drop the S.No. column as it does not add any value to the analysis
df.drop("S.No.", axis=1, inplace=True)
335/12:
# checking column datatypes and number of non-null values
df.info()
335/13:
# checking for duplicate values
df.duplicated().sum()
335/14: df[df.duplicated(keep=False) == True]
341/1:
# this will help in making the Python code more structured automatically (good coding practice)
%load_ext nb_black

# Library to suppress warnings or deprecation notes
import warnings

warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data

import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# Libraries to build decision tree classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV

# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    plot_confusion_matrix,
    make_scorer,
)
341/2: data = pd.read_csv("Loan_Delinquent_Dataset.csv")
341/3:
# copying data to another varaible to avoid any changes to original data
loan = data.copy()
341/4: loan.head()
348/1:
# Library to suppress warnings or deprecation notes
import warnings

warnings.filterwarnings("ignore")

# Libraries to help with reading and manipulating data

import pandas as pd
import numpy as np

# Library to split data
from sklearn.model_selection import train_test_split

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)




# Libraries to build decision tree classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To tune different models
from sklearn.model_selection import GridSearchCV

# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    plot_confusion_matrix,
    make_scorer,
)
348/2:
# loading the dataset
data = pd.read_csv("Loan_Modelling.csv")
348/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
348/4:
# let's view a sample of the data
data.sample(n=10, random_state=1)
348/5: data["ID"].nunique()
348/6:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
348/7:
# drop the ID column as it does not add any value to the analysis
df.drop("ID", axis=1, inplace=True)
348/8:

# Also replacing the categorical var with actual values
df['Education'] = df['Education'].replace({1: 'Undergrad', 2: 'Graduate', 3: 'Professional'})
df.head()
348/9:
# checking column datatypes and number of non-null values
df.info()
348/10:
# checking for duplicate values
df.duplicated().sum()
348/11:
df = pd.get_dummies(df, columns=['Education'])
df.head()
348/12:
#Let's review the data information once again
df.info()
348/13:
#Let's get a detail description of the processed data
df.describe().round(2).T
348/14:
# checking Income grouped by Age
df.groupby(["Age"])["Income"].mean().round(2).sort_values(ascending=False)
348/15: !pip install uszipcode
348/16: !pip install uszipcode
348/17:
from uszipcode import SearchEngine
search = SearchEngine()
county = []

for i in np.arange(0, len(df['ZIPCode'])):
  zipcode = search.by_zipcode(df['ZIPCode'][i])
  county.append(zipcode.county)

df['county'] = county
df.head()
350/1:
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
import numpy as np
import seaborn as sns
350/2: ch = pd.read_csv("Churn.csv")
350/3: ch
350/4:
plt.hist(ch['total day minutes'], bins= 10, facecolor= 'tan')
plt.xlabel('Total Day Minutes')
plt.ylabel('No. of Customers')
plt.show()
350/5:
import seaborn as sns
g = sns.FacetGrid(ch, col="churn")
g.map(plt.hist, "total day minutes")
350/6: ch['voice mail plan'].value_counts()
350/7:
sns.set(style="whitegrid", color_codes=True)
sns.countplot(x="voice mail plan", hue= "churn", data=ch)
350/8: sns.boxplot(x = "international plan", y = "area code", data=ch)
350/9: pd.crosstab(ch['area code'],ch['churn'])
350/10: pd.pivot_table(ch, index = ['area code','voice mail plan'], columns=['international plan'], aggfunc=len)
350/11: pd.pivot_table(ch, index = ['area code','voice mail plan'], columns=['international plan'], aggfunc=len)
350/12: pd.pivot_table(ch, 'total intl minutes', index = ['area code','voice mail plan'], columns=['international plan'],  aggfunc=sum)
350/13:
plt.figure(figsize = (16,16))
corr = ch.corr()
sns.heatmap(corr, annot = True)
350/14: ch['total night calls'].std()
350/15: sns.distplot(ch['total night calls'])
350/16: ch.hist(by='churn', column = 'area code')
350/17:
ch['area code']= ch['area code'].astype('category')
sns.countplot(x="area code", hue= "churn", data=ch)
351/1:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
353/1:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
353/2: !pip install xgboost
353/3:
import pandas as pd
import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
358/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
358/2: pima=pd.read_csv("pima-indians-diabetes.csv")
358/3:
# copying data to another varaible to avoid any changes to original data
data=pima.copy()
361/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Libraries to split data, impute missing values 
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
361/2: wine = pd.read_csv('Cars-dataset.csv',sep=';')
361/3:
# copying data to another varaible to avoid any changes to original data
data = wine.copy()
361/4: data.head()
361/5: wine = pd.read_csv('Cars-dataset.csv')
361/6:
# copying data to another varaible to avoid any changes to original data
data = wine.copy()
361/7: data.head()
361/8: data.tail()
361/9: data.shape
361/10: data.info()
361/11: data.describe().T
361/12:
plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.1g',cmap="Spectral")
plt.show()
361/13:
plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.2f')
plt.show()
361/14:
replaceStruct = {
                "Gender":     {"Male": 1, "Female": 2 },
               
                    }
oneHotCols=["Gender"]
361/15:
creditData=creditData.replace(replaceStruct)
creditData=pd.get_dummies(creditData, columns=oneHotCols)
creditData.head(10)
361/16:
Data=creditData.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/17:
replaceStruct = {
                "Gender":     {"Male": 1, "Female": 2 },
               
                    }
oneHotCols=["Gender"]
361/18:
Data=creditData.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/19:
Data=Data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/20:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/21:
replaceStruct = {
                "Gender":     {"Male": 0, "Female": 1 },
               
                    }
oneHotCols=["Gender"]
361/22:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/23:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/24:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
361/25: y.value_counts(1)
361/26:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape, X_test.shape)
361/27: y.value_counts(1)
361/28: y_test.value_counts(1)
361/29:
X = Data.drop('Gender',axis=1)
y = Data['Gender']
361/30:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/31:
X = Data.drop('Gender',axis=1)
y = Data['Gender']
361/32:
X = Data.drop('Gender_0',axis=1)
y = Data['Gender_0']
361/33:
X = Data.drop('Gender_0', 'Gender_1',axis=1)
y = Data['Gender_0', 'Gender_1']
361/34:
X = data.drop('Class',axis=1)
y = data['Class']
361/35:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/36:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape, X_test.shape)
361/37: y.value_counts(1)
361/38: y_test.value_counts(1)
361/39:
X = data.drop("default" , axis=1)
y = data.pop("default")
361/40:
X = data.drop("Opt_service" , axis=1)
y = data.pop("Opt_service)
361/41:
X = data.drop("Opt_service" , axis=1)
y = data.pop("Opt_service")
361/42: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
361/43:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
361/44:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    #Predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    #Accuracy of the model
    train_acc = model.score(X_train,y_train)
    test_acc = model.score(X_test,y_test)
    
    #Recall of the model
    train_recall = metrics.recall_score(y_train,pred_train)
    test_recall = metrics.recall_score(y_test,pred_test)
    
    #Precision of the model
    train_precision = metrics.precision_score(y_train,pred_train)
    test_precision = metrics.precision_score(y_test,pred_test)
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))
        
    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",model.score(X_train,y_train))
        print("Accuracy on test set : ",model.score(X_test,y_test))
        print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
        print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
        print("Precision on training set : ",metrics.precision_score(y_train,pred_train))
        print("Precision on test set : ",metrics.precision_score(y_test,pred_test))
    
    return score_list # returning the list with train and test scores
361/45:
#base_estimator for bagging classifier is a decision tree by default
bagging_estimator=BaggingClassifier(random_state=1)
bagging_estimator.fit(X_train,y_train)
361/46:
X = Data.drop("Opt_service" , axis=1)
y = Data.pop("Opt_service")
361/47: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
361/48:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
361/49:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    #Predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    #Accuracy of the model
    train_acc = model.score(X_train,y_train)
    test_acc = model.score(X_test,y_test)
    
    #Recall of the model
    train_recall = metrics.recall_score(y_train,pred_train)
    test_recall = metrics.recall_score(y_test,pred_test)
    
    #Precision of the model
    train_precision = metrics.precision_score(y_train,pred_train)
    test_precision = metrics.precision_score(y_test,pred_test)
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))
        
    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",model.score(X_train,y_train))
        print("Accuracy on test set : ",model.score(X_test,y_test))
        print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
        print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
        print("Precision on training set : ",metrics.precision_score(y_train,pred_train))
        print("Precision on test set : ",metrics.precision_score(y_test,pred_test))
    
    return score_list # returning the list with train and test scores
361/50:
#base_estimator for bagging classifier is a decision tree by default
bagging_estimator=BaggingClassifier(random_state=1)
bagging_estimator.fit(X_train,y_train)
361/51:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_score=get_metrics_score(bagging_estimator)
361/52: make_confusion_matrix(bagging_estimator,y_test)
361/53:
replaceStruct = {
                "Gender":     {"Male": 0, "Female": 1 },
               
                    }
oneHotCols=["Gender"]
361/54:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/55:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/56:
X = Data.drop('Gender',axis=1)
y = Data['Gender']
361/57:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/58:
X = Data.drop("Opt_service" , axis=1)
y = Data.pop("Opt_service")
361/59: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
361/60:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/61:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
361/62:
X = data.drop('Gender',axis=1)
y = data['Gender']
361/63:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape, X_test.shape)
361/64: y.value_counts(1)
361/65: y_test.value_counts(1)
361/66:
X = Data.drop("Opt_service" , axis=1)
y = Data.pop("Opt_service")
361/67: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
361/68:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
361/69:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    #Predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    #Accuracy of the model
    train_acc = model.score(X_train,y_train)
    test_acc = model.score(X_test,y_test)
    
    #Recall of the model
    train_recall = metrics.recall_score(y_train,pred_train)
    test_recall = metrics.recall_score(y_test,pred_test)
    
    #Precision of the model
    train_precision = metrics.precision_score(y_train,pred_train)
    test_precision = metrics.precision_score(y_test,pred_test)
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))
        
    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",model.score(X_train,y_train))
        print("Accuracy on test set : ",model.score(X_test,y_test))
        print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
        print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
        print("Precision on training set : ",metrics.precision_score(y_train,pred_train))
        print("Precision on test set : ",metrics.precision_score(y_test,pred_test))
    
    return score_list # returning the list with train and test scores
361/70:
#base_estimator for bagging classifier is a decision tree by default
bagging_estimator=BaggingClassifier(random_state=1)
bagging_estimator.fit(X_train,y_train)
361/71:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_score=get_metrics_score(bagging_estimator)
361/72: make_confusion_matrix(bagging_estimator,y_test)
368/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Libraries to split data, impute missing values 
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
368/2: wine = pd.read_csv('Cars-dataset.csv')
368/3:
# copying data to another varaible to avoid any changes to original data
data = wine.copy()
368/4: data.head()
368/5: data.tail()
368/6: data.shape
368/7: data.info()
368/8: data.describe().T
368/9:
plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.2f')
plt.show()
368/10:
replaceStruct = {
                "Gender":     {"Male": 0, "Female": 1 },
               
                    }
oneHotCols=["Gender"]
368/11:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
368/12:
X = data.drop('Gender',axis=1)
y = data['Gender']
368/13:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape, X_test.shape)
368/14: y.value_counts(1)
368/15: y_test.value_counts(1)
368/16:
X = Data.drop("Opt_service" , axis=1)
y = Data.pop("Opt_service")
368/17: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
368/18:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
368/19:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    #Predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    #Accuracy of the model
    train_acc = model.score(X_train,y_train)
    test_acc = model.score(X_test,y_test)
    
    #Recall of the model
    train_recall = metrics.recall_score(y_train,pred_train)
    test_recall = metrics.recall_score(y_test,pred_test)
    
    #Precision of the model
    train_precision = metrics.precision_score(y_train,pred_train)
    test_precision = metrics.precision_score(y_test,pred_test)
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))
        
    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",model.score(X_train,y_train))
        print("Accuracy on test set : ",model.score(X_test,y_test))
        print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
        print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
        print("Precision on training set : ",metrics.precision_score(y_train,pred_train))
        print("Precision on test set : ",metrics.precision_score(y_test,pred_test))
    
    return score_list # returning the list with train and test scores
368/20:
#base_estimator for bagging classifier is a decision tree by default
bagging_estimator=BaggingClassifier(random_state=1)
bagging_estimator.fit(X_train,y_train)
368/21:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_score=get_metrics_score(bagging_estimator)
368/22: make_confusion_matrix(bagging_estimator,y_test)
368/23:
#Train the random forest classifier
rf_estimator=RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)
368/24:
#Using above defined function to get accuracy, recall and precision on train and test set
rf_estimator_score=get_metrics_score(rf_estimator)
368/25: make_confusion_matrix(rf_estimator,y_test)
368/26:
bagging_lr=BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1,max_iter=1000),random_state=1)
bagging_lr.fit(X_train,y_train)
368/27:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Libraries to split data, impute missing values 
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
368/28:
bagging_lr=BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1,max_iter=1000),random_state=1)
bagging_lr.fit(X_train,y_train)
368/29:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_lr_score=get_metrics_score(bagging_lr)
368/30: make_confusion_matrix(bagging_lr,y_test)
368/31:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
368/32:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_tuned_score=get_metrics_score(bagging_estimator_tuned)
368/33:
abc = AdaBoostClassifier(random_state=1)
abc.fit(X_train,y_train)
368/34:
#Using above defined function to get accuracy, recall and precision on train and test set
abc_score=get_metrics_score(abc)
368/35: make_confusion_matrix(abc,y_test)
368/36:
gbc = GradientBoostingClassifier(random_state=1)
gbc.fit(X_train,y_train)
368/37:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_score=get_metrics_score(gbc)
368/38: make_confusion_matrix(gbc,y_test)
368/39:
xgb = XGBClassifier(random_state=1,eval_metric='logloss')
xgb.fit(X_train,y_train)
368/40:
#Using above defined function to get accuracy, recall and precision on train and test set
xgb_score=get_metrics_score(xgb)
368/41: make_confusion_matrix(xgb,y_test)
368/42:
importances = abc_tuned.feature_importances_
indices = np.argsort(importances)
feature_names = list(X.columns)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
368/43:
importances = xgb.feature_importances_
indices = np.argsort(importances)
feature_names = list(X.columns)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
368/44:
importances = gbc.feature_importances_
indices = np.argsort(importances)
feature_names = list(X.columns)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
368/45:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "base_estimator":[DecisionTreeClassifier(max_depth=1, random_state=1),DecisionTreeClassifier(max_depth=2, random_state=1),DecisionTreeClassifier(max_depth=3, random_state=1)],
    "n_estimators": np.arange(10,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(abc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
abc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
abc_tuned.fit(X_train, y_train)
368/46:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "base_estimator":[DecisionTreeClassifier(max_depth=1, random_state=1),DecisionTreeClassifier(max_depth=2, random_state=1),DecisionTreeClassifier(max_depth=3, random_state=1)],
    "n_estimators": np.arange(10,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/47:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "base_estimator":[DecisionTreeClassifier(max_depth=1, random_state=1),DecisionTreeClassifier(max_depth=2, random_state=1),DecisionTreeClassifier(max_depth=3, random_state=1)],
    "n_estimators": np.arange(10,110,10),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/48:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "base_estimator":[GradientBoostingClassifier(max_depth=1, random_state=1),DecisionTreeClassifier(max_depth=2, random_state=1),DecisionTreeClassifier(max_depth=3, random_state=1)],
    "n_estimators": np.arange(10,110,10),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/49:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(10,110,10),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/50:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(10,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/51:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(50,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/52:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(abc_tuned)
368/53:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(50,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/54:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(abc_tuned)
368/55:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/56:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(100,110,100),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/57:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/58:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/59:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/60:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(50,110,50),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/61:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/62:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/63:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators":np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/64:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators":np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/65:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.1,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/66:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(50,110,50),
    "learning_rate":np.arange(0.01,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/67:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.01,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/68:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/69:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Libraries to split data, impute missing values 
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
368/70: wine = pd.read_csv('Cars-dataset.csv')
368/71:
# copying data to another varaible to avoid any changes to original data
data = wine.copy()
368/72: data.head()
368/73: data.tail()
368/74: data.shape
368/75: data.info()
368/76: data.describe().T
368/77:
plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,fmt='.2f')
plt.show()
368/78:
replaceStruct = {
                "Gender":     {"Male": 0, "Female": 1 },
               
                    }
oneHotCols=["Gender"]
368/79:
Data=data.replace(replaceStruct)
Data=pd.get_dummies(Data, columns=oneHotCols)
Data.head(10)
368/80:
X = data.drop('Gender',axis=1)
y = data['Gender']
368/81:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_train.shape, X_test.shape)
368/82: y.value_counts(1)
368/83: y_test.value_counts(1)
368/84:
X = Data.drop("Opt_service" , axis=1)
y = Data.pop("Opt_service")
368/85: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)
368/86:
## Function to create confusion matrix
def make_confusion_matrix(model,y_actual,labels=[1, 0]):
    '''
    model : classifier to predict values of X
    y_actual : ground truth  
    
    '''
    y_predict = model.predict(X_test)
    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])
    df_cm = pd.DataFrame(cm, index = [i for i in ["Actual - No","Actual - Yes"]],
                  columns = [i for i in ['Predicted - No','Predicted - Yes']])
    group_counts = ["{0:0.0f}".format(value) for value in
                cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in
                         cm.flatten()/np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in
              zip(group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    plt.figure(figsize = (10,7))
    sns.heatmap(df_cm, annot=labels,fmt='')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
368/87:
##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision
def get_metrics_score(model,flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    #Predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    #Accuracy of the model
    train_acc = model.score(X_train,y_train)
    test_acc = model.score(X_test,y_test)
    
    #Recall of the model
    train_recall = metrics.recall_score(y_train,pred_train)
    test_recall = metrics.recall_score(y_test,pred_test)
    
    #Precision of the model
    train_precision = metrics.precision_score(y_train,pred_train)
    test_precision = metrics.precision_score(y_test,pred_test)
    
    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))
        
    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True: 
        print("Accuracy on training set : ",model.score(X_train,y_train))
        print("Accuracy on test set : ",model.score(X_test,y_test))
        print("Recall on training set : ",metrics.recall_score(y_train,pred_train))
        print("Recall on test set : ",metrics.recall_score(y_test,pred_test))
        print("Precision on training set : ",metrics.precision_score(y_train,pred_train))
        print("Precision on test set : ",metrics.precision_score(y_test,pred_test))
    
    return score_list # returning the list with train and test scores
368/88:
#base_estimator for bagging classifier is a decision tree by default
bagging_estimator=BaggingClassifier(random_state=1)
bagging_estimator.fit(X_train,y_train)
368/89:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_score=get_metrics_score(bagging_estimator)
368/90: make_confusion_matrix(bagging_estimator,y_test)
368/91:
#Train the random forest classifier
rf_estimator=RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)
368/92:
#Using above defined function to get accuracy, recall and precision on train and test set
rf_estimator_score=get_metrics_score(rf_estimator)
368/93: make_confusion_matrix(rf_estimator,y_test)
368/94:
bagging_lr=BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1,max_iter=1000),random_state=1)
bagging_lr.fit(X_train,y_train)
368/95:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_lr_score=get_metrics_score(bagging_lr)
368/96: make_confusion_matrix(bagging_lr,y_test)
368/97:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
368/98:
#Using above defined function to get accuracy, recall and precision on train and test set
bagging_estimator_tuned_score=get_metrics_score(bagging_estimator_tuned)
368/99:
abc = AdaBoostClassifier(random_state=1)
abc.fit(X_train,y_train)
368/100:
#Using above defined function to get accuracy, recall and precision on train and test set
abc_score=get_metrics_score(abc)
368/101: make_confusion_matrix(abc,y_test)
368/102:
gbc = GradientBoostingClassifier(random_state=1)
gbc.fit(X_train,y_train)
368/103:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_score=get_metrics_score(gbc)
368/104: make_confusion_matrix(gbc,y_test)
368/105:
xgb = XGBClassifier(random_state=1,eval_metric='logloss')
xgb.fit(X_train,y_train)
368/106:
#Using above defined function to get accuracy, recall and precision on train and test set
xgb_score=get_metrics_score(xgb)
368/107: make_confusion_matrix(xgb,y_test)
368/108:
importances = gbc.feature_importances_
indices = np.argsort(importances)
feature_names = list(X.columns)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
368/109:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(50,110,50),
    "learning_rate":np.arange(0.01,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/110:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/111:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(100,110,100),
    "learning_rate":np.arange(0.01,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/112:
#Using above defined function to get accuracy, recall and precision on train and test set
gbc_tuned_score=get_metrics_score(gbc_tuned)
368/113:
# Choose the type of classifier. 
gbc_tuned = GradientBoostingClassifier(random_state=1)

# Grid of parameters to choose from
## add from article
parameters = {
    #Let's try different max_depth for base_estimator
    "n_estimators": np.arange(400,110,400),
    "learning_rate":np.arange(0.01,2,0.01)
}

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
gbc_tuned.fit(X_train, y_train)
368/114:
estimators = [('Random Forest',rf_tuned), ('Gradient Boosting',gbc_tuned), ('Decision Tree',dtree_estimator)]

final_estimator = xgb_tuned

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/115:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator,X_test,y_test)
368/116:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Libraries to split data, impute missing values 
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
368/117:
estimators = [('Random Forest',rf_tuned), ('Gradient Boosting',gbc_tuned), ('Decision Tree',dtree_estimator)]

final_estimator = xgb_tuned

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/118:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator,X_test,y_test)
368/119:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
368/120:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator,X_test,y_test)
368/121:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
368/122:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator,X_test,y_test)
368/123:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.18,1:0.82},random_state=1,oob_score=True,bootstrap=True)

parameters = {  
                'max_depth': list(np.arange(5,30,5)) + [None],
                'max_features': ['sqrt','log2',None],
                'min_samples_leaf': np.arange(1,15,5),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(10,110,10)}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer, cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
368/124:
estimators = [('Random Forest',rf_tuned), ('Gradient Boosting',gbc_tuned), ('Decision Tree',dtree_estimator)]

final_estimator = xgb_tuned

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/125:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
d_tree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",d_tree_model_train_perf)
d_tree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",d_tree_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(d_tree,X_test,y_test)
368/126:
#Choose the type of classifier. 
dtree_estimator = DecisionTreeClassifier(class_weight={0:0.18,1:0.72},random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': np.arange(2,30), 
              'min_samples_leaf': [1, 2, 5, 7, 10],
              'max_leaf_nodes' : [2, 3, 5, 10,15],
              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_estimator.fit(X_train, y_train)
368/127:
#Calculating different metrics
dtree_estimator_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_estimator_model_train_perf)
dtree_estimator_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(dtree_estimator,X_test,y_test)
368/128:
#Calculating different metrics
rf_tuned_model_train_perf=model_performance_classification_sklearn(rf_tuned,X_train,y_train)
print("Training performance:\n",rf_tuned_model_train_perf)
rf_tuned_model_test_perf=model_performance_classification_sklearn(rf_tuned,X_test,y_test)
print("Testing performance:\n",rf_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_tuned,X_test,y_test)
368/129:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print(bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print(bagging_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier,X_test,y_test)
368/130:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
368/131:
#Calculating different metrics
bagging_estimator_tuned_model_train_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_train,y_train)
print(bagging_estimator_tuned_model_train_perf)
bagging_estimator_tuned_model_test_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_test,y_test)
print(bagging_estimator_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(bagging_estimator_tuned,X_test,y_test)
368/132:
estimators = [('Random Forest',rf_tuned), ('Bagging',bagging_estimator_tuned), ('Decision Tree',dtree_estimator)]

final_estimator = xgb_tuned

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/133:
estimators = [('Bagging',bagging_estimator_tuned), ('Decision Tree',dtree_estimator)]

final_estimator = rf_tuned

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/134:
#Calculating different metrics
stacking_classifier_model_train_perf=model_performance_classification_sklearn(stacking_classifier,X_train,y_train)
print("Training performance:\n",stacking_classifier_model_train_perf)
stacking_classifier_model_test_perf=model_performance_classification_sklearn(stacking_classifier,X_test,y_test)
print("Testing performance:\n",stacking_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(stacking_classifier,X_test,y_test)
368/135:
estimators = [('Bagging',bagging_classifier, ('Decision Tree',d_tree)]

final_estimator = rf_estimator

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/136:
estimators = [('Bagging',bagging_classifier), ('Decision Tree',d_tree)]

final_estimator = rf_estimator

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/137:
#Calculating different metrics
stacking_classifier_model_train_perf=model_performance_classification_sklearn(stacking_classifier,X_train,y_train)
print("Training performance:\n",stacking_classifier_model_train_perf)
stacking_classifier_model_test_perf=model_performance_classification_sklearn(stacking_classifier,X_test,y_test)
print("Testing performance:\n",stacking_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(stacking_classifier,X_test,y_test)
368/138:
#Fitting the model
ab_classifier = AdaBoostClassifier(random_state=1)
ab_classifier.fit(X_train,y_train)

#Calculating different metrics
ab_classifier_model_train_perf=model_performance_classification_sklearn(ab_classifier,X_train,y_train)
print(ab_classifier_model_train_perf)
ab_classifier_model_test_perf=model_performance_classification_sklearn(ab_classifier,X_test,y_test)
print(ab_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(ab_classifier,X_test,y_test)
368/139:
#Fitting the model
gb_classifier = GradientBoostingClassifier(random_state=1)
gb_classifier.fit(X_train,y_train)

#Calculating different metrics
gb_classifier_model_train_perf=model_performance_classification_sklearn(gb_classifier,X_train,y_train)
print("Training performance:\n",gb_classifier_model_train_perf)
gb_classifier_model_test_perf=model_performance_classification_sklearn(gb_classifier,X_test,y_test)
print("Testing performance:\n",gb_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(gb_classifier,X_test,y_test)
368/140:
#Fitting the model
xgb_classifier = XGBClassifier(random_state=1, eval_metric='logloss')
xgb_classifier.fit(X_train,y_train)

#Calculating different metrics
xgb_classifier_model_train_perf=model_performance_classification_sklearn(xgb_classifier,X_train,y_train)
print("Training performance:\n",xgb_classifier_model_train_perf)
xgb_classifier_model_test_perf=model_performance_classification_sklearn(xgb_classifier,X_test,y_test)
print("Testing performance:\n",xgb_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(xgb_classifier,X_test,y_test)
368/141:
estimators = [('Adaboost',ab_classifier), ('Gradiant Boost',gb_classifier)]

final_estimator = xgb_classifier

stacking_classifier= StackingClassifier(estimators=estimators,final_estimator=final_estimator)

stacking_classifier.fit(X_train,y_train)
368/142:
#Calculating different metrics
stacking_classifier_model_train_perf=model_performance_classification_sklearn(stacking_classifier,X_train,y_train)
print("Training performance:\n",stacking_classifier_model_train_perf)
stacking_classifier_model_test_perf=model_performance_classification_sklearn(stacking_classifier,X_test,y_test)
print("Testing performance:\n",stacking_classifier_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(stacking_classifier,X_test,y_test)
369/1:
# -*- coding: utf-8 -*-
"""
Created on Sun May  2 17:42:36 2021

@author: taka
"""
# generate 2D random distribution(uniform noise)
# array, then compute mean and standard deviation along column and row, independently.
#finally, plot with errobrbar
 
import numpy as np
import matplotlib.pyplot as plt

a = np.ceil(np.random.rand(5,7)*10)
mean_a_axis0 = np.mean(a, axis=0)
mean_a_axis1 = np.mean(a, axis=1)
std_a_axis0  = np.std(a, axis=0)
std_a_axis1  = np.std(a, axis=1)


print("data: \n",a)

print("\n")
print("mean column: \n", mean_a_axis0)
print("mean row: \n",mean_a_axis1)

print("\n")
print("standard deviation column: \n", mean_a_axis0)
print("standard deviation row: \n",mean_a_axis1)

plt.close("all")
plt.figure
plt.plot(a.T,'o')
x = np.arange(0,len(mean_a_axis0))
plt.errorbar(x, mean_a_axis0, yerr = std_a_axis0, capsize=5, fmt='o', markersize=10, ecolor='black', markeredgecolor = "black", color='w')
plt.plot(mean_a_axis0,'r--')

plt.show
370/1:
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 26 23:05:09 2021

@author: taka
"""

def function01(i):
    out = "abd-" + str(i)
    return(out)

str01 = function01(1)
print(str01)
370/2:
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 26 23:05:09 2021

@author: taka
"""

def function01(i):
    out = "abd-" + str(i)
    return()

str01 = function01(1)
print(str01)
370/3:
# -*- coding: utf-8 -*-
"""
Created on Mon Apr 26 23:05:09 2021

@author: taka
"""

def function01(i):
    out = "abd-" + str(i)
    return(out)

str01 = function01(1)
print(str01)
372/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
372/2:
# loading the dataset
data = pd.read_csv("Tourism.xlsx", sep=";")
376/1:
# loading the dataset
data = pd.read_excel('Tourism.xlsx')
376/2:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
376/3:
# loading the dataset
data = pd.read_excel('Tourism.xlsx')
376/4:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
376/5:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
376/6:
#load the head of the data
data.head()
376/7:
#load the tail of the data
data.tail()
376/8:
# let's view a sample of the data
data.sample(n=10, random_state=1)
376/9:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
376/10:
# checking column datatypes and number of non-null values
df.info()
376/11:
# checking for duplicate values
df.duplicated().sum()
376/12:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
376/13:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
376/14:
#Let's review the data information once again
df.info()
376/15:
#Let's get a detail description of the processed data
df.describe().round(2).T
378/1:
# viewing the column values
df["Mileage"].head(10)
378/2:
df_mileage = df["Mileage"].str.split(" ", expand=True)
df_mileage.head()
378/3:
# let's verify that there are two units
df_mileage[1].value_counts()
378/4:
# we will create two new columns for mileage values and units
df["mileage_num"] = df_mileage[0].astype(float)
df["mileage_unit"] = df_mileage[1]

# Checking the new dataframe
df.head()
378/5:
# Let's check if the units correspond to the fuel types
df.groupby(by=["Fuel_Type", "mileage_unit"]).size()
378/6:
# viewing the column values
df["Engine"].head(10)
378/7:
df_engine = df["Engine"].str.split(" ", expand=True)
df_engine.head()
378/8:
# let's verify that there is only one unit
df_engine[1].value_counts()
378/9:
# we will create a new column for engine values
df["engine_num"] = df_engine[0].astype(float)

# Checking the new dataframe
df.head()
378/10:
# viewing the column values
df["Power"].head(10)
378/11:
df_power = df["Power"].str.split(" ", expand=True)
df_power.head()
378/12:
# let's verify that there is only one unit
df_power[1].value_counts()
378/13:
# we will create a new column for power values
df["power_num"] = df_power[0].astype(float)

# Checking the new dataframe
df.head()
378/14:
# viewing the column values
df["New_Price"].head(10)
378/15:
df_new_price = df["New_Price"].str.split(" ", expand=True)
df_new_price.head()
378/16:
# let's verify that there is only one unit
df_new_price[1].value_counts()
378/17:
#Let's compute the missing values
df[df["Seats"].isnull()]
378/18: df.groupby(["Brand", "Model"], as_index=False)["Seats"].median()
378/19:
# imputing missing values in Seats
df["Seats"] = df.groupby(["Brand", "Model"])["Seats"].transform(
    lambda x: x.fillna(x.median())
)
378/20:
# Checking missing values in Seats
df[df["Seats"].isnull()]
378/21: df["Seats"] = df["Seats"].fillna(5.0)
378/22:
cols_list = ["mileage_num", "engine_num", "power_num", "new_price_num"]

for col in cols_list:
    df[col] = df.groupby(["Brand", "Model"])[col].transform(
        lambda x: x.fillna(x.median())
    )

df.isnull().sum()
378/23:
cols_list = ["mileage_num", "power_num", "new_price_num"]

for col in cols_list:
    df[col] = df.groupby(["Brand"])[col].transform(lambda x: x.fillna(x.median()))

df.isnull().sum()
378/24:
cols_list = ["power_num", "new_price_num"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
378/25: df = df[df["Price"].notna()]
378/26: df["Age"] = df["Age"].astype(float)
378/27:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
378/28:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
378/29:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
378/30:
#load the head of the data
data.head()
378/31:
#load the tail of the data
data.tail()
378/32:
# let's view a sample of the data
data.sample(n=10, random_state=1)
378/33:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
378/34:
# checking column datatypes and number of non-null values
df.info()
378/35:
# checking for duplicate values
df.duplicated().sum()
378/36:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
378/37:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
378/38: df["Age"] = df["Age"].astype(float)
378/39:
#Let's review the data information once again
df.info()
378/40: df['Age'] = df['Age'].astype(int)
378/41: df["Age"] = df["Age"].astype(int)
378/42:

# displaying the datatypes
display(df.dtypes)
  
# converting 'Weight' and 'Salary' from float to int
df = df.astype({"Age":'int', "DurationOfPitch":'int'}) 
  
# displaying the datatypes
display(df.dtypes)
378/43:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
378/44:

# displaying the datatypes
display(df.dtypes)
  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df[colname] = df[colname].astype('category')
    
df.info()
378/45:

# displaying the datatypes
display(df.dtypes)
  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
378/46:
# checking column datatypes and number of non-null values
df.info()
385/1:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
385/2:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
385/3:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
385/4:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
385/5:
#load the head of the data
data.head()
385/6:
#load the tail of the data
data.tail()
385/7:
# let's view a sample of the data
data.sample(n=10, random_state=1)
385/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
385/9:
# checking for duplicate values
df.duplicated().sum()
385/10:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
385/11:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
385/12:

# displaying the datatypes
display(df.dtypes)
  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
385/13:
# checking column datatypes and number of non-null values
df.info()
385/14:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
385/15:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
385/16: histogram_boxplot(df, "Age", kde=True)
385/17:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome", "colname"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
385/18:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
385/19:

  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
385/20:
df.drop['colname']
df
385/21:
df = df.drop['colname']
df
385/22:
# drop the ID column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
385/23:
# drop the ID column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
df
385/24:
# drop the ID column as it does not add any value to the analysis

df
385/25:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
385/26:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
385/27:
# drop the ID column as it does not add any value to the analysis

df
385/28: histogram_boxplot(df, "DurationOfPitch", kde=True)DurationOfPitch
385/29: histogram_boxplot(df, "DurationOfPitch", kde=True)
385/30: histogram_boxplot(df, "NumberOfPersonVisiting", kde=True)
385/31: histogram_boxplot(df, "MonthlyIncome", kde=True)
385/32:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
385/33: labeled_barplot(df, "Age", perc=False, n=10)
385/34: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
385/35: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
385/36: labeled_barplot(df, "CityTier", perc=False, n=10)
385/37: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
385/38: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
385/39: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
385/40: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
385/41: labeled_barplot(df, "Passport   ", perc=False, n=10)
385/42: labeled_barplot(df, "Passport", perc=False, n=10)
385/43: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
385/44: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
385/45: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
385/46:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()
385/47:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
385/48:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
385/49:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
385/50:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
385/51:
#load the head of the data
data.head()
385/52:
#load the tail of the data
data.tail()
385/53:
# let's view a sample of the data
data.sample(n=10, random_state=1)
385/54:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
385/55:
# checking for duplicate values
df.duplicated().sum()
385/56:

  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
385/57:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
385/58:
# drop the ID column as it does not add any value to the analysis

df
385/59:
# checking column datatypes and number of non-null values
df.info()
385/60:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
385/61:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
385/62:
#Let's get a detail description of the processed data
df.describe().round(2).T
385/63:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
385/64: histogram_boxplot(df, "Age", kde=True)
385/65: histogram_boxplot(df, "DurationOfPitch", kde=True)
385/66: histogram_boxplot(df, "MonthlyIncome", kde=True)
385/67:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
385/68: labeled_barplot(df, "Age", perc=False, n=10)
385/69: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
385/70: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
385/71: labeled_barplot(df, "CityTier", perc=False, n=10)
385/72: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
385/73: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
385/74: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
385/75: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
385/76: labeled_barplot(df, "Passport", perc=False, n=10)
385/77: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
385/78: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
385/79: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
385/80:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
385/81:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
385/82:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
385/83: display(df.dtypes)
385/84: df.isnull.sum()
385/85: df.isnull.sum(axis=1)
385/86: df.isnull().sum()
385/87:
# drop the ID column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
385/88: df
385/89: df.info
385/90: df.info()
385/91: df.isnull.sum()
385/92: df.isnull().sum()
386/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
386/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
386/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
386/4:
#load the head of the data
data.head()
386/5:
#load the tail of the data
data.tail()
386/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
386/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
386/8:
# checking for duplicate values
df.duplicated().sum()
386/9:

  
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']
# the other categorical variables have lots of levels
# and I wouldn't dummy encode them as such

for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
386/10:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
386/11:
# checking column datatypes and number of non-null values
df.info()
386/12:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
386/13: df
386/14:
# most rows don't have missing values now
num_missing = df.isnull().sum(axis=1)
num_missing.value_counts()
386/15:
#Let's get a detail description of the processed data
df.describe().round(2).T
386/16:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
386/17: histogram_boxplot(df, "Age", kde=True)
386/18: histogram_boxplot(df, "DurationOfPitch", kde=True)
386/19: histogram_boxplot(df, "MonthlyIncome", kde=True)
386/20:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
386/21: labeled_barplot(df, "Age", perc=False, n=10)
386/22: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
386/23: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
386/24: labeled_barplot(df, "CityTier", perc=False, n=10)
386/25: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
386/26: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
386/27: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
386/28: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
386/29: labeled_barplot(df, "Passport", perc=False, n=10)
386/30: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
386/31: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
386/32: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
386/33:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
386/34:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
386/35:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
386/36: display(df.dtypes)
386/37: df.isnull().sum()
386/38:
# drop the ID column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
386/39: df.info()
386/40: df.isnull().sum()
386/41:
sns.pairplot(data=data,hue="Class")
plt.show()
386/42:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
386/43: boxplot('NumberOfPersonVisiting')
386/44: data.columns
386/45:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="Class",y=data[x],palette="PuBu")
    plt.show()
386/46: boxplot('Age')
386/47: boxplot('DurationOfPitch')
386/48:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
386/49: boxplot('Age')
386/50: boxplot('DurationOfPitch')
386/51: boxplot('CityTier')
386/52: boxplot('NumberOfPersonVisiting')
386/53: boxplot('Pregnancies')
386/54: boxplot('NumberOfFollowups')
386/55: boxplot('PreferredPropertyStar')
386/56: boxplot('NumberOfTrips')
386/57: boxplot('Passport')
386/58: boxplot('PitchSatisfactionScore')
386/59: boxplot('NumberOfChildrenVisiting')
386/60: boxplot('MonthlyIncome')
389/1: To predict which customer is more likely to purchase the newly introduced travel package.
389/2: data["CustomerID"].nunique()
389/3:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
389/4:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
389/5:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
389/6:
#load the head of the data
data.head()
389/7:
#load the tail of the data
data.tail()
389/8:
# let's view a sample of the data
data.sample(n=10, random_state=1)
389/9: data["CustomerID"].nunique()
389/10: df.info()
389/11:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
389/12:
# checking for duplicate values
df.duplicated().sum()
389/13: df.info()
389/14:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
389/15:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
389/16: labeled_barplot(df, "Age", perc=False)
389/17: labeled_barplot(df, "Age", perc=False, n=20)
389/18: labeled_barplot(df, "Age", perc=False, n=25)
389/19: labeled_barplot(df, "Age", perc=False, n=20)
389/20: labeled_barplot(df, "MonthlyIncome", perc=False, n=30)
389/21: labeled_barplot(df, "MonthlyIncome", perc=False, n=10)
389/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
389/23: ### Observation:
389/24:
#let's get all the name of the columns
data.columns
391/1:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
391/2:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
391/3:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
391/4:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
391/5:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
391/6:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
391/7:
#load the head of the data
data.head()
391/8:
#load the tail of the data
data.tail()
391/9:
# let's view a sample of the data
data.sample(n=10, random_state=1)
391/10: data["CustomerID"].nunique()
391/11:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
391/12: df.info()
391/13:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
391/14:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
391/15: y.value_counts(1)
391/16: y_test.value_counts(1)
391/17:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
391/18:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
391/19:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
391/20:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
391/21:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
391/22:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
395/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
395/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
395/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
395/4:
#load the head of the data
data.head()
395/5:
#load the tail of the data
data.tail()
395/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
395/7: data["CustomerID"].nunique()
395/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
395/9: df.info()
395/10:
# checking for duplicate values
df.duplicated().sum()
395/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
395/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
395/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
395/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
395/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
395/16: histogram_boxplot(df, "Age", kde=True)
395/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
395/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
395/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
395/20: labeled_barplot(df, "Age", perc=False, n=20)
395/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
395/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
395/23: labeled_barplot(df, "CityTier", perc=False, n=10)
395/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
395/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
395/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
395/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
395/28: labeled_barplot(df, "Passport", perc=False, n=10)
395/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
395/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
395/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
395/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
395/33:
#let's get all the name of the columns
data.columns
395/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
395/35: boxplot('Age')
395/36: boxplot('DurationOfPitch')
395/37: boxplot('CityTier')
395/38: boxplot('NumberOfFollowups')
395/39: boxplot('MonthlyIncome')
395/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
395/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
395/42: display(df.dtypes)
395/43: df.isnull().sum()
395/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
395/45: df.info()
395/46: df.isnull().sum()
395/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
395/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
395/49: y.value_counts(1)
395/50: y_test.value_counts(1)
395/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
395/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
395/53:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
395/54:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
395/55:
* Random forest is overfitting the training data as there is a huge difference between training and test scores for all the metrics.
* The test recall is even lower than the decision tree but has a higher test precision.
395/56:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
395/57:
#Choose the type of classifier. 
dtree_estimator = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': np.arange(2,10), 
              'min_samples_leaf': [5, 7, 10, 15],
              'max_leaf_nodes' : [2, 3, 5, 10,15],
              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_estimator.fit(X_train, y_train)
395/58:
#Calculating different metrics
dtree_estimator_model_train_perf=model_performance_classification_sklearn(dtree_estimator,X_train,y_train)
print("Training performance:\n",dtree_estimator_model_train_perf)
dtree_estimator_model_test_perf=model_performance_classification_sklearn(dtree_estimator,X_test,y_test)
print("Testing performance:\n",dtree_estimator_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(dtree_estimator, X_test, y_test)
395/59:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
398/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
398/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
398/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
398/4:
#load the head of the data
data.head()
398/5:
#load the tail of the data
data.tail()
398/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
398/7: data["CustomerID"].nunique()
398/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
398/9: df.info()
398/10:
# checking for duplicate values
df.duplicated().sum()
398/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
398/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
398/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
398/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
398/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
398/16: histogram_boxplot(df, "Age", kde=True)
398/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
398/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
398/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
398/20: labeled_barplot(df, "Age", perc=False, n=20)
398/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
398/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
398/23: labeled_barplot(df, "CityTier", perc=False, n=10)
398/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
398/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
398/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
398/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
398/28: labeled_barplot(df, "Passport", perc=False, n=10)
398/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
398/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
398/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
398/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
398/33:
#let's get all the name of the columns
data.columns
398/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
398/35: boxplot('Age')
398/36: boxplot('DurationOfPitch')
398/37: boxplot('CityTier')
398/38: boxplot('NumberOfFollowups')
398/39: boxplot('MonthlyIncome')
398/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
398/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
398/42: display(df.dtypes)
398/43: df.isnull().sum()
398/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
398/45: df.info()
398/46: df.isnull().sum()
398/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
398/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
398/49: y.value_counts(1)
398/50: y_test.value_counts(1)
398/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
398/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
398/53:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
398/54:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
398/55:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
398/56:
#Choose the type of classifier. 
dtree_estimator = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': np.arange(2,10), 
              'min_samples_leaf': [5, 7, 10, 15],
              'max_leaf_nodes' : [2, 3, 5, 10,15],
              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_estimator.fit(X_train, y_train)
398/57:
#Calculating different metrics
dtree_estimator_model_train_perf=model_performance_classification_sklearn(dtree_estimator,X_train,y_train)
print("Training performance:\n",dtree_estimator_model_train_perf)
dtree_estimator_model_test_perf=model_performance_classification_sklearn(dtree_estimator,X_test,y_test)
print("Testing performance:\n",dtree_estimator_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(dtree_estimator, X_test, y_test)
398/58:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
400/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV
400/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
400/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
400/4:
#load the head of the data
data.head()
400/5:
#load the tail of the data
data.tail()
400/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
400/7: data["CustomerID"].nunique()
400/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
400/9: df.info()
400/10:
# checking for duplicate values
df.duplicated().sum()
400/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
400/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
400/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
400/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
400/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
400/16: histogram_boxplot(df, "Age", kde=True)
400/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
400/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
400/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
400/20: labeled_barplot(df, "Age", perc=False, n=20)
400/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
400/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
400/23: labeled_barplot(df, "CityTier", perc=False, n=10)
400/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
400/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
400/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
400/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
400/28: labeled_barplot(df, "Passport", perc=False, n=10)
400/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
400/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
400/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
400/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
400/33:
#let's get all the name of the columns
data.columns
400/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
400/35: boxplot('Age')
400/36: boxplot('DurationOfPitch')
400/37: boxplot('CityTier')
400/38: boxplot('NumberOfFollowups')
400/39: boxplot('MonthlyIncome')
400/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
400/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
400/42: display(df.dtypes)
400/43: df.isnull().sum()
400/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
400/45: df.info()
400/46: df.isnull().sum()
400/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
400/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
400/49: y.value_counts(1)
400/50: y_test.value_counts(1)
400/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
400/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
400/53:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
400/54:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
400/55:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
400/56:
#Choose the type of classifier. 
dtree_estimator = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': np.arange(2,10), 
              'min_samples_leaf': [5, 7, 10, 15],
              'max_leaf_nodes' : [2, 3, 5, 10,15],
              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_estimator.fit(X_train, y_train)
400/57:
#Calculating different metrics
dtree_estimator_model_train_perf=model_performance_classification_sklearn(dtree_estimator,X_train,y_train)
print("Training performance:\n",dtree_estimator_model_train_perf)
dtree_estimator_model_test_perf=model_performance_classification_sklearn(dtree_estimator,X_test,y_test)
print("Testing performance:\n",dtree_estimator_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(dtree_estimator, X_test, y_test)
400/58:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
400/59:
#Calculating different metrics
rf_tuned_model_train_perf=model_performance_classification_sklearn(rf_tuned,X_train,y_train)
print("Training performance:\n",rf_tuned_model_train_perf)
rf_tuned_model_test_perf=model_performance_classification_sklearn(rf_tuned,X_test,y_test)
print("Testing performance:\n",rf_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_tuned, X_test, y_test)
400/60:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
400/61:
#Calculating different metrics
bagging_estimator_tuned_model_train_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_train,y_train)
print("Training performance:\n",bagging_estimator_tuned_model_train_perf)
bagging_estimator_tuned_model_test_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_test,y_test)
print("Testing performance:\n",bagging_estimator_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(bagging_estimator_tuned, X_test, y_test)
400/62:
# training performance comparison

models_train_comp_df = pd.concat(
    [dtree_model_train_perf.T,dtree_estimator_model_train_perf.T,rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
     bagging_classifier_model_train_perf.T,bagging_estimator_tuned_model_train_perf.T],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Training performance comparison:")
models_train_comp_df
400/63:
# testing performance comparison

models_test_comp_df = pd.concat(
    [dtree_model_test_perf.T,dtree_estimator_model_test_perf.T,rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,
     bagging_classifier_model_test_perf.T, bagging_estimator_tuned_model_test_perf.T],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Testing performance comparison:")
models_test_comp_df
400/64:
# Text report showing the rules of a decision tree -
feature_names = list(X_train.columns)
print(tree.export_text(dtree_estimator,feature_names=feature_names,show_weights=True))
400/65:
feature_names = X_train.columns
importances = dtree_estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
400/66:
ab_regressor=AdaBoostRegressor(random_state=1)
ab_regressor.fit(X_train,y_train)
396/1:
import warnings
warnings.filterwarnings("ignore")

import numpy as np   
import pandas as pd    
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
400/67:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split





# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
400/68:
ab_regressor=AdaBoostRegressor(random_state=1)
ab_regressor.fit(X_train,y_train)
400/69: ab_regressor_score=get_model_score(ab_regressor)
400/70:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
400/71: ab_regressor_score=get_model_score(ab_regressor)
400/72:
# Choose the type of classifier. 
ab_tuned = AdaBoostRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': np.arange(10,100,10), 
              'learning_rate': [1, 0.1, 0.5, 0.01],
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(ab_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
ab_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
ab_tuned.fit(X_train, y_train)
400/73: ab_tuned_score=get_model_score(ab_tuned)
400/74:
feature_names = X_train.columns
importances = ab_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
400/75:
gb_estimator=GradientBoostingRegressor(random_state=1)
gb_estimator.fit(X_train,y_train)
400/76: gb_estimator_score=get_model_score(gb_estimator)
400/77: - Gradient boosting is generalizing well and giving decent results but not as good as random forest.
400/78:
# Choose the type of classifier. 
gb_tuned = GradientBoostingRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': np.arange(50,200,25), 
              'subsample':[0.7,0.8,0.9,1],
              'max_features':[0.7,0.8,0.9,1],
              'max_depth':[3,5,7,10]
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(gb_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gb_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
gb_tuned.fit(X_train, y_train)
401/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split





# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
401/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
401/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
401/4:
#load the head of the data
data.head()
401/5:
#load the tail of the data
data.tail()
401/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
401/7: data["CustomerID"].nunique()
401/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
401/9: df.info()
401/10:
# checking for duplicate values
df.duplicated().sum()
401/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
401/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
401/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
401/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
401/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
401/16: histogram_boxplot(df, "Age", kde=True)
401/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
401/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
401/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
401/20: labeled_barplot(df, "Age", perc=False, n=20)
401/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
401/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
401/23: labeled_barplot(df, "CityTier", perc=False, n=10)
401/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
401/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
401/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
401/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
401/28: labeled_barplot(df, "Passport", perc=False, n=10)
401/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
401/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
401/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
401/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
401/33:
#let's get all the name of the columns
data.columns
401/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
401/35: boxplot('Age')
401/36: boxplot('DurationOfPitch')
401/37: boxplot('CityTier')
401/38: boxplot('NumberOfFollowups')
401/39: boxplot('MonthlyIncome')
401/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
401/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
401/42: display(df.dtypes)
401/43: df.isnull().sum()
401/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
401/45: df.info()
401/46: df.isnull().sum()
401/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
401/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
401/49: y.value_counts(1)
401/50: y_test.value_counts(1)
401/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
401/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
401/53:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
401/54:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
401/55:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
401/56:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split





# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
401/57:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
401/58:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
403/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split





# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
403/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
403/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
403/4:
#load the head of the data
data.head()
403/5:
#load the tail of the data
data.tail()
403/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
403/7: data["CustomerID"].nunique()
403/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
403/9: df.info()
403/10:
# checking for duplicate values
df.duplicated().sum()
403/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
403/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
403/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
403/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
403/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
403/16: histogram_boxplot(df, "Age", kde=True)
403/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
403/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
403/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
403/20: labeled_barplot(df, "Age", perc=False, n=20)
403/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
403/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
403/23: labeled_barplot(df, "CityTier", perc=False, n=10)
403/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
403/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
403/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
403/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
403/28: labeled_barplot(df, "Passport", perc=False, n=10)
403/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
403/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
403/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
403/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
403/33:
#let's get all the name of the columns
data.columns
403/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
403/35: boxplot('Age')
403/36: boxplot('DurationOfPitch')
403/37: boxplot('CityTier')
403/38: boxplot('NumberOfFollowups')
403/39: boxplot('MonthlyIncome')
403/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
403/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
403/42: display(df.dtypes)
403/43: df.isnull().sum()
403/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
403/45: df.info()
403/46: df.isnull().sum()
403/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
403/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
403/49: y.value_counts(1)
403/50: y_test.value_counts(1)
403/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
403/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
403/53:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
403/54:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
403/55:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
403/56:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV



# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
403/57:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
404/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV



# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
404/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
404/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
404/4:
#load the head of the data
data.head()
404/5:
#load the tail of the data
data.tail()
404/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
404/7: data["CustomerID"].nunique()
404/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
404/9: df.info()
404/10:
# checking for duplicate values
df.duplicated().sum()
404/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
404/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
404/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
404/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
404/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
404/16: histogram_boxplot(df, "Age", kde=True)
404/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
404/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
404/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
404/20: labeled_barplot(df, "Age", perc=False, n=20)
404/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
404/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
404/23: labeled_barplot(df, "CityTier", perc=False, n=10)
404/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
404/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
404/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
404/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
404/28: labeled_barplot(df, "Passport", perc=False, n=10)
404/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
404/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
404/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
404/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
404/33:
#let's get all the name of the columns
data.columns
404/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
404/35: boxplot('Age')
404/36: boxplot('DurationOfPitch')
404/37: boxplot('CityTier')
404/38: boxplot('NumberOfFollowups')
404/39: boxplot('MonthlyIncome')
404/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
404/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
404/42: display(df.dtypes)
404/43: df.isnull().sum()
404/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
404/45: df.info()
404/46: df.isnull().sum()
404/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
404/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
404/49: y.value_counts(1)
404/50: y_test.value_counts(1)
404/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
404/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
404/53:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
404/54:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
404/55:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
404/56:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
404/57:
#Choose the type of classifier. 
dtree_estimator = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': np.arange(2,10), 
              'min_samples_leaf': [5, 7, 10, 15],
              'max_leaf_nodes' : [2, 3, 5, 10,15],
              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_estimator.fit(X_train, y_train)
404/58:
#Calculating different metrics
dtree_estimator_model_train_perf=model_performance_classification_sklearn(dtree_estimator,X_train,y_train)
print("Training performance:\n",dtree_estimator_model_train_perf)
dtree_estimator_model_test_perf=model_performance_classification_sklearn(dtree_estimator,X_test,y_test)
print("Testing performance:\n",dtree_estimator_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(dtree_estimator, X_test, y_test)
404/59:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
404/60:
#Calculating different metrics
rf_tuned_model_train_perf=model_performance_classification_sklearn(rf_tuned,X_train,y_train)
print("Training performance:\n",rf_tuned_model_train_perf)
rf_tuned_model_test_perf=model_performance_classification_sklearn(rf_tuned,X_test,y_test)
print("Testing performance:\n",rf_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_tuned, X_test, y_test)
404/61:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
404/62:
#Calculating different metrics
bagging_estimator_tuned_model_train_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_train,y_train)
print("Training performance:\n",bagging_estimator_tuned_model_train_perf)
bagging_estimator_tuned_model_test_perf=model_performance_classification_sklearn(bagging_estimator_tuned,X_test,y_test)
print("Testing performance:\n",bagging_estimator_tuned_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(bagging_estimator_tuned, X_test, y_test)
404/63:
# training performance comparison

models_train_comp_df = pd.concat(
    [dtree_model_train_perf.T,dtree_estimator_model_train_perf.T,rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
     bagging_classifier_model_train_perf.T,bagging_estimator_tuned_model_train_perf.T],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Training performance comparison:")
models_train_comp_df
404/64:
# testing performance comparison

models_test_comp_df = pd.concat(
    [dtree_model_test_perf.T,dtree_estimator_model_test_perf.T,rf_estimator_model_test_perf.T,rf_tuned_model_test_perf.T,
     bagging_classifier_model_test_perf.T, bagging_estimator_tuned_model_test_perf.T],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Testing performance comparison:")
models_test_comp_df
404/65:
# Text report showing the rules of a decision tree -
feature_names = list(X_train.columns)
print(tree.export_text(dtree_estimator,feature_names=feature_names,show_weights=True))
404/66:
feature_names = X_train.columns
importances = dtree_estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
404/67:
ab_regressor=AdaBoostRegressor(random_state=1)
ab_regressor.fit(X_train,y_train)
404/68: ab_regressor_score=get_model_score(ab_regressor)
404/69:
# Choose the type of classifier. 
ab_tuned = AdaBoostRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': np.arange(10,100,10), 
              'learning_rate': [1, 0.1, 0.5, 0.01],
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(ab_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
ab_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
ab_tuned.fit(X_train, y_train)
404/70: ab_tuned_score=get_model_score(ab_tuned)
404/71:
feature_names = X_train.columns
importances = ab_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
404/72:
gb_estimator=GradientBoostingRegressor(random_state=1)
gb_estimator.fit(X_train,y_train)
404/73: gb_estimator_score=get_model_score(gb_estimator)
404/74:
# Choose the type of classifier. 
gb_tuned = GradientBoostingRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': np.arange(50,200,25), 
              'subsample':[0.7,0.8,0.9,1],
              'max_features':[0.7,0.8,0.9,1],
              'max_depth':[3,5,7,10]
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(gb_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
gb_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
gb_tuned.fit(X_train, y_train)
404/75: gb_tuned_score=get_model_score(gb_tuned)
404/76:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print(pd.DataFrame(gb_tuned.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
404/77:
feature_names = X_train.columns
importances = gb_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
404/78: !pip install xgboost
404/79:
xgb_estimator=XGBRegressor(random_state=1)
xgb_estimator.fit(X_train,y_train)
404/80: xgb_estimator_score=get_model_score(xgb_estimator)
404/81:
# Choose the type of classifier. 
xgb_tuned = XGBRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': [75,100,125,150], 
              'subsample':[0.7, 0.8, 0.9, 1],
              'gamma':[0, 1, 3, 5],
              'colsample_bytree':[0.7, 0.8, 0.9, 1],
              'colsample_bylevel':[0.7, 0.8, 0.9, 1]
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(xgb_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
xgb_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
xgb_tuned.fit(X_train, y_train)
404/82: xgb_tuned_score=get_model_score(xgb_tuned)
404/83:
# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )

print(pd.DataFrame(xgb_tuned.feature_importances_, columns = ["Imp"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))
404/84:
feature_names = X_train.columns
importances = xgb_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
404/85:
estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),
           ('Gradient Boosting', gb_tuned)]
final_estimator=XGBRegressor(random_state=1)
404/86:
estimators=[('Decision Tree', dtree_estimator),('Random Forest', rf_tuned),
           ('Gradient Boosting', gb_tuned)]
final_estimator=XGBRegressor(random_state=1)
404/87:
stacking_estimator=StackingRegressor(estimators=estimators, final_estimator=final_estimator,cv=5)
stacking_estimator.fit(X_train,y_train)
404/88:
# Choose the type of classifier. 
dtree_tuned = DecisionTreeRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': list(np.arange(2,20)) + [None], 
              'min_samples_leaf': [1, 3, 5, 7, 10],
              'max_leaf_nodes' : [2, 3, 5, 10, 15] + [None],
              'min_impurity_decrease': [0.001, 0.01, 0.1, 0.0]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_tuned.fit(X_train, y_train)
404/89: dtree_tuned_score=get_model_score(dtree_tuned)
404/90:
estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),
           ('Gradient Boosting', gb_tuned)]
final_estimator=XGBRegressor(random_state=1)
404/91:
stacking_estimator=StackingRegressor(estimators=estimators, final_estimator=final_estimator,cv=5)
stacking_estimator.fit(X_train,y_train)
404/92:
estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),
           ('Gradient Boosting', gb_tuned)]
final_estimator=XGBRegressor(random_state=1)
404/93:
stacking_estimator=StackingRegressor(estimators=estimators, final_estimator=final_estimator,cv=5)
stacking_estimator.fit(X_train,y_train)
404/94: stacking_estimator_score=get_model_score(stacking_estimator)
404/95:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
404/96: rf_tuned_score=get_model_score(rf_tuned)
404/97: bagging_tuned_score=get_model_score(bagging_estimator_tuned )
404/98:
# training performance comparison

models_train_comp_df = pd.concat(
    [dtree_model_train_perf.T,dtree_estimator_model_train_perf.T,rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
     bagging_classifier_model_train_perf.T,bagging_estimator_tuned_model_train_perf.T],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Training performance comparison:")
models_train_comp_df
404/99:
estimators=[('Decision Tree', dtree_tuned),('Random Forest', rf_tuned),
           ('Gradient Boosting', gb_tuned)]
final_estimator=XGBRegressor(random_state=1)
404/100:
stacking_estimator=StackingRegressor(estimators=estimators, final_estimator=final_estimator,cv=5)
stacking_estimator.fit(X_train,y_train)
405/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV



# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
405/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
405/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
405/4:
#load the head of the data
data.head()
405/5:
#load the tail of the data
data.tail()
405/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
405/7: data["CustomerID"].nunique()
405/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
405/9: df.info()
405/10:
# checking for duplicate values
df.duplicated().sum()
405/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
405/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
405/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
405/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
405/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
405/16: histogram_boxplot(df, "Age", kde=True)
405/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
405/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
405/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
405/20: labeled_barplot(df, "Age", perc=False, n=20)
405/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
405/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
405/23: labeled_barplot(df, "CityTier", perc=False, n=10)
405/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
405/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
405/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
405/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
405/28: labeled_barplot(df, "Passport", perc=False, n=10)
405/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
405/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
405/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
405/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
405/33:
#let's get all the name of the columns
data.columns
405/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
405/35: boxplot('Age')
405/36: boxplot('DurationOfPitch')
405/37: boxplot('CityTier')
405/38: boxplot('NumberOfFollowups')
405/39: boxplot('MonthlyIncome')
405/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
405/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
405/42: display(df.dtypes)
405/43: df.isnull().sum()
405/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
405/45: df.info()
405/46: df.isnull().sum()
405/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
405/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
405/49: y.value_counts(1)
405/50: y_test.value_counts(1)
405/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
405/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
405/53:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
405/54:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
405/55:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
405/56:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
405/57:
# Choose the type of classifier. 
dtree_tuned = DecisionTreeRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': list(np.arange(2,20)) + [None], 
              'min_samples_leaf': [1, 3, 5, 7, 10],
              'max_leaf_nodes' : [2, 3, 5, 10, 15] + [None],
              'min_impurity_decrease': [0.001, 0.01, 0.1, 0.0]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_tuned.fit(X_train, y_train)
405/58:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
405/59:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
405/60: rf_tuned_score=get_model_score(rf_tuned)
405/61:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
405/62: bagging_tuned_score=get_model_score(bagging_estimator_tuned )
405/63:
# training performance comparison

models_train_comp_df = pd.concat(
    [dtree_model_train_perf.T,dtree_estimator_model_train_perf.T,rf_estimator_model_train_perf.T,rf_tuned_model_train_perf.T,
     bagging_classifier_model_train_perf.T,bagging_estimator_tuned_model_train_perf.T],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Bagging Classifier",
    "Bagging Estimator Tuned"]
print("Training performance comparison:")
models_train_comp_df
406/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV



# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV


# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split



from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
406/2:
# loading the dataset
data = pd.read_excel('Tourism.xlsx', sheet_name='Tourism')
406/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
406/4:
#load the head of the data
data.head()
406/5:
#load the tail of the data
data.tail()
406/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
406/7: data["CustomerID"].nunique()
406/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
406/9: df.info()
406/10:
# checking for duplicate values
df.duplicated().sum()
406/11:
cat_vars = ['Age', 'DurationOfPitch', 'NumberOfFollowups',
            'PreferredPropertyStar', 'NumberOfTrips', 
            'NumberOfChildrenVisiting','MonthlyIncome']


for colname in cat_vars:
    df['colname'] = df[colname].astype('category')
    
df.info()
406/12:
cols_list = ["Age", "DurationOfPitch", "NumberOfFollowups", "PreferredPropertyStar", "NumberOfTrips",
            "NumberOfChildrenVisiting", "MonthlyIncome"]

for col in cols_list:
    df[col] = df[col].fillna(df[col].median())

df.isnull().sum()
406/13:
# counting the number of missing values per row
df.isnull().sum(axis=1).value_counts()
406/14:
#Let's get a detail description of the processed data
df.describe().round(2).T
406/15:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
406/16: histogram_boxplot(df, "Age", kde=True)
406/17: histogram_boxplot(df, "DurationOfPitch", kde=True)
406/18: histogram_boxplot(df, "MonthlyIncome", kde=True)
406/19:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
406/20: labeled_barplot(df, "Age", perc=False, n=20)
406/21: labeled_barplot(df, "DurationOfPitch", perc=False, n=10)
406/22: labeled_barplot(df, "MonthlyIncome", perc=False, n=20)
406/23: labeled_barplot(df, "CityTier", perc=False, n=10)
406/24: labeled_barplot(df, "NumberOfPersonVisiting", perc=False, n=10)
406/25: labeled_barplot(df, "NumberOfFollowups", perc=False, n=10)
406/26: labeled_barplot(df, "PreferredPropertyStar", perc=False, n=10)
406/27: labeled_barplot(df, "NumberOfTrips", perc=False, n=10)
406/28: labeled_barplot(df, "Passport", perc=False, n=10)
406/29: labeled_barplot(df, "PitchSatisfactionScore", perc=False, n=10)
406/30: labeled_barplot(df, "NumberOfChildrenVisiting", perc=False, n=10)
406/31:
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1f", cmap="Spectral")
plt.show()
406/32:
sns.pairplot(data=data,hue="ProdTaken")
plt.show()
406/33:
#let's get all the name of the columns
data.columns
406/34:
### Function to plot boxplot
def boxplot(x):
    plt.figure(figsize=(10,7))
    sns.boxplot(data=data, x="ProdTaken",y=data[x],palette="PuBu")
    plt.show()
406/35: boxplot('Age')
406/36: boxplot('DurationOfPitch')
406/37: boxplot('CityTier')
406/38: boxplot('NumberOfFollowups')
406/39: boxplot('MonthlyIncome')
406/40:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
406/41:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
406/42: display(df.dtypes)
406/43: df.isnull().sum()
406/44:
# drop the colname column as it does not add any value to the analysis
df.drop("colname", axis=1, inplace=True)
406/45: df.info()
406/46: df.isnull().sum()
406/47:
X = df.drop('ProdTaken',axis=1)
y = df['ProdTaken']
406/48:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
406/49: y.value_counts(1)
406/50: y_test.value_counts(1)
406/51:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
406/52:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
406/53:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
406/54:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
406/55:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
406/56:
#Fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,y_train)

#Calculating different metrics
bagging_classifier_model_train_perf=model_performance_classification_sklearn(bagging_classifier,X_train,y_train)
print("Training performance:\n",bagging_classifier_model_train_perf)
bagging_classifier_model_test_perf=model_performance_classification_sklearn(bagging_classifier,X_test,y_test)
print("Testing performance:\n",bagging_classifier_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(bagging_classifier, X_test, y_test)
406/57:
# Choose the type of classifier. 
dtree_tuned = DecisionTreeRegressor(random_state=1)

# Grid of parameters to choose from
parameters = {'max_depth': list(np.arange(2,20)) + [None], 
              'min_samples_leaf': [1, 3, 5, 7, 10],
              'max_leaf_nodes' : [2, 3, 5, 10, 15] + [None],
              'min_impurity_decrease': [0.001, 0.01, 0.1, 0.0]
             }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.r2_score)

# Run the grid search
grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
dtree_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
dtree_tuned.fit(X_train, y_train)
406/58:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
406/59:
# Choose the type of classifier. 
rf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)

parameters = {  
                'max_depth': list(np.arange(3,10,1)),
                'max_features': np.arange(0.6,1.1,0.1),
                'max_samples': np.arange(0.7,1.1,0.1),
                'min_samples_split': np.arange(2, 20, 5),
                'n_estimators': np.arange(30,160,20),
                'min_impurity_decrease': [0.0001,0.001,0.01,0.1]
}


# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data. 
rf_tuned.fit(X_train, y_train)
406/60: rf_tuned_score=get_model_score(rf_tuned)
406/61:
# Choose the type of classifier. 
bagging_estimator_tuned = BaggingClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'max_samples': [0.7,0.8,0.9,1], 
              'max_features': [0.7,0.8,0.9,1],
              'n_estimators' : [10,20,30,40,50],
             }

# Type of scoring used to compare parameter combinations
acc_scorer = metrics.make_scorer(metrics.recall_score)

# Run the grid search
grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
bagging_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
bagging_estimator_tuned.fit(X_train, y_train)
406/62: bagging_tuned_score=get_model_score(bagging_estimator_tuned)
406/63:
# Text report showing the rules of a decision tree -
feature_names = list(X_train.columns)
print(tree.export_text(dtree_estimator,feature_names=feature_names,show_weights=True))
415/1: **Data Dictionary**
415/2:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
415/3: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
415/4:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
415/5: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
415/6:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
415/7:
#load the head of the data
data.head()
415/8:
#load the tail of the data
data.tail()
415/9:
# let's view a sample of the data
data.sample(n=10, random_state=1)
415/10:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
415/11:
# checking column datatypes and number of non-null values
df.info()
415/12:
# checking for duplicate values
df.duplicated().sum()
415/13: df.isnull.sum()
415/14: df.isnull.sum(axis=1)
415/15: df.isnull().sum(axis=1)
415/16: df.isnull().sum()
415/17: df['TypeofContact'] = df['TypeofContact'].astype('float64')  # converting
415/18: df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting
415/19:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting
415/20:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting
415/21:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting  
df['Designation'] = df['TypeofContact'].astype('category')  # converting
415/22:
# checking column datatypes and number of non-null values
df.info()
415/23: df['Age'].fillna(df['Age'].median(), inplace=True)
415/24:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome '].fillna(df['MonthlyIncome '].median(), inplace=True)
415/25:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
415/26: df.isnull().sum()
415/27:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
415/28:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
415/29:
#Let's review the data information once again
df.info()
415/30:
#Let's get a detail description of the processed data
df.describe().round(2).T
415/31:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
415/32:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
415/33: y.value_counts(1)
415/34: y_test.value_counts(1)
415/35:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
415/36:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
415/37:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
415/38:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
415/39:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
415/40:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
432/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
432/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
432/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
432/4:
#load the head of the data
data.head()
432/5:
#load the tail of the data
data.tail()
432/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
432/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
432/8:
# checking column datatypes and number of non-null values
df.info()
432/9:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting  
df['Designation'] = df['TypeofContact'].astype('category')  # converting
432/10:
# checking column datatypes and number of non-null values
df.info()
432/11:
# checking for duplicate values
df.duplicated().sum()
432/12: df.isnull().sum()
432/13:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
432/14: df.isnull().sum()
432/15:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
432/16:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
432/17:
#Let's review the data information once again
df.info()
432/18:
#Let's get a detail description of the processed data
df.describe().round(2).T
432/19:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
432/20:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
432/21: y.value_counts(1)
432/22: y_test.value_counts(1)
432/23:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
432/24:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
432/25:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
432/26: df['Age'] = df['Age'].astype('int')  # converting
432/27:
df['Age'] = df['Age'].astype('int')  # converting  
df['DurationOfPitch'] = df['DurationOfPitch'].astype('int')
df['NumberOfFollowups'] = df['NumberOfFollowups'].astype('int')
df['PreferredPropertyStar'] = df['PreferredPropertyStar'].astype('int')
df['NumberOfChildrenVisiting'] = df['NumberOfChildrenVisiting'].astype('int')
df['MonthlyIncome'] = df['MonthlyIncome'].astype('int')
432/28:
# checking column datatypes and number of non-null values
df.info()
432/29:
df['Age'] = df['Age'].astype('int')  # converting  
df['DurationOfPitch'] = df['DurationOfPitch'].astype('int')
df['NumberOfFollowups'] = df['NumberOfFollowups'].astype('int')
df['PreferredPropertyStar'] = df['PreferredPropertyStar'].astype('int')
df['NumberOfChildrenVisiting'] = df['NumberOfChildrenVisiting'].astype('int')
df['MonthlyIncome'] = df['MonthlyIncome'].astype('int')
df['NumberOfTrips'] = df['NumberOfTrips'].astype('int')
432/30:
# checking column datatypes and number of non-null values
df.info()
432/31:
# checking for duplicate values
df.duplicated().sum()
432/32: df.isnull().sum()
432/33:
# checking column datatypes and number of non-null values
df.info()
433/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
433/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
433/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
433/4:
#load the head of the data
data.head()
433/5:
#load the tail of the data
data.tail()
433/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
433/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
433/8:
# checking column datatypes and number of non-null values
df.info()
433/9:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting  
df['Designation'] = df['TypeofContact'].astype('category')  # converting
433/10:
# checking column datatypes and number of non-null values
df.info()
433/11:
# checking for duplicate values
df.duplicated().sum()
433/12: df.isnull().sum()
433/13:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
433/14: df.isnull().sum()
433/15:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
433/16:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
433/17:
#Let's review the data information once again
df.info()
433/18:
#Let's get a detail description of the processed data
df.describe().round(2).T
433/19:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
433/20:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
433/21: y.value_counts(1)
433/22: y_test.value_counts(1)
433/23:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
433/24:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
433/25:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
433/26:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
433/27:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
433/28:
#Let's review the data information once again
df.info()
433/29:
#Let's get a detail description of the processed data
df.describe().round(2).T
433/30:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
433/31:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
433/32: y.value_counts(1)
433/33: y_test.value_counts(1)
433/34:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
433/35:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
433/36:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
433/37:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
434/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
434/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
434/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
434/4:
#load the head of the data
data.head()
434/5:
#load the tail of the data
data.tail()
434/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
434/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
434/8:
# checking column datatypes and number of non-null values
df.info()
434/9:
# checking column datatypes and number of non-null values
df.info()
434/10:
# checking for duplicate values
df.duplicated().sum()
434/11: df.isnull().sum()
434/12:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
434/13: df.isnull().sum()
434/14:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
434/15:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
434/16:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting  
df['Designation'] = df['TypeofContact'].astype('category')  # converting
435/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
435/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
435/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
435/4:
#load the head of the data
data.head()
435/5:
#load the tail of the data
data.tail()
435/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
435/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
435/8:
# checking column datatypes and number of non-null values
df.info()
435/9:
# checking column datatypes and number of non-null values
df.info()
435/10:
# checking for duplicate values
df.duplicated().sum()
435/11: df.isnull().sum()
435/12:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
435/13: df.isnull().sum()
435/14:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
435/15:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
435/16:
df['TypeofContact'] = df['TypeofContact'].astype('category')  # converting  
df['Occupation'] = df['Occupation'].astype('category')  # converting  
df['Gender'] = df['Gender'].astype('category')  # converting  
df['ProductPitched'] = df['TypeofContact'].astype('category')  # converting  
df['MaritalStatus'] = df['TypeofContact'].astype('category')  # converting  
df['Designation'] = df['TypeofContact'].astype('category')  # converting
435/17:

df.info()
435/18:
#Let's get a detail description of the processed data
df.describe().round(2).T
435/19:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
435/20:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
435/21: y.value_counts(1)
435/22: y_test.value_counts(1)
435/23:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
435/24:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
435/25:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
437/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
437/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
437/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
437/4:
#load the head of the data
data.head()
437/5:
#load the tail of the data
data.tail()
437/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
437/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
437/8:
# checking column datatypes and number of non-null values
df.info()
437/9:
# checking column datatypes and number of non-null values
df.info()
437/10:
# checking for duplicate values
df.duplicated().sum()
437/11: df.isnull().sum()
437/12:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
437/13: df.isnull().sum()
437/14:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
437/15:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
437/16: df.info()
437/17:
#Let's get a detail description of the processed data
df.describe().round(2).T
437/18:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
437/19:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
437/20: y.value_counts(1)
437/21: y_test.value_counts(1)
437/22:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
437/23:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
437/24:
#Fitting the model
d_tree = DecisionTreeClassifier(random_state=1)
d_tree.fit(X_train,y_train)

#Calculating different metrics
dtree_model_train_perf=model_performance_classification_sklearn(d_tree,X_train,y_train)
print("Training performance:\n",dtree_model_train_perf)
dtree_model_test_perf=model_performance_classification_sklearn(d_tree,X_test,y_test)
print("Testing performance:\n",dtree_model_test_perf)
#Creating confusion matrix
confusion_matrix_sklearn(d_tree, X_test, y_test)
437/25:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
437/26:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
437/27:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
437/28:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
437/29:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": -1},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
437/30:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
437/31: df.info()
437/32:
#Let's get a detail description of the processed data
df.describe().round(2).T
437/33:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
437/34:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
437/35: y.value_counts(1)
437/36: y_test.value_counts(1)
437/37:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
437/38:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
437/39:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
439/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
439/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
439/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
439/4:
#load the head of the data
data.head()
439/5:
#load the tail of the data
data.tail()
439/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
439/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
439/8:
# checking column datatypes and number of non-null values
df.info()
439/9:
# checking column datatypes and number of non-null values
df.info()
439/10:
# checking for duplicate values
df.duplicated().sum()
439/11: df.isnull().sum()
439/12: df.isnull().sum()
439/13:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": -1},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
439/14:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
439/15:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
439/16: df.info()
439/17:
#Let's get a detail description of the processed data
df.describe().round(2).T
439/18:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
439/19:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
439/20: y.value_counts(1)
439/21: y_test.value_counts(1)
439/22:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
439/23:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
439/24:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
439/25: df.isnull().sum()
440/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
440/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
440/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
440/4:
#load the head of the data
data.head()
440/5:
#load the tail of the data
data.tail()
440/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
440/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
440/8:
# checking column datatypes and number of non-null values
df.info()
440/9:
# checking column datatypes and number of non-null values
df.info()
440/10:
# checking for duplicate values
df.duplicated().sum()
440/11: df.isnull().sum()
440/12: df.isnull().sum()
440/13:
replaceStruct = {
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": -1},
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "default":     {"no": 0, "yes": 1 }
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
440/14:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
440/15: df.info()
440/16: df.isnull().sum()
440/17:
#Let's get a detail description of the processed data
df.describe().round(2).T
440/18:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
440/19:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
440/20: y.value_counts(1)
440/21: y_test.value_counts(1)
440/22:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
440/23:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
440/24:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
440/25:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
440/26:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
440/27:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
440/28:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
440/29:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
440/30: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
440/31:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
440/32:
#load the head of the data
data.head()
440/33:
#load the tail of the data
data.tail()
440/34:
# let's view a sample of the data
data.sample(n=10, random_state=1)
440/35:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
440/36:
# checking column datatypes and number of non-null values
df.info()
440/37:
# checking for duplicate values
df.duplicated().sum()
440/38: df.isnull().sum()
440/39:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited": 1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
440/40:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
440/41:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
440/42: df.info()
440/43: df.isnull().sum()
440/44:
#Let's get a detail description of the processed data
df.describe().round(2).T
440/45:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
440/46:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
440/47: y.value_counts(1)
440/48: y_test.value_counts(1)
440/49:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
440/50:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
440/51:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
440/52:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited":1, "Self Enquiry":2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
440/53:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
440/54:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited":1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
440/55:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
445/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
445/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
445/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
445/4:
#load the head of the data
data.head()
445/5:
#load the tail of the data
data.tail()
445/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
445/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
445/8:
# checking column datatypes and number of non-null values
df.info()
445/9:
# checking for duplicate values
df.duplicated().sum()
445/10: df.isnull().sum()
445/11:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited":1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
445/12:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
445/13:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
445/14: df.info()
445/15: df.isnull().sum()
445/16:
#Let's get a detail description of the processed data
df.describe().round(2).T
445/17:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
445/18:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
445/19: y.value_counts(1)
445/20: y_test.value_counts(1)
445/21:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
445/22:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
445/23:
dtree=DecisionTreeRegressor(random_state=1)
dtree.fit(X_train,y_train)
445/24:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
445/25:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
445/26:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
445/27:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
445/28:
le = preprocessing.LabelEncoder()
le.fit(["TypeofContact", "Designation", "MaritalStatus", "ProductPitched"])
LabelEncoder()
445/29:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
class sklearn.preprocessing.LabelEncoder
445/30:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
class sklearn preprocessing.LabelEncoder
445/31:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
class sklearn.preprocessing.LabelEncoder
445/32:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
445/33:
le = preprocessing.LabelEncoder()
le.fit(["TypeofContact", "Designation", "MaritalStatus", "ProductPitched"])
LabelEncoder()
445/34:
class sklearn.preprocessing.LabelEncode
le = preprocessing.LabelEncoder()
le.fit(["TypeofContact", "Designation", "MaritalStatus", "ProductPitched"])
LabelEncoder()
445/35:
class sklearn preprocessing.LabelEncode
le = preprocessing.LabelEncoder()
le.fit(["TypeofContact", "Designation", "MaritalStatus", "ProductPitched"])
LabelEncoder()
446/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
446/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
446/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
446/4:
#load the head of the data
data.head()
446/5:
#load the tail of the data
data.tail()
446/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
446/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
446/8:
# checking column datatypes and number of non-null values
df.info()
446/9:
# checking for duplicate values
df.duplicated().sum()
446/10: df.isnull().sum()
446/11:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact":{"Company Invited":1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
446/12:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
446/13:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
446/14: df.info()
446/15: df.isnull().sum()
446/16:
#Let's get a detail description of the processed data
df.describe().round(2).T
446/17:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
446/18:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
446/19: y.value_counts(1)
446/20: y_test.value_counts(1)
446/21:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
446/22:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
446/23:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
446/24:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
446/25:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4},
                "TypeofContact": {"Company Invited":1, "Self Enquiry": 2}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
446/26:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
446/27:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
446/28:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
447/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
447/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
447/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
447/4:
#load the head of the data
data.head()
447/5:
#load the tail of the data
data.tail()
447/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
447/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
447/8:
# checking column datatypes and number of non-null values
df.info()
447/9:
# checking for duplicate values
df.duplicated().sum()
447/10: df.isnull().sum()
447/11:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
447/12:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
447/13: df.drop("TypeofContact")
447/14: df.drop("TypeofContact", axis=1)
447/15: df.drop('TypeofContact', axis=1)
447/16: df = df.drop(TypeofContact, axis = 1)
447/17: df = df.drop('TypeofContact', axis = 1)
448/1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
448/2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
448/3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
448/4:
#load the head of the data
data.head()
448/5:
#load the tail of the data
data.tail()
448/6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
448/7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
448/8:
# checking column datatypes and number of non-null values
df.info()
448/9:
# checking for duplicate values
df.duplicated().sum()
448/10: df.isnull().sum()
448/11:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
448/12:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
448/13: df = df.drop('TypeofContact', axis = 1)
448/14:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
448/15: df.info()
448/16: df.isnull().sum()
448/17:
#Let's get a detail description of the processed data
df.describe().round(2).T
448/18:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
448/19:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
448/20: y.value_counts(1)
448/21: y_test.value_counts(1)
448/22:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
448/23:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
448/24:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
448/25:
#Fitting the model
rf_estimator = RandomForestClassifier(random_state=1)
rf_estimator.fit(X_train,y_train)

#Calculating different metrics
rf_estimator_model_train_perf=model_performance_classification_sklearn(rf_estimator,X_train,y_train)
print("Training performance:\n",rf_estimator_model_train_perf)
rf_estimator_model_test_perf=model_performance_classification_sklearn(rf_estimator,X_test,y_test)
print("Testing performance:\n",rf_estimator_model_test_perf)

#Creating confusion matrix
confusion_matrix_sklearn(rf_estimator, X_test, y_test)
448/26: %history -g
448/27: %history -g -f Tour Package
450/1:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(data, bins=50)
450/2:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
450/3: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
450/4:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
450/5:
#load the head of the data
data.head()
450/6:
#load the tail of the data
data.tail()
450/7:
# let's view a sample of the data
data.sample(n=10, random_state=1)
450/8:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
450/9:
# checking column datatypes and number of non-null values
df.info()
450/10:
# checking for duplicate values
df.duplicated().sum()
450/11: df.isnull().sum()
450/12:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
plt.hist(df, bins=50)
450/13:
# The following code plots a histrogram using the matplotlib package.
# The bins argument creates class intervals. In this case we are creating 50 such intervals
sns.distplot(data) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
450/14:
sns.distplot(df) # plots a frequency polygon superimposed on a histogram using the seaborn package.
# seaborn automatically creates class intervals. The number of bins can also be manually set.
450/15:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
450/16:
# checking column datatypes and number of non-null values
df.info()
450/17:
# checking for duplicate values
df.duplicated().sum()
450/18: df.isnull().sum()
450/19:
#Let's check the number of missing values in the whole features.
df.isnull().sum()
450/20:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
450/21:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
450/22:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
450/23: df.info()
450/24: df.isnull().sum()
450/25:
#Let's get a detail description of the processed data
df.describe().round(2).T
450/26:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
450/27: histogram_boxplot(df, "Age", kde=True)
450/28: histogram_boxplot(df, "CityTier", kde=True)
450/29: histogram_boxplot(df, "DurationOfPitch", kde=True)
450/30: histogram_boxplot(df, "NumberOfFollowups", kde=True)
450/31: histogram_boxplot(df, "NumberOfTrips", kde=True)
450/32: histogram_boxplot(df, "PitchSatisfactionScore", kde=True)
450/33: histogram_boxplot(df, "MonthlyIncome", kde=True)
450/34:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
450/35:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
450/36: labeled_barplot(df, "Age", perc=False, n=10)
450/37:
#Let's get a detail description of the processed data
df.describe().round(2).T
   1:
# Library to suppress warnings or deprecation notes 
import warnings
warnings.filterwarnings('ignore')

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

# Library to split data 
from sklearn.model_selection import train_test_split

# Libraries to import decision tree classifier and different ensemble classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split

# Libtune to tune model, get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score,f1_score,roc_auc_score
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
#To install xgboost library use - !pip install xgboost 
from xgboost import XGBClassifier
from sklearn import preprocessing
   2: data = pd. read_excel('Tourism.xlsx', sheet_name= 'Tourism')
   3:
# checking shape of the data
print(f"There are {data.shape[0]} rows and {data.shape[1]} columns.")
   4:
#load the head of the data
data.head()
   5:
#load the tail of the data
data.tail()
   6:
# let's view a sample of the data
data.sample(n=10, random_state=1)
   7:
# let's create a copy of the data to avoid any changes to original data
df = data.copy()
   8:
# checking column datatypes and number of non-null values
df.info()
   9:
# checking for duplicate values
df.duplicated().sum()
  10:
#Let's check the number of missing values in the whole features.
df.isnull().sum()
  11:
df['Age'].fillna(df['Age'].median(), inplace=True)
df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)
df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].median(), inplace=True)
df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].median(), inplace=True)
df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace=True)
df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].median(), inplace=True)
df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)
  12: df.info()
  13: df.isnull().sum()
  14:
# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram
  15: histogram_boxplot(df, "Age", kde=True)
  16: histogram_boxplot(df, "DurationOfPitch", kde=True)
  17: histogram_boxplot(df, "NumberOfFollowups", kde=True)
  18: histogram_boxplot(df, "NumberOfTrips", kde=True)
  19: histogram_boxplot(df, "MonthlyIncome", kde=True)
  20:
# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot
  21: labeled_barplot(df, "Age", perc=False, n=10)
  22: labeled_barplot(df, "Age", perc=False, n=10)
  23: labeled_barplot(df, "Age", perc=False, n=10)
  24: labeled_barplot(df, "Age", perc=False, n=10)
  25: labeled_barplot(df, "Age", perc=False, n=10)
  26: labeled_barplot(df, "Age", perc=False, n=10)
  27: labeled_barplot(df, "Age", perc=False, n=10)
  28:
replaceStruct = {
                "Occupation": {"Salaried": 1, "Free lancer": 2 , "Small Business": 3, "Large Business": 4},
                "Gender": {"Female": 1, "Male": 2 , "Fe male": 3},
                "ProductPitched":     {"Basic": 1, "Standard": 2 ,"Deluxe": 3 ,"Super Deluxe": 4},
                "MaritalStatus":     {"Single": 0, "Unmarried": 1 },
                "Designation":     {"Executive": 1, "Manager": 2, "Senior Manager": 3, "AVP": 4}
                    }
oneHotCols=["TypeofContact","Occupation","Gender","ProductPitched","MaritalStatus","Designation"]
  29:
df=df.replace(replaceStruct)
df=pd.get_dummies(df,columns=oneHotCols)
df.head(10)
  30:
X = data.drop('ProdTaken',axis=1)
y = data['ProdTaken']
  31:
# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
  32: y.value_counts(1)
  33: y_test.value_counts(1)
  34:
# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf
  35:
##  Function to calculate r2_score and RMSE on train and test data
def get_model_score(model, flag=True):
    '''
    model : classifier to predict values of X

    '''
    # defining an empty list to store train and test results
    score_list=[] 
    
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)
    
    train_r2=metrics.r2_score(y_train,pred_train)
    test_r2=metrics.r2_score(y_test,pred_test)
    train_rmse=np.sqrt(metrics.mean_squared_error(y_train,pred_train))
    test_rmse=np.sqrt(metrics.mean_squared_error(y_test,pred_test))
    
    #Adding all scores in the list
    score_list.extend((train_r2,test_r2,train_rmse,test_rmse))
    
    # If the flag is set to True then only the following print statements will be dispayed, the default value is True
    if flag==True: 
        print("R-sqaure on training set : ",metrics.r2_score(y_train,pred_train))
        print("R-square on test set : ",metrics.r2_score(y_test,pred_test))
        print("RMSE on training set : ",np.sqrt(metrics.mean_squared_error(y_train,pred_train)))
        print("RMSE on test set : ",np.sqrt(metrics.mean_squared_error(y_test,pred_test)))
    
    # returning the list with train and test scores
    return score_list
  36:
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
  37: %history -g -f Tour Package
