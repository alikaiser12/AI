{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSi3xW74q1uVKVkg09l5Kk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alikaiser12/AI/blob/main/Rizwan_recipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpxaE_Hj6Fro",
        "outputId": "7d5b78ff-ce19-472f-b8c8-63ab2d00c2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kq6pvpY06JR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# One-cell Colab script:\n",
        "# Recipe→Property predictor + Forward optimization to hit target specs\n",
        "# Works with stress–strain columns like:\n",
        "#   Strain(%)_Cel10_e, Stress(kPa)_Cel10_e, ...\n",
        "# If you already have true formulation columns (e.g., monomer%, crosslinker%), set RECIPE_FEATURES below.\n",
        "# ================================\n",
        "!pip -q install scikit-learn pandas numpy scipy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import differential_evolution\n",
        "\n",
        "# ========= CONFIG (edit these) =========\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"  # <-- upload or mount Drive and set the path\n",
        "\n",
        "# If you have explicit formulation columns in your sheet (e.g., 'monomer_wt%', 'crosslinker_wt%', ...), put them here:\n",
        "RECIPE_FEATURES: list = []   # e.g., [\"monomer_wt%\", \"crosslinker_wt%\", \"initiator_wt%\", \"temp_C\", \"cure_min\"]\n",
        "\n",
        "# Choose which properties to target. Provide *any subset*; others will be ignored in the objective.\n",
        "# Common property keys computed by this script:\n",
        "#   \"E0_5_kPa\",\"E5_10_kPa\",\"TanE10_kPa\",\"Yield_strain_frac\",\"Yield_stress_kPa\",\n",
        "#   \"Resilience_kJ_m3\",\"UTS_kPa\",\"Strain_UTS_frac\",\"Fracture_strain_frac\",\n",
        "#   \"Fracture_stress_kPa\",\"Toughness_kJ_m3\",\"Stress@5%_kPa\",\"Stress@10%_kPa\",\n",
        "#   \"Stress@15%_kPa\",\"Stress@20%_kPa\",\"Secant_0_15_kPa\"\n",
        "TARGETS: Dict[str, float] = {\n",
        "    \"UTS_kPa\": 90.0,            # example: optimize for UTS only\n",
        "    # \"Toughness_kJ_m3\": 7.0,   # add more targets if you want\n",
        "    # \"Strain_UTS_frac\": 0.16,\n",
        "}\n",
        "\n",
        "# Optimization settings (tweak if needed)\n",
        "DE_MAXITER = 180\n",
        "DE_POPSIZE = 18\n",
        "# ======================================\n",
        "\n",
        "# ---------- Helpers for stress–strain parsing ----------\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Your data is like 0, 500, 1000 ⇒ 0.0%, 5.0%, 10.0%. Convert to strain fraction.\n",
        "    return (eps_raw.astype(float) / 100.0) / 100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float) -> Optional[float]:\n",
        "    if b <= a: return None\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init * (xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0 * (x1 - x0) / (y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float) -> Optional[float]:\n",
        "    x = p / 100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_recipe_from_label(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "# ---------- Load & derive properties ----------\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "labels_strain, labels_stress, labels = find_pairs(df)\n",
        "\n",
        "records = []\n",
        "for lab in labels:\n",
        "    eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "    sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "    mask = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "    eps_raw, sig_raw = eps_raw[mask], sig_raw[mask]\n",
        "    if len(eps_raw) < 3: continue\n",
        "\n",
        "    eps = to_fraction(eps_raw)\n",
        "    eps, sig = ensure_sorted(eps, sig_raw)\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "    # Moduli\n",
        "    E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "    E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "    TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "\n",
        "    # Yield via 0.2% offset, resilience\n",
        "    eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "    resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "\n",
        "    # UTS & fracture\n",
        "    uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "    frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "    toughness = integrate_toughness(eps, sig, None)\n",
        "\n",
        "    # Probes\n",
        "    s5  = stress_at(f, 5, lo, hi)\n",
        "    s10 = stress_at(f, 10, lo, hi)\n",
        "    s15 = stress_at(f, 15, lo, hi)\n",
        "    s20 = stress_at(f, 20, lo, hi)\n",
        "    Sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "\n",
        "    rec = {\n",
        "        \"label\": lab,\n",
        "        \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "        \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "        \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "        \"Resilience_kJ_m3\":  resilience,\n",
        "        \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "        \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "        \"Toughness_kJ_m3\": toughness,\n",
        "        \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "        \"Secant_0_15_kPa\": Sec_0_15,\n",
        "    }\n",
        "\n",
        "    # Recipe features\n",
        "    if RECIPE_FEATURES:\n",
        "        # Expect these columns to be present in df (per-row constants or per-label sheet)\n",
        "        # If your recipe data is in another sheet/table, merge it here on \"label\".\n",
        "        for c in RECIPE_FEATURES:\n",
        "            rec[c] = df[c].iloc[0] if c in df.columns else np.nan\n",
        "    else:\n",
        "        mat_type, mat_level = parse_recipe_from_label(lab)\n",
        "        rec[\"mat_type\"] = mat_type\n",
        "        rec[\"mat_level\"] = mat_level\n",
        "\n",
        "    records.append(rec)\n",
        "\n",
        "props_df = pd.DataFrame(records)\n",
        "\n",
        "# ---------- Build model dataset ----------\n",
        "# Choose X (recipe) and Y (properties). If RECIPE_FEATURES empty, use mat_type + mat_level from labels.\n",
        "if RECIPE_FEATURES:\n",
        "    Xcols = RECIPE_FEATURES\n",
        "else:\n",
        "    Xcols = [\"mat_type\", \"mat_level\"]\n",
        "\n",
        "Ycols_all = [\n",
        "    \"E0_5_kPa\",\"E5_10_kPa\",\"TanE10_kPa\",\"Yield_strain_frac\",\"Yield_stress_kPa\",\n",
        "    \"Resilience_kJ_m3\",\"UTS_kPa\",\"Strain_UTS_frac\",\"Fracture_strain_frac\",\n",
        "    \"Fracture_stress_kPa\",\"Toughness_kJ_m3\",\"Stress@5%_kPa\",\"Stress@10%_kPa\",\n",
        "    \"Stress@15%_kPa\",\"Stress@20%_kPa\",\"Secant_0_15_kPa\"\n",
        "]\n",
        "# Only keep Y columns that exist (depending on curve coverage)\n",
        "Ycols = [c for c in Ycols_all if c in props_df.columns]\n",
        "\n",
        "# Fill NaN values in target columns with the mean of existing values\n",
        "for col in Ycols:\n",
        "    if props_df[col].isnull().any():\n",
        "        mean_val = props_df[col].mean()\n",
        "        props_df[col].fillna(mean_val, inplace=True)\n",
        "\n",
        "data = props_df[Xcols + Ycols].dropna()\n",
        "\n",
        "if len(data) < 3:\n",
        "    raise RuntimeError(\"Not enough complete rows to train. Ensure recipe features are set or label parsing works and curves cover the needed ranges.\")\n",
        "\n",
        "\n",
        "X = data[Xcols].copy()\n",
        "Y = data[Ycols].copy()\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "transformers = []\n",
        "if any(X[c].dtype == \"O\" for c in X.columns):  # has categorical\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "    transformers.append((\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols))\n",
        "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
        "if num_cols:\n",
        "    transformers.append((\"num\", StandardScaler(), num_cols))\n",
        "preprocess = ColumnTransformer(transformers) if transformers else \"passthrough\"\n",
        "\n",
        "base = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "reg = Pipeline([(\"prep\", preprocess), (\"rf\", MultiOutputRegressor(base))])\n",
        "\n",
        "# Train / quick eval\n",
        "ts = 0.33 if len(X) >= 6 else 0.5 if len(X) >= 4 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, Y_tr = X, Y\n",
        "    X_te, Y_te = X.iloc[:0], Y.iloc[:0]\n",
        "\n",
        "reg.fit(X_tr, Y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    Y_hat = reg.predict(X_te)\n",
        "    for i, col in enumerate(Y.columns):\n",
        "        r2 = r2_score(Y_te.iloc[:, i], Y_hat[:, i])\n",
        "        mae = mean_absolute_error(Y_te.iloc[:, i], Y_hat[:, i])\n",
        "        print(f\"[Test] {col:>18} | R2={r2:6.3f} MAE={mae:8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (dataset small).\")\n",
        "\n",
        "# ---------- Forward optimization ----------\n",
        "# We only consider the properties listed in TARGETS.\n",
        "target_keys = [k for k in TARGETS.keys() if k in Y.columns]\n",
        "if len(target_keys) == 0:\n",
        "    raise ValueError(\"None of the TARGETS keys match the computed properties. Check your TARGETS dict.\")\n",
        "\n",
        "target_vec = np.array([TARGETS[k] for k in target_keys], float)\n",
        "\n",
        "# feature bounds for continuous variables\n",
        "bounds = []\n",
        "discrete_enums = {}  # for categorical enumeration\n",
        "for c in X.columns:\n",
        "    if np.issubdtype(X[c].dtype, np.number):\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "    else:\n",
        "        # categorical -> enumerate later; mark placeholder in bounds to keep indexing aligned\n",
        "        discrete_enums[c] = sorted(X[c].unique().tolist())\n",
        "        bounds.append(None)\n",
        "\n",
        "# Build a template row for prediction\n",
        "def build_row(vec, cat_choice):\n",
        "    row = {}\n",
        "    j = 0\n",
        "    for i, col in enumerate(X.columns):\n",
        "        if col in discrete_enums:\n",
        "            row[col] = cat_choice[col]\n",
        "        else:\n",
        "            row[col] = float(vec[j]); j += 1\n",
        "    return pd.DataFrame([row])\n",
        "\n",
        "# Loss over selected targets (weighted MSE + MAPE)\n",
        "Y_slice_idx = [Y.columns.get_loc(k) for k in target_keys]\n",
        "var = np.var(Y.iloc[:, Y_slice_idx].values, axis=0)\n",
        "weights = 1.0 / np.where(var <= 1e-12, 1.0, var)\n",
        "\n",
        "def loss_from_row(dfrow):\n",
        "    yhat = reg.predict(dfrow)[0]\n",
        "    ysel = yhat[Y_slice_idx]\n",
        "    eps = 1e-8\n",
        "    mse = np.mean(weights * (ysel - target_vec)**2)\n",
        "    mape = np.mean(np.abs(ysel - target_vec) / (np.abs(target_vec) + eps))\n",
        "    return 0.5*mse + 0.5*mape, yhat\n",
        "\n",
        "best = {\"loss\": np.inf, \"row\": None, \"yhat\": None}\n",
        "\n",
        "# Enumerate categoricals (if any), optimize continuous with differential evolution\n",
        "cat_product = [{}]\n",
        "for c, vals in discrete_enums.items():\n",
        "    new = []\n",
        "    for base_choice in cat_product:\n",
        "        for v in vals:\n",
        "            choice = base_choice.copy(); choice[c] = v\n",
        "            new.append(choice)\n",
        "    cat_product = new\n",
        "\n",
        "if len(cat_product) == 0:\n",
        "    cat_product = [{}]\n",
        "\n",
        "for cat_choice in cat_product:\n",
        "    # collect bounds for this subspace\n",
        "    cont_bounds = [b for (b, col) in zip(bounds, X.columns) if col not in discrete_enums]\n",
        "\n",
        "    if len(cont_bounds) == 0:\n",
        "        # No continuous features — just evaluate categorical combo\n",
        "        dfrow = build_row([], cat_choice)\n",
        "        loss, yhat = loss_from_row(dfrow)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "        continue\n",
        "\n",
        "    def _wrap(x):\n",
        "        dfrow = build_row(x, cat_choice)\n",
        "        loss, _ = loss_from_row(dfrow)\n",
        "        return loss\n",
        "\n",
        "    result = differential_evolution(_wrap, bounds=cont_bounds, maxiter=DE_MAXITER, popsize=DE_POPSIZE, tol=1e-6, polish=True, seed=42)\n",
        "    dfrow = build_row(result.x, cat_choice)\n",
        "    loss, yhat = loss_from_row(dfrow)\n",
        "    if loss < best[\"loss\"]:\n",
        "        best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "\n",
        "opt_recipe = best[\"row\"]\n",
        "opt_pred = pd.DataFrame([best[\"yhat\"]], columns=Y.columns)\n",
        "report = pd.concat({\"recipe_opt\": opt_recipe.reset_index(drop=True), \"predicted_props\": opt_pred[target_keys].reset_index(drop=True)}, axis=1)\n",
        "\n",
        "print(\"\\n=== Optimal recipe (for the specified TARGETS) ===\")\n",
        "display_cols = list(opt_recipe.columns)\n",
        "print(opt_recipe[display_cols].to_string(index=False))\n",
        "\n",
        "print(\"\\n=== Predicted properties on targets ===\")\n",
        "print(opt_pred[target_keys].to_string(index=False))\n",
        "\n",
        "# Save artifacts\n",
        "props_out = \"/content/derived_properties_table.csv\"\n",
        "opt_out = \"/content/optimal_recipe_and_predictions.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "report.to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "N2IxmBFy6JZn",
        "outputId": "7ef24ffb-93fa-42b3-d411-c6089202e0cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3831001079.py:101: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n",
            "/tmp/ipython-input-3831001079.py:199: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  props_df[col].fillna(mean_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Not enough complete rows to train. Ensure recipe features are set or label parsing works and curves cover the needed ranges.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3831001079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough complete rows to train. Ensure recipe features are set or label parsing works and curves cover the needed ranges.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Not enough complete rows to train. Ensure recipe features are set or label parsing works and curves cover the needed ranges."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Build model dataset (robust to sparse columns / NaNs) ----------\n",
        "# Choose X (recipe) and Y (properties). If RECIPE_FEATURES empty, use mat_type + mat_level from labels.\n",
        "if RECIPE_FEATURES:\n",
        "    Xcols = RECIPE_FEATURES\n",
        "else:\n",
        "    Xcols = [\"mat_type\", \"mat_level\"]\n",
        "\n",
        "# Which targets do you want to fit/optimize on?\n",
        "all_cols_available = set(props_df.columns)\n",
        "target_keys = [k for k in TARGETS.keys() if k in all_cols_available]\n",
        "\n",
        "if len(target_keys) == 0:\n",
        "    raise ValueError(\n",
        "        f\"None of your TARGETS {list(TARGETS.keys())} exist in computed properties. \"\n",
        "        f\"Available: {sorted(all_cols_available)}\"\n",
        "    )\n",
        "\n",
        "# Count how many non-NaN rows exist per requested target\n",
        "counts = {k: int(props_df[k].notna().sum()) for k in target_keys}\n",
        "print(\"Non-NaN rows per requested target:\", counts)\n",
        "\n",
        "# Keep only targets with at least 2 rows (so the model can learn something)\n",
        "Ycols = [k for k in target_keys if counts[k] >= 2]\n",
        "\n",
        "# If nothing qualifies, fall back to a single, common property such as UTS_kPa (if present)\n",
        "if len(Ycols) == 0:\n",
        "    if \"UTS_kPa\" in props_df.columns and props_df[\"UTS_kPa\"].notna().sum() >= 2:\n",
        "        print(\"No requested target has >=2 rows. Falling back to Y=['UTS_kPa'].\")\n",
        "        Ycols = [\"UTS_kPa\"]\n",
        "        target_keys = [\"UTS_kPa\"]\n",
        "        TARGETS = {\"UTS_kPa\": TARGETS.get(\"UTS_kPa\", float(props_df[\"UTS_kPa\"].median()))}\n",
        "    else:\n",
        "        # Print helpful diagnostics and stop\n",
        "        avail_counts = {c: int(props_df[c].notna().sum()) for c in [\n",
        "            \"UTS_kPa\",\"Toughness_kJ_m3\",\"Strain_UTS_frac\",\"E0_5_kPa\",\"Stress@5%_kPa\",\"Stress@10%_kPa\"\n",
        "        ] if c in props_df.columns}\n",
        "        raise RuntimeError(\n",
        "            \"Not enough complete rows for your selected targets. \"\n",
        "            f\"Try targeting a property with more coverage. Coverage snapshot: {avail_counts}\"\n",
        "        )\n",
        "\n",
        "use_cols = Xcols + Ycols\n",
        "data = props_df[use_cols].dropna(subset=Ycols)\n",
        "if len(data) < 2:\n",
        "    raise RuntimeError(\n",
        "        f\"After filtering to X={Xcols} and Y={Ycols}, \"\n",
        "        f\"only {len(data)} complete rows remain. \"\n",
        "        \"Remove sparsely available targets or expand your dataset.\"\n",
        "    )\n",
        "\n",
        "X = data[Xcols].copy()\n",
        "Y = data[Ycols].copy()\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "transformers = []\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
        "\n",
        "if cat_cols:\n",
        "    transformers.append((\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols))\n",
        "if num_cols:\n",
        "    transformers.append((\"num\", StandardScaler(), num_cols))\n",
        "\n",
        "preprocess = ColumnTransformer(transformers) if transformers else \"passthrough\"\n",
        "\n",
        "base = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "reg = Pipeline([(\"prep\", preprocess), (\"rf\", MultiOutputRegressor(base))])\n",
        "\n",
        "# Train / quick eval (handles tiny datasets)\n",
        "ts = 0.33 if len(X) >= 6 else 0.5 if len(X) >= 4 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, Y_tr = X, Y\n",
        "    X_te, Y_te = X.iloc[:0], Y.iloc[:0]\n",
        "\n",
        "reg.fit(X_tr, Y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    Y_hat = reg.predict(X_te)\n",
        "    for i, col in enumerate(Y.columns):\n",
        "        r2 = r2_score(Y_te.iloc[:, i], Y_hat[:, i])\n",
        "        mae = mean_absolute_error(Y_te.iloc[:, i], Y_hat[:, i])\n",
        "        print(f\"[Test] {col:>18} | R2={r2:6.3f} MAE={mae:8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (dataset small).\")\n",
        "\n",
        "# ---------- Forward optimization (only on your selected targets) ----------\n",
        "# bounds for continuous features; enumerate any categoricals\n",
        "bounds = []\n",
        "discrete_enums = {}\n",
        "for c in X.columns:\n",
        "    if np.issubdtype(X[c].dtype, np.number):\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "    else:\n",
        "        discrete_enums[c] = sorted(X[c].unique().tolist())\n",
        "        bounds.append(None)\n",
        "\n",
        "def build_row(vec, cat_choice):\n",
        "    row = {}\n",
        "    j = 0\n",
        "    for i, col in enumerate(X.columns):\n",
        "        if col in discrete_enums:\n",
        "            row[col] = cat_choice[col]\n",
        "        else:\n",
        "            row[col] = float(vec[j]); j += 1\n",
        "    return pd.DataFrame([row])\n",
        "\n",
        "# assemble the target vector in the same order as Ycols\n",
        "target_vec = np.array([TARGETS[k] for k in Ycols], float)\n",
        "\n",
        "# weights for loss: inverse variance of Y-train slice to balance scales\n",
        "var = np.var(Y_tr[Ycols].values, axis=0) if len(Y_tr) > 0 else np.var(Y[Ycols].values, axis=0)\n",
        "weights = 1.0 / np.where(var <= 1e-12, 1.0, var)\n",
        "\n",
        "def loss_from_row(dfrow):\n",
        "    yhat = reg.predict(dfrow)[0]\n",
        "    # yhat aligns to Ycols order\n",
        "    eps = 1e-8\n",
        "    mse = np.mean(weights * (yhat - target_vec)**2)\n",
        "    mape = np.mean(np.abs(yhat - target_vec) / (np.abs(target_vec) + eps))\n",
        "    return 0.5*mse + 0.5*mape, yhat\n",
        "\n",
        "# enumerate categoricals; DE over continuous\n",
        "from scipy.optimize import differential_evolution\n",
        "best = {\"loss\": np.inf, \"row\": None, \"yhat\": None}\n",
        "\n",
        "cat_product = [{}]\n",
        "for c, vals in discrete_enums.items():\n",
        "    new = []\n",
        "    for base_choice in cat_product:\n",
        "        for v in vals:\n",
        "            tmp = base_choice.copy(); tmp[c] = v\n",
        "            new.append(tmp)\n",
        "    cat_product = new\n",
        "if len(cat_product) == 0:\n",
        "    cat_product = [{}]\n",
        "\n",
        "for cat_choice in cat_product:\n",
        "    cont_bounds = [b for (b, col) in zip(bounds, X.columns) if col not in discrete_enums]\n",
        "    if len(cont_bounds) == 0:\n",
        "        dfrow = build_row([], cat_choice)\n",
        "        loss, yhat = loss_from_row(dfrow)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "        continue\n",
        "\n",
        "    def _wrap(x):\n",
        "        dfrow = build_row(x, cat_choice)\n",
        "        loss, _ = loss_from_row(dfrow)\n",
        "        return loss\n",
        "\n",
        "    result = differential_evolution(_wrap, bounds=cont_bounds, maxiter=DE_MAXITER, popsize=DE_POPSIZE, tol=1e-6, polish=True, seed=42)\n",
        "    dfrow = build_row(result.x, cat_choice)\n",
        "    loss, yhat = loss_from_row(dfrow)\n",
        "    if loss < best[\"loss\"]:\n",
        "        best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "\n",
        "opt_recipe = best[\"row\"]\n",
        "opt_pred = pd.DataFrame([best[\"yhat\"]], columns=Ycols)\n",
        "\n",
        "print(\"\\n=== Optimal recipe (for YOUR targets only) ===\")\n",
        "print(opt_recipe.to_string(index=False))\n",
        "print(\"\\n=== Predicted properties (on those targets) ===\")\n",
        "print(opt_pred.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPuMMj2B7N83",
        "outputId": "c1c9c12b-3ddc-4b15-a416-95aedc15469f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-NaN rows per requested target: {'UTS_kPa': 10}\n",
            "[Test]            UTS_kPa | R2=-0.526 MAE=  13.781\n",
            "\n",
            "=== Optimal recipe (for YOUR targets only) ===\n",
            "mat_type  mat_level\n",
            "     Cel  15.148414\n",
            "\n",
            "=== Predicted properties (on those targets) ===\n",
            "  UTS_kPa\n",
            "82.020833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(props_df.isna().mean().sort_values())  # fraction of NaNs per column\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJeAu_ZF7Og-",
        "outputId": "8361cf6f-b314-406c-bca6-3709a109fe27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label                   0.0\n",
            "E0_5_kPa                0.0\n",
            "E5_10_kPa               0.0\n",
            "TanE10_kPa              0.0\n",
            "Yield_strain_frac       0.0\n",
            "Yield_stress_kPa        0.0\n",
            "Resilience_kJ_m3        0.0\n",
            "UTS_kPa                 0.0\n",
            "Strain_UTS_frac         0.0\n",
            "Fracture_strain_frac    0.0\n",
            "Fracture_stress_kPa     0.0\n",
            "Toughness_kJ_m3         0.0\n",
            "Stress@5%_kPa           0.0\n",
            "Stress@10%_kPa          0.0\n",
            "Stress@15%_kPa          0.0\n",
            "Secant_0_15_kPa         0.0\n",
            "mat_level               0.0\n",
            "mat_type                0.0\n",
            "Stress@20%_kPa          1.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Single-cell: Composition design for target UTS (and others)\n",
        "# ============================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ========== USER CONFIG ==========\n",
        "# 1) Stress–strain file (must have paired columns like Strain(%)_X and Stress(kPa)_X, where X is a label)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE of the two):\n",
        "#    (A) Separate file (CSV or XLSX) with a table that contains 'label' + composition columns\n",
        "COMPOSITION_PATH = \"/content/composition_table.xlsx\"   # e.g., \"/content/recipe_table.csv\" or \"\"\n",
        "#    (B) OR a sheet *inside* DATA_PATH:\n",
        "COMPOSITION_SHEET = \"\"  # e.g., \"composition\" if you kept it in the same workbook\n",
        "\n",
        "# 3) Which columns in your composition table are the actual composition variables you want the optimizer to choose?\n",
        "#    Examples: wt% columns that sum to ~100, or molar ratios, etc. These must be numeric.\n",
        "#    If empty, the code will fall back to ('mat_type', 'mat_level') parsed from labels (coarse).\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # \"monomer_wt%\", \"crosslinker_wt%\", \"initiator_wt%\", \"solvent_wt%\", \"salt_mM\"\n",
        "]\n",
        "\n",
        "# 4) Sum constraint for composition variables (e.g., sum of wt% must equal 100)\n",
        "#    Set to None if not applicable. Otherwise set a float like 100.0\n",
        "COMPOSITION_TOTAL = 100.0  # or None\n",
        "\n",
        "# 5) Targets (use any subset). Start with UTS only, then add more as needed.\n",
        "TARGETS: Dict[str, float] = {\n",
        "    \"UTS_kPa\": 90.0,\n",
        "    # \"Toughness_kJ_m3\": 7.0,\n",
        "    # \"Strain_UTS_frac\": 0.16,\n",
        "}\n",
        "# =================================\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Your data pattern looks like: [0, 500, 1000, ...] -> 0.0%, 5.0%, 10.0% -> 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "# ======== Derive properties from curves ========\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ======== Load composition table if provided ========\n",
        "def load_composition() -> pd.DataFrame:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ======== Build modeling table ========\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    X = df_model[COMPOSITION_COLS].astype(float).copy()\n",
        "    # Optional: detect if columns are percentages that should sum to ~COMPOSITION_TOTAL\n",
        "    sum_close = None\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        s = X[COMPOSITION_COLS].sum(axis=1)\n",
        "        sum_close = np.median(np.isclose(s, COMPOSITION_TOTAL, rtol=0.01, atol=1e-2))\n",
        "        print(f\"[Info] Median samples match sum≈{COMPOSITION_TOTAL}? -> {bool(sum_close)}\")\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    # Encode mat_type as categorical + mat_level numeric\n",
        "    X = props_df[[\"mat_type\", \"mat_level\"]].copy()\n",
        "\n",
        "# Y: ONLY use the targets you care about (robust against NaNs in other properties)\n",
        "available = set(props_df.columns)\n",
        "target_keys = [k for k in TARGETS if k in available]\n",
        "if not target_keys:\n",
        "    raise ValueError(f\"None of your TARGETS {list(TARGETS.keys())} exist in derived properties. Available: {sorted(available)}\")\n",
        "Y = props_df[[\"label\"] + target_keys].dropna(subset=target_keys)\n",
        "\n",
        "# Align X and Y by label\n",
        "if MODE == \"composition\":\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + target_keys].dropna(subset=target_keys + COMPOSITION_COLS)\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    Y = XY[target_keys].astype(float)\n",
        "else:\n",
        "    XY = props_df[[\"label\", \"mat_type\", \"mat_level\"] + target_keys].dropna(subset=target_keys)\n",
        "    X = XY[[\"mat_type\", \"mat_level\"]].copy()\n",
        "    Y = XY[target_keys].astype(float)\n",
        "\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train. Ensure targets have at least 2 valid samples and composition columns are present (if using composition mode).\")\n",
        "\n",
        "# ======== Model pipeline ========\n",
        "transformers = []\n",
        "if MODE == \"composition\":\n",
        "    # all numeric\n",
        "    num_cols = X.columns.tolist()\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), num_cols)])\n",
        "else:\n",
        "    # mat_type categorical + mat_level numeric\n",
        "    cat_cols = [\"mat_type\"]\n",
        "    num_cols = [\"mat_level\"]\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "    ])\n",
        "\n",
        "base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "reg = Pipeline([(\"prep\", preprocess), (\"rf\", MultiOutputRegressor(base))])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, Y_tr = X, Y\n",
        "    X_te, Y_te = X.iloc[:0], Y.iloc[:0]\n",
        "\n",
        "reg.fit(X_tr, Y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    Y_hat = reg.predict(X_te)\n",
        "    print(\"=== Quick holdout ===\")\n",
        "    for i, col in enumerate(Y.columns):\n",
        "        print(f\"{col:>18}  R2={r2_score(Y_te.iloc[:, i], Y_hat[:, i]):6.3f}  MAE={mean_absolute_error(Y_te.iloc[:, i], Y_hat[:, i]):8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ======== Forward design: get composition (or knobs) for TARGETS ========\n",
        "# Build loss on your targets only\n",
        "tgt = np.array([TARGETS[k] for k in target_keys], float)\n",
        "\n",
        "# Separate bounds for numeric and handle categoricals\n",
        "bounds = []\n",
        "cont_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "discrete_enums = {c: sorted(X[c].unique().tolist()) for c in cat_cols}\n",
        "\n",
        "for c in cont_cols:\n",
        "    lo, hi = float(X[c].min()), float(X[c].max())\n",
        "    span = hi - lo if hi > lo else 1.0\n",
        "    bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "# Build a template row for prediction\n",
        "def build_row(cont_vec, cat_choice):\n",
        "    row = {}\n",
        "    j = 0\n",
        "    for col in X.columns:\n",
        "        if col in cat_cols:\n",
        "            row[col] = cat_choice[col]\n",
        "        else:\n",
        "            row[col] = float(cont_vec[j]); j += 1\n",
        "    return pd.DataFrame([row], columns=X.columns) # Ensure columns order matches X\n",
        "\n",
        "# Sum-to-constant constraint (if composition percentages)\n",
        "constraints = []\n",
        "if MODE == \"composition\" and COMPOSITION_TOTAL is not None:\n",
        "    def sum_eq(cont_vec, cont_cols=cont_cols, total=COMPOSITION_TOTAL):\n",
        "        # This assumes cont_cols are the composition components that sum\n",
        "        return float(np.sum(cont_vec) - total)\n",
        "    constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "# penalty for going outside historical range (soft corners)\n",
        "if cont_cols:\n",
        "    mins = X[cont_cols].min().values\n",
        "    maxs = X[cont_cols].max().values\n",
        "else:\n",
        "    mins = np.array([])\n",
        "    maxs = np.array([])\n",
        "\n",
        "\n",
        "def objective(cont_vec, cat_choice):\n",
        "    row = build_row(cont_vec, cat_choice)\n",
        "    pred = reg.predict(row)[0]\n",
        "    # Weighted MSE + MAPE on targets\n",
        "    var = np.var(Y_tr.values, axis=0) if len(Y_tr) else np.var(Y.values, axis=0)\n",
        "    var = var if var.size == pred.size else np.ones_like(pred)  # safeguard\n",
        "    w = 1.0 / np.where(var <= 1e-12, 1.0, var)\n",
        "    eps = 1e-8\n",
        "    # align only the target indices (Y order == target_keys order)\n",
        "    # our Y_tr columns equal target_keys in order already\n",
        "    y_pred_targets = pred[[Y.columns.get_loc(k) for k in target_keys]] # Ensure target columns are selected from prediction\n",
        "    mse = np.mean(w * (y_pred_targets - tgt)**2)\n",
        "    mape = np.mean(np.abs(y_pred_targets - tgt)/(np.abs(tgt)+eps))\n",
        "    base_loss = 0.5*mse + 0.5*mape\n",
        "\n",
        "    # Soft penalty if outside observed domain (only for continuous features)\n",
        "    penalty = 0.0\n",
        "    if cont_cols:\n",
        "        over_low = np.maximum(0.0, mins - cont_vec)\n",
        "        over_high = np.maximum(0.0, cont_vec - maxs)\n",
        "        penalty = 1e-3 * np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    return base_loss + penalty\n",
        "\n",
        "\n",
        "# Initial guess for continuous: median of continuous columns\n",
        "x0_cont = X[cont_cols].median().values if cont_cols else np.array([])\n",
        "\n",
        "# Enumerate categoricals; minimize continuous for each category combination\n",
        "from scipy.optimize import minimize\n",
        "best = {\"loss\": np.inf, \"row\": None, \"yhat\": None}\n",
        "\n",
        "cat_product = [{}]\n",
        "if cat_cols:\n",
        "    import itertools\n",
        "    cat_values = [discrete_enums[c] for c in cat_cols]\n",
        "    cat_product = [{c: v for c, v in zip(cat_cols, combo)} for combo in itertools.product(*cat_values)]\n",
        "\n",
        "\n",
        "for cat_choice in cat_product:\n",
        "    if not cont_cols:\n",
        "        # No continuous features — just evaluate categorical combo\n",
        "        dfrow = build_row(np.array([]), cat_choice)\n",
        "        loss, yhat = objective(np.array([]), cat_choice), reg.predict(dfrow)[0] # Evaluate loss with objective directly\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "        continue\n",
        "\n",
        "    def _wrap(cont_vec):\n",
        "        return objective(cont_vec, cat_choice)\n",
        "\n",
        "    result = minimize(\n",
        "        _wrap, x0_cont,\n",
        "        method=\"SLSQP\",\n",
        "        bounds=bounds,\n",
        "        constraints=constraints,\n",
        "        options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False}\n",
        "    )\n",
        "\n",
        "    if result.success:\n",
        "        loss = result.fun\n",
        "        dfrow = build_row(result.x, cat_choice)\n",
        "        yhat = reg.predict(dfrow)[0]\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "    else:\n",
        "        print(f\"[Warn] Optimization failed for {cat_choice}: {result.message}\")\n",
        "\n",
        "\n",
        "if best[\"row\"] is None:\n",
        "     raise RuntimeError(\"Optimization failed for all combinations.\")\n",
        "\n",
        "\n",
        "opt_recipe = best[\"row\"]\n",
        "opt_pred = pd.DataFrame([best[\"yhat\"]], columns=Y.columns) # Use Y.columns for full prediction columns\n",
        "report = pd.concat({\"recipe_opt\": opt_recipe.reset_index(drop=True), \"predicted_targets\": opt_pred[target_keys].reset_index(drop=True)}, axis=1) # Report only target columns\n",
        "\n",
        "\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "if MODE == \"composition\":\n",
        "    print(\"Mode: COMPOSITION → PROPERTIES\")\n",
        "    print(\"\\nRecommended composition:\")\n",
        "    display_cols = COMPOSITION_COLS\n",
        "else:\n",
        "    print(\"Mode: LABEL FALLBACK (no composition provided)\")\n",
        "    print(\"\\nRecommended knobs parsed from label (not true chemistry):\")\n",
        "    display_cols = X.columns.tolist()\n",
        "\n",
        "print(opt_recipe[display_cols].to_string(index=False))\n",
        "print(\"\\nPredicted target properties:\")\n",
        "print(report.to_string(index=False))\n",
        "\n",
        "# Save files\n",
        "props_out = \"/content/_derived_properties_table.csv\"\n",
        "opt_out = \"/content/_optimal_recipe_and_predictions.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "report.to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# Helpful tip: if you get \"not enough rows\", print coverage per target:\n",
        "# print(props_df[target_keys].notna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j2_lZzq7Oj_",
        "outputId": "b79481bf-de1b-4f59-b004-288a7484ffc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3718803818.py:103: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n",
            "=== Quick holdout ===\n",
            "           UTS_kPa  R2=-0.548  MAE=  13.855\n",
            "\n",
            "================ RESULTS ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended knobs parsed from label (not true chemistry):\n",
            "mat_type  mat_level\n",
            "     Cel        5.0\n",
            "\n",
            "Predicted target properties:\n",
            "recipe_opt           predicted_targets\n",
            "  mat_type mat_level           UTS_kPa\n",
            "       Cel       5.0         61.194722\n",
            "\n",
            "Saved:\n",
            "  /content/_derived_properties_table.csv\n",
            "  /content/_optimal_recipe_and_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you’ll see\n",
        "\n",
        "If you provided COMPOSITION_COLS and a composition table with label matching the stress–strain labels, you’ll get a recommended chemical composition (respecting a sum=100 constraint if you set COMPOSITION_TOTAL=100.0) that best hits your TARGETS.\n",
        "\n",
        "If you skip composition, you’ll get recommended coarse knobs (mat_type, mat_level), which is not a full chemistry but still runs.\n",
        "\n",
        "Common pitfalls (and quick fixes)\n",
        "\n",
        "Error: not enough rows → Start with TARGETS = {\"UTS_kPa\": <value>}; UTS is almost always available.\n",
        "\n",
        "Sum constraint → If your variables are wt%, keep COMPOSITION_TOTAL=100.0. If not percentages, set it to None.\n",
        "\n",
        "Column names → Make sure COMPOSITION_COLS exactly match your table’s numeric columns and that a label column matches the stress–strain label suffixes.\n",
        "\n",
        "If you share the names of your actual composition columns (e.g., AAm_wt, BIS_wt, APS_wt, TEMED_wt, Water_wt, NaCl_mM), I can pre-fill COMPOSITION_COLS (and the appropriate COMPOSITION_TOTAL) for you."
      ],
      "metadata": {
        "id": "L7RIDUa09MOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) UTS only"
      ],
      "metadata": {
        "id": "r8vbEUwd-MA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Single-cell: Composition design for a chosen target property (or set)\n",
        "# ============================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ========== USER CONFIG ==========\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/composition_table.xlsx\" or \"/content/recipe_table.csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet inside DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level')\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # \"monomer_wt%\", \"crosslinker_wt%\", \"initiator_wt%\", \"solvent_wt%\", \"salt_mM\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum=constant\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) >>>>> CHANGE THIS FOR EACH TARGET <<<<<\n",
        "TARGETS: Dict[str, float] = {\n",
        "    \"UTS_kPa\": 90.0,    # example default; replace per the presets below\n",
        "}\n",
        "# =================================\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "def load_composition() -> pd.DataFrame:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# Build X (recipe features) and Y (targets)\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + list(TARGETS.keys())]\n",
        "    XY = XY.dropna(subset=COMPOSITION_COLS + list(TARGETS.keys()))\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    Y = XY[list(TARGETS.keys())].astype(float)\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\"] + list(TARGETS.keys())].dropna(subset=list(TARGETS.keys()))\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    Y = XY[list(TARGETS.keys())].astype(float)\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train. Ensure your chosen target has at least 2 valid samples.\")\n",
        "\n",
        "# Pipeline\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "reg = Pipeline([(\"prep\", preprocess), (\"rf\", MultiOutputRegressor(base))])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts>0:\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, Y_tr = X, Y\n",
        "    X_te, Y_te = X.iloc[:0], Y.iloc[:0]\n",
        "\n",
        "reg.fit(X_tr, Y_tr)\n",
        "if len(X_te)>0:\n",
        "    Y_hat = reg.predict(X_te)\n",
        "    print(\"=== Quick holdout ===\")\n",
        "    for i, col in enumerate(Y.columns):\n",
        "        print(f\"{col:>18}  R2={r2_score(Y_te.iloc[:, i], Y_hat[:, i]):6.3f}  MAE={mean_absolute_error(Y_te.iloc[:, i], Y_hat[:, i]):8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# Forward design (SLSQP with optional sum constraint)\n",
        "tgt = np.array([TARGETS[k] for k in Y.columns], float)\n",
        "\n",
        "# Separate bounds for numeric and handle categoricals\n",
        "bounds = []\n",
        "cont_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "discrete_enums = {c: sorted(X[c].unique().tolist()) for c in cat_cols}\n",
        "\n",
        "for c in cont_cols:\n",
        "    lo, hi = float(X[c].min()), float(X[c].max())\n",
        "    span = hi - lo if hi > lo else 1.0\n",
        "    bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "# Build a template row for prediction\n",
        "def build_row(cont_vec, cat_choice):\n",
        "    row = {}\n",
        "    j = 0\n",
        "    for col in X.columns:\n",
        "        if col in cat_cols:\n",
        "            row[col] = cat_choice[col]\n",
        "        else:\n",
        "            row[col] = float(cont_vec[j]); j += 1\n",
        "    return pd.DataFrame([row], columns=X.columns) # Ensure columns order matches X\n",
        "\n",
        "# Sum-to-constant constraint (if composition percentages)\n",
        "constraints = []\n",
        "if MODE == \"composition\" and COMPOSITION_TOTAL is not None:\n",
        "    def sum_eq(cont_vec, cont_cols=cont_cols, total=COMPOSITION_TOTAL):\n",
        "        # This assumes cont_cols are the composition components that sum\n",
        "        return float(np.sum(cont_vec) - total)\n",
        "    constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "# penalty for going outside historical range (soft corners)\n",
        "if cont_cols:\n",
        "    mins = X[cont_cols].min().values\n",
        "    maxs = X[cont_cols].max().values\n",
        "else:\n",
        "    mins = np.array([])\n",
        "    maxs = np.array([])\n",
        "\n",
        "\n",
        "def objective(cont_vec, cat_choice):\n",
        "    row = build_row(cont_vec, cat_choice)\n",
        "    pred = reg.predict(row)[0]\n",
        "    # Weighted MSE + MAPE on targets\n",
        "    var = np.var(Y_tr.values, axis=0) if len(Y_tr) else np.var(Y.values, axis=0)\n",
        "    var = var if var.size == pred.size else np.ones_like(pred)  # safeguard\n",
        "    w = 1.0 / np.where(var <= 1e-12, 1.0, var)\n",
        "    eps = 1e-8\n",
        "    # align only the target indices (Y order == target_keys order)\n",
        "    # our Y_tr columns equal target_keys in order already\n",
        "    y_pred_targets = pred[[Y.columns.get_loc(k) for k in target_keys]] # Ensure target columns are selected from prediction\n",
        "    mse = np.mean(w * (y_pred_targets - tgt)**2)\n",
        "    mape = np.mean(np.abs(y_pred_targets - tgt)/(np.abs(tgt)+eps))\n",
        "    base_loss = 0.5*mse + 0.5*mape\n",
        "\n",
        "    # Soft penalty if outside observed domain (only for continuous features)\n",
        "    penalty = 0.0\n",
        "    if cont_cols:\n",
        "        over_low = np.maximum(0.0, mins - cont_vec)\n",
        "        over_high = np.maximum(0.0, cont_vec - maxs)\n",
        "        penalty = 1e-3 * np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    return base_loss + penalty\n",
        "\n",
        "\n",
        "# Initial guess for continuous: median of continuous columns\n",
        "x0_cont = X[cont_cols].median().values if cont_cols else np.array([])\n",
        "\n",
        "# Enumerate categoricals; minimize continuous for each category combination\n",
        "from scipy.optimize import minimize\n",
        "best = {\"loss\": np.inf, \"row\": None, \"yhat\": None}\n",
        "\n",
        "cat_product = [{}]\n",
        "if cat_cols:\n",
        "    import itertools\n",
        "    cat_values = [discrete_enums[c] for c in cat_cols]\n",
        "    cat_product = [{c: v for c, v in zip(cat_cols, combo)} for combo in itertools.product(*cat_values)]\n",
        "\n",
        "\n",
        "for cat_choice in cat_product:\n",
        "    if not cont_cols:\n",
        "        # No continuous features — just evaluate categorical combo\n",
        "        dfrow = build_row(np.array([]), cat_choice)\n",
        "        loss, yhat = objective(np.array([]), cat_choice), reg.predict(dfrow)[0] # Evaluate loss with objective directly\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "        continue\n",
        "\n",
        "    def _wrap(cont_vec):\n",
        "        return objective(cont_vec, cat_choice)\n",
        "\n",
        "    result = minimize(\n",
        "        _wrap, x0_cont,\n",
        "        method=\"SLSQP\",\n",
        "        bounds=bounds,\n",
        "        constraints=constraints,\n",
        "        options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False}\n",
        "    )\n",
        "\n",
        "    if result.success:\n",
        "        loss = result.fun\n",
        "        dfrow = build_row(result.x, cat_choice)\n",
        "        yhat = reg.predict(dfrow)[0]\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "    else:\n",
        "        print(f\"[Warn] Optimization failed for {cat_choice}: {result.message}\")\n",
        "\n",
        "\n",
        "if best[\"row\"] is None:\n",
        "     raise RuntimeError(\"Optimization failed for all combinations.\")\n",
        "\n",
        "\n",
        "opt_recipe = best[\"row\"]\n",
        "opt_pred = pd.DataFrame([best[\"yhat\"]], columns=Y.columns) # Use Y.columns for full prediction columns\n",
        "report = pd.concat({\"recipe_opt\": opt_recipe.reset_index(drop=True), \"predicted_targets\": opt_pred[target_keys].reset_index(drop=True)}, axis=1) # Report only target columns\n",
        "\n",
        "\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "if MODE == \"composition\":\n",
        "    print(\"Mode: COMPOSITION → PROPERTIES\")\n",
        "    print(\"\\nRecommended composition:\")\n",
        "    display_cols = COMPOSITION_COLS\n",
        "else:\n",
        "    print(\"Mode: LABEL FALLBACK (no composition provided)\")\n",
        "    print(\"\\nRecommended knobs parsed from label (not true chemistry):\")\n",
        "    display_cols = X.columns.tolist()\n",
        "\n",
        "print(opt_recipe[display_cols].to_string(index=False))\n",
        "print(\"\\nPredicted target properties:\")\n",
        "print(report.to_string(index=False))\n",
        "\n",
        "# Save files\n",
        "props_out = \"/content/_derived_properties_table.csv\"\n",
        "opt_out = \"/content/_optimal_recipe_and_predictions.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "report.to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# Helpful tip: if you get \"not enough rows\", print coverage per target:\n",
        "# print(props_df[target_keys].notna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yGmYXuy7OnH",
        "outputId": "4c0f0304-eb9d-4368-deec-86e569d19134"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3754016808.py:95: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Quick holdout ===\n",
            "           UTS_kPa  R2=-0.548  MAE=  13.855\n",
            "\n",
            "================ RESULTS ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended knobs parsed from label (not true chemistry):\n",
            "mat_type  mat_level\n",
            "     Cel        5.0\n",
            "\n",
            "Predicted target properties:\n",
            "recipe_opt           predicted_targets\n",
            "  mat_type mat_level           UTS_kPa\n",
            "       Cel       5.0         61.194722\n",
            "\n",
            "Saved:\n",
            "  /content/_derived_properties_table.csv\n",
            "  /content/_optimal_recipe_and_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a single, copy-paste Colab cell tailored to optimize Toughness only. It:\n",
        "\n",
        "Reads your stress–strain file from Google Drive (path you gave)\n",
        "\n",
        "Derives mechanical properties per curve\n",
        "\n",
        "Trains a recipe → Toughness predictor\n",
        "\n",
        "Designs a recipe/composition to hit your target Toughness_kJ_m3\n",
        "\n",
        "Works with either a true composition table (recommended) or a fallback using mat_type + mat_level parsed from labels\n",
        "\n",
        "How to use:\n",
        "\n",
        "Make sure your file exists at the path you gave.\n",
        "\n",
        "(Optional) If you have a composition table, fill COMPOSITION_PATH (or COMPOSITION_SHEET) and list its numeric columns in COMPOSITION_COLS (e.g., monomer %, crosslinker %, …). Set COMPOSITION_TOTAL=100.0 if they are wt%.\n",
        "\n",
        "Set your toughness target in TARGET_TOUGHNESS. Run the cel"
      ],
      "metadata": {
        "id": "WWks_umw_B1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Toughness only"
      ],
      "metadata": {
        "id": "YzxBuDaS-JvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# One-cell: Toughness-only composition design\n",
        "# ============================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive (needed for the provided path) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE: separate file OR sheet within DATA_PATH)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level')\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition cols are wt% (or any total constraint), set the target sum (e.g., 100.0). Else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None\n",
        "\n",
        "# 5) Toughness target (kJ/m^3). Set your desired value here:\n",
        "TARGET_TOUGHNESS = 7.0\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Your data pattern looks like: [0, 500, 1000, ...] -> 0.0%, 5.0%, 10.0% -> 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))  # kJ/m^3 if stress in kPa and strain is fraction\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and Y (toughness) ----------\n",
        "TARGET_KEY = \"Toughness_kJ_m3\"\n",
        "TARGETS = {TARGET_KEY: TARGET_TOUGHNESS} # Ensure TARGETS dict is created\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    Y = XY[[TARGET_KEY]].astype(float)\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    Y = XY[[TARGET_KEY]].astype(float)\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for Toughness. Ensure at least 2 valid samples with non-NaN Toughness and recipe info.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "reg = Pipeline([(\"prep\", preprocess), (\"rf\", MultiOutputRegressor(base))])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, Y_tr = X, Y\n",
        "    X_te, Y_te = X.iloc[:0], Y.iloc[:0]\n",
        "\n",
        "reg.fit(X_tr, Y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    Y_hat = reg.predict(X_te)\n",
        "    r2 = r2_score(Y_te.iloc[:, 0], Y_hat[:, 0])\n",
        "    mae = mean_absolute_error(Y_te.iloc[:, 0], Y_hat[:, 0])\n",
        "    print(f\"[Holdout] Toughness  R2={r2:6.3f}  MAE={mae:8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for Toughness ----------\n",
        "tgt = np.array([TARGET_TOUGHNESS], float)  # single target\n",
        "\n",
        "# Separate bounds for numeric and handle categoricals\n",
        "bounds = []\n",
        "cont_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
        "cat_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
        "discrete_enums = {c: sorted(X[c].unique().tolist()) for c in cat_cols}\n",
        "\n",
        "for c in cont_cols:\n",
        "    lo, hi = float(X[c].min()), float(X[c].max())\n",
        "    span = hi - lo if hi > lo else 1.0\n",
        "    bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "# Build a template row for prediction\n",
        "def build_row(cont_vec, cat_choice):\n",
        "    row = {}\n",
        "    j = 0\n",
        "    for col in X.columns:\n",
        "        if col in cat_cols:\n",
        "            row[col] = cat_choice[col]\n",
        "        else:\n",
        "            row[col] = float(cont_vec[j]); j += 1\n",
        "    return pd.DataFrame([row], columns=X.columns) # Ensure columns order matches X\n",
        "\n",
        "# Sum-to-constant constraint (if composition percentages)\n",
        "constraints = []\n",
        "if MODE == \"composition\" and COMPOSITION_TOTAL is not None:\n",
        "    def sum_eq(cont_vec, cont_cols=cont_cols, total=COMPOSITION_TOTAL):\n",
        "        # This assumes cont_cols are the composition components that sum\n",
        "        return float(np.sum(cont_vec) - total)\n",
        "    constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "# penalty for going outside historical range (soft corners)\n",
        "if cont_cols:\n",
        "    mins = X[cont_cols].min().values\n",
        "    maxs = X[cont_cols].max().values\n",
        "else:\n",
        "    mins = np.array([])\n",
        "    maxs = np.array([])\n",
        "\n",
        "\n",
        "def objective(x, cat_choice): # Modified to accept cat_choice\n",
        "    row = build_row(x, cat_choice) # Use build_row with cat_choice\n",
        "    pred = reg.predict(row)[0]   # shape (1,)\n",
        "    # Weighted MSE + MAPE (weights from train variance, robust to scale)\n",
        "    var = np.var(Y_tr.values, axis=0) if len(Y_tr) else np.var(Y.values, axis=0)\n",
        "    w = 1.0 / np.where(var <= 1e-12, 1.0, var)\n",
        "    eps = 1e-8\n",
        "    # Use Y.columns directly to slice prediction\n",
        "    y_pred_targets = pred[[Y.columns.get_loc(k) for k in Y.columns]]\n",
        "    mse = np.mean(w * (y_pred_targets - tgt)**2)\n",
        "    mape = np.mean(np.abs(y_pred_targets - tgt) / (np.abs(tgt) + eps))\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    # Soft penalty outside observed domain\n",
        "    over_low = np.maximum(0.0, mins - x)\n",
        "    over_high = np.maximum(0.0, x - maxs)\n",
        "    return base + 1e-3 * np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "# Start from median composition/knob values\n",
        "x0_cont = X[cont_cols].median().values if cont_cols else np.array([])\n",
        "\n",
        "# Enumerate categoricals; minimize continuous for each category combination\n",
        "from scipy.optimize import minimize\n",
        "best = {\"loss\": np.inf, \"row\": None, \"yhat\": None}\n",
        "\n",
        "cat_product = [{}]\n",
        "if cat_cols:\n",
        "    import itertools\n",
        "    cat_values = [discrete_enums[c] for c in cat_cols]\n",
        "    cat_product = [{c: v for c, v in zip(cat_cols, combo)} for combo in itertools.product(*cat_values)]\n",
        "\n",
        "for cat_choice in cat_product:\n",
        "    if not cont_cols:\n",
        "        # No continuous features — just evaluate categorical combo\n",
        "        dfrow = build_row(np.array([]), cat_choice)\n",
        "        # Evaluate loss with objective directly, passing empty array for continuous and the cat_choice\n",
        "        loss = objective(np.array([]), cat_choice)\n",
        "        yhat = reg.predict(dfrow)[0]\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "        continue\n",
        "\n",
        "    def _wrap(x):\n",
        "        return objective(x, cat_choice) # Pass cat_choice to objective\n",
        "\n",
        "    result = minimize(\n",
        "        _wrap, x0_cont,\n",
        "        method=\"SLSQP\",\n",
        "        bounds=bounds,\n",
        "        constraints=constraints,\n",
        "        options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False}\n",
        "    )\n",
        "\n",
        "    if result.success:\n",
        "        loss = result.fun\n",
        "        dfrow = build_row(result.x, cat_choice)\n",
        "        yhat = reg.predict(dfrow)[0]\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": dfrow, \"yhat\": yhat})\n",
        "    else:\n",
        "        print(f\"[Warn] Optimization failed for {cat_choice}: {result.message}\")\n",
        "\n",
        "\n",
        "if best[\"row\"] is None:\n",
        "     raise RuntimeError(\"Optimization failed for all combinations.\")\n",
        "\n",
        "\n",
        "opt_recipe = best[\"row\"]\n",
        "opt_pred = pd.DataFrame([best[\"yhat\"]], columns=Y.columns) # Use Y.columns for full prediction columns\n",
        "\n",
        "print(\"\\n================ RESULTS (Toughness only) ================\")\n",
        "print(f\"Mode: {'COMPOSITION → PROPERTIES' if MODE=='composition' else 'LABEL FALLBACK (no composition provided)'}\")\n",
        "print(\"\\nRecommended recipe:\")\n",
        "display_cols = COMPOSITION_COLS if MODE == \"composition\" else X.columns.tolist() # Define display_cols here\n",
        "print(opt_recipe[display_cols].to_string(index=False))\n",
        "print(\"\\nPredicted Toughness_kJ_m3:\")\n",
        "print(opt_pred[[TARGET_KEY]].to_string(index=False)) # Print only the target key\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_Toughness.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_recipe.reset_index(drop=True),\n",
        "           \"predicted_Toughness\": opt_pred[[TARGET_KEY]].reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False) # Save only the target key in the report\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you hit \"Not enough rows\", check coverage:\n",
        "# print(props_df[[\"Toughness_kJ_m3\"]].notna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tRpbb_C9Noc",
        "outputId": "d5ca9430-fd45-4cfa-fedd-4bb1f725b8e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-505269262.py:98: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))  # kJ/m^3 if stress in kPa and strain is fraction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Holdout] Toughness  R2=-0.320  MAE=   0.835\n",
            "\n",
            "================ RESULTS (Toughness only) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe:\n",
            "mat_type  mat_level\n",
            "     Cel        5.0\n",
            "\n",
            "Predicted Toughness_kJ_m3:\n",
            " Toughness_kJ_m3\n",
            "        4.537019\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_Toughness.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Strain at UTS only"
      ],
      "metadata": {
        "id": "foJdf5Ho_ToM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "# One-cell: Strain at UTS only (design recipe/composition)\n",
        "# ===================================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive for your path ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE): separate file OR a sheet inside DATA_PATH\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level').\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum target; else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) Target STRAIN AT UTS (fraction). Example: 0.18 = 18% strain.\n",
        "TARGET_STRAIN_UTS = 0.18\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Data pattern: 0, 500, 1000 -> 0.0%, 5.0%, 10.0% -> 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        # Core properties\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and Y (strain at UTS) ----------\n",
        "TARGET_KEY = \"Strain_UTS_frac\"  # single target\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    y = XY[TARGET_KEY].astype(float).values  # 1D vector\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for Strain_UTS_frac. Need at least 2 valid samples.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "reg = Pipeline([(\"prep\", preprocess),\n",
        "                (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, y_tr = X, y\n",
        "    X_te, y_te = X.iloc[:0], np.array([])\n",
        "\n",
        "reg.fit(X_tr, y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    y_hat = reg.predict(X_te)\n",
        "    print(f\"[Holdout] Strain_UTS_frac  R2={r2_score(y_te, y_hat):6.3f}  MAE={mean_absolute_error(y_te, y_hat):8.4f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for Strain_UTS_frac ----------\n",
        "tgt = float(TARGET_STRAIN_UTS)\n",
        "mins, maxs = X.min().values, X.max().values\n",
        "\n",
        "# Helper to evaluate loss for a candidate row\n",
        "def _loss_from_row(dfrow):\n",
        "    pred = float(reg.predict(dfrow).ravel()[0])\n",
        "    # Weighted MSE + MAPE (robust)\n",
        "    var = float(np.var(y_tr)) if len(y_tr) else float(np.var(y))\n",
        "    w = 1.0 if var <= 1e-12 else 1.0/var\n",
        "    eps = 1e-8\n",
        "    mse = w * (pred - tgt)**2\n",
        "    mape = abs(pred - tgt) / (abs(tgt) + eps)\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    return base, pred\n",
        "\n",
        "# ===== Case A: composition mode — continuous optimization over composition columns =====\n",
        "if MODE == \"composition\":\n",
        "    # Bounds with ±5% padding\n",
        "    bounds = []\n",
        "    for c in X.columns:\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "    # Sum-to-constant constraint if set (e.g., wt% totals to 100)\n",
        "    constraints = []\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        def sum_eq(x, total=COMPOSITION_TOTAL):\n",
        "            return float(np.sum(x) - total)\n",
        "        constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "    def objective(x):\n",
        "        row = pd.DataFrame([x], columns=X.columns)\n",
        "        base, _ = _loss_from_row(row)\n",
        "        # Soft penalty for going outside historical domain\n",
        "        over_low = np.maximum(0.0, mins - x)\n",
        "        over_high = np.maximum(0.0, x - maxs)\n",
        "        return base + 1e-3*np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    x0 = X.median().values\n",
        "    res = minimize(objective, x0, bounds=bounds, constraints=constraints, method=\"SLSQP\",\n",
        "                   options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False})\n",
        "    x_opt = res.x\n",
        "    opt_row = pd.DataFrame([x_opt], columns=X.columns)\n",
        "    _, pred_val = _loss_from_row(opt_row)\n",
        "    mode_label = \"COMPOSITION → PROPERTIES\"\n",
        "\n",
        "# ===== Case B: label_fallback — enumerate mat_type (discrete) + optimize mat_level (continuous) =====\n",
        "else:\n",
        "    mat_types = sorted(X[\"mat_type\"].unique().tolist())\n",
        "    level_min, level_max = float(X[\"mat_level\"].min()), float(X[\"mat_level\"].max())\n",
        "    span = max(1e-9, level_max - level_min)\n",
        "    bounds = [(level_min - 0.05*span, level_max + 0.05*span)]\n",
        "\n",
        "    best = {\"loss\": np.inf, \"row\": None, \"pred\": None}\n",
        "\n",
        "    for mt in mat_types:\n",
        "        def objective(x):\n",
        "            row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(x[0])}])\n",
        "            base, _ = _loss_from_row(row)\n",
        "            # Soft penalty outside observed level range\n",
        "            over_low = max(0.0, level_min - x[0])\n",
        "            over_high = max(0.0, x[0] - level_max)\n",
        "            return base + 1e-3*(over_low**2 + over_high**2)\n",
        "\n",
        "        x0 = np.array([(level_min + level_max)/2.0])\n",
        "        res = minimize(objective, x0, bounds=bounds, method=\"SLSQP\",\n",
        "                       options={\"maxiter\": 500, \"ftol\": 1e-9, \"disp\": False})\n",
        "        row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(res.x[0])}])\n",
        "        loss, pred = _loss_from_row(row)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": row, \"pred\": pred})\n",
        "\n",
        "    opt_row = best[\"row\"]\n",
        "    pred_val = best[\"pred\"]\n",
        "    mode_label = \"LABEL FALLBACK (no composition provided)\"\n",
        "\n",
        "# ---------- Report & save ----------\n",
        "print(\"\\n================ RESULTS (Strain at UTS only) ================\")\n",
        "print(f\"Mode: {mode_label}\")\n",
        "print(\"\\nRecommended recipe (composition or knobs):\")\n",
        "print(opt_row.to_string(index=False))\n",
        "print(\"\\nPredicted Strain_UTS_frac:\")\n",
        "print(pd.DataFrame([pred_val], columns=[TARGET_KEY]).to_string(index=False))\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_StrainUTS.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_row.reset_index(drop=True),\n",
        "           \"predicted_Strain_UTS\": pd.DataFrame([pred_val], columns=[TARGET_KEY]).reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you get \"Not enough rows\", check coverage:\n",
        "# print(props_df[[TARGET_KEY]].notna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwaoPQIT_Hiq",
        "outputId": "9304863a-94c4-44bc-d489-09e99dfc962b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-586796648.py:97: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n",
            "[Holdout] Strain_UTS_frac  R2= 0.065  MAE=  0.0071\n",
            "\n",
            "================ RESULTS (Strain at UTS only) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe (composition or knobs):\n",
            "mat_type  mat_level\n",
            "     Cel       10.0\n",
            "\n",
            "Predicted Strain_UTS_frac:\n",
            " Strain_UTS_frac\n",
            "        0.156046\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_StrainUTS.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have real composition fields (e.g., AAm_wt, BIS_wt, APS_wt, …), list them in COMPOSITION_COLS and keep COMPOSITION_TOTAL=100.0 if they are wt%. The output then becomes a true chemical recipe satisfying the sum constraint.\n",
        "\n",
        "Without composition, the code optimizes label-derived knobs (mat_type, mat_level) instead—useful for exploration but not a full chemistry prescription.\n",
        "\n",
        "Start with a realistic TARGET_STRAIN_UTS within your dataset’s observed range for best results; then explore beyond with caution."
      ],
      "metadata": {
        "id": "qOnTz_BoAsOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Initial modulus (0–5%) only\n",
        "Here’s a single, copy-paste Colab cell to design a recipe for a target Initial Modulus (0–5%) (E0_5_kPa). It:\n",
        "\n",
        "reads your file from Google Drive (/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx),\n",
        "\n",
        "derives properties (including a robust calculation of E0_5_kPa),\n",
        "\n",
        "trains a model (recipe → E0_5_kPa), and\n",
        "\n",
        "finds a recipe/composition that hits your target.\n",
        "\n",
        "It supports either a true composition table (recommended) or a fallback using mat_type + mat_level parsed from labels.\n",
        "\n",
        "Set your target here: TARGET_E0_5_KPA = 550.0 (example).\n",
        "If you have an explicit composition table, fill COMPOSITION_PATH or COMPOSITION_SHEET and list numeric columns in COMPOSITION_COLS."
      ],
      "metadata": {
        "id": "dM0jRGSY_8pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# One-cell: Initial Modulus (0–5%) only (E0_5_kPa)\n",
        "# Recipe/composition design to hit a target E0_5\n",
        "# =====================================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive for your provided path ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE: separate file OR sheet within DATA_PATH)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level').\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum target; else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) Target Initial Modulus (0–5%), in kPa:\n",
        "TARGET_E0_5_KPA = 550.0\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Typical pattern: 0, 500, 1000 → 0.0%, 5.0%, 10.0% → 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    \"\"\"\n",
        "    Linear fit of σ(ε) in [a,b] using a dense interpolation (20 pts).\n",
        "    Returns slope (kPa per strain-fraction) and intercept.\n",
        "    If the full [0, 0.05] window isn't available, we shrink to what's available (down to 1%).\n",
        "    \"\"\"\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a_eff = max(lo, a)\n",
        "    b_eff = min(hi, b)\n",
        "    if b_eff - a_eff < 0.01:  # need at least ~1% strain span for a stable fit\n",
        "        return None\n",
        "    xs = np.linspace(a_eff, b_eff, 20)\n",
        "    ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a tidy table with per-label properties.\n",
        "    Crucially, E0_5_kPa is computed by fitting σ(ε) on [0, min(0.05, ε_max)], with a minimum 0.01 window.\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3:\n",
        "            continue\n",
        "\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        # --- Initial modulus: fit over [0, min(0.05, hi)]\n",
        "        fit = linear_fit_window(eps, sig, 0.00, min(0.05, hi))\n",
        "        E0_5 = fit[0] if fit is not None else np.nan\n",
        "\n",
        "        # (The rest are optional; computed for context but not used in training)\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and y (E0_5_kPa) ----------\n",
        "TARGET_KEY = \"E0_5_kPa\"  # single target\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for E0_5_kPa. Need at least 2 valid samples.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "reg = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, y_tr = X, y\n",
        "    X_te, y_te = X.iloc[:0], np.array([])\n",
        "\n",
        "reg.fit(X_tr, y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    y_hat = reg.predict(X_te)\n",
        "    print(f\"[Holdout] E0_5_kPa  R2={r2_score(y_te, y_hat):6.3f}  MAE={mean_absolute_error(y_te, y_hat):8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for E0_5_kPa ----------\n",
        "tgt = float(TARGET_E0_5_KPA)\n",
        "mins, maxs = X.min().values, X.max().values\n",
        "\n",
        "def _loss_from_row(dfrow):\n",
        "    pred = float(reg.predict(dfrow).ravel()[0])\n",
        "    # Weighted MSE + MAPE (robust scaling)\n",
        "    var = float(np.var(y_tr)) if len(y_tr) else float(np.var(y))\n",
        "    w = 1.0 if var <= 1e-12 else 1.0/var\n",
        "    eps = 1e-8\n",
        "    mse = w * (pred - tgt)**2\n",
        "    mape = abs(pred - tgt) / (abs(tgt) + eps)\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    return base, pred\n",
        "\n",
        "# ===== Case A: composition mode — continuous optimization over composition columns =====\n",
        "if MODE == \"composition\":\n",
        "    # Bounds with ±5% padding of historical range\n",
        "    bounds = []\n",
        "    for c in X.columns:\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "    # Sum-to-constant constraint if set (e.g., wt% totals to 100)\n",
        "    constraints = []\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        def sum_eq(x, total=COMPOSITION_TOTAL):\n",
        "            return float(np.sum(x) - total)\n",
        "        constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "    def objective(x):\n",
        "        row = pd.DataFrame([x], columns=X.columns)\n",
        "        base, _ = _loss_from_row(row)\n",
        "        # Soft penalty outside observed domain\n",
        "        over_low = np.maximum(0.0, mins - x)\n",
        "        over_high = np.maximum(0.0, x - maxs)\n",
        "        return base + 1e-3*np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    x0 = X.median().values\n",
        "    res = minimize(objective, x0, bounds=bounds, constraints=constraints, method=\"SLSQP\",\n",
        "                   options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False})\n",
        "    x_opt = res.x\n",
        "    opt_row = pd.DataFrame([x_opt], columns=X.columns)\n",
        "    _, pred_val = _loss_from_row(opt_row)\n",
        "    mode_label = \"COMPOSITION → PROPERTIES\"\n",
        "\n",
        "# ===== Case B: label_fallback — enumerate mat_type (discrete) + optimize mat_level (continuous) =====\n",
        "else:\n",
        "    mat_types = sorted(X[\"mat_type\"].unique().tolist())\n",
        "    level_min, level_max = float(X[\"mat_level\"].min()), float(X[\"mat_level\"].max())\n",
        "    span = max(1e-9, level_max - level_min)\n",
        "    bounds = [(level_min - 0.05*span, level_max + 0.05*span)]\n",
        "\n",
        "    best = {\"loss\": np.inf, \"row\": None, \"pred\": None}\n",
        "\n",
        "    for mt in mat_types:\n",
        "        def objective(x):\n",
        "            row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(x[0])}])\n",
        "            base, _ = _loss_from_row(row)\n",
        "            # Soft penalty outside observed level range\n",
        "            over_low = max(0.0, level_min - x[0])\n",
        "            over_high = max(0.0, x[0] - level_max)\n",
        "            return base + 1e-3*(over_low**2 + over_high**2)\n",
        "\n",
        "        x0 = np.array([(level_min + level_max)/2.0])\n",
        "        res = minimize(objective, x0, bounds=bounds, method=\"SLSQP\",\n",
        "                       options={\"maxiter\": 500, \"ftol\": 1e-9, \"disp\": False})\n",
        "        row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(res.x[0])}])\n",
        "        loss, pred = _loss_from_row(row)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": row, \"pred\": pred})\n",
        "\n",
        "    opt_row = best[\"row\"]\n",
        "    pred_val = best[\"pred\"]\n",
        "    mode_label = \"LABEL FALLBACK (no composition provided)\"\n",
        "\n",
        "# ---------- Report & save ----------\n",
        "print(\"\\n================ RESULTS (Initial Modulus 0–5%) ================\")\n",
        "print(f\"Mode: {mode_label}\")\n",
        "print(\"\\nRecommended recipe (composition or knobs):\")\n",
        "print(opt_row.to_string(index=False))\n",
        "print(\"\\nPredicted E0_5_kPa:\")\n",
        "print(pd.DataFrame([pred_val], columns=[TARGET_KEY]).to_string(index=False))\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_E0_5.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_row.reset_index(drop=True),\n",
        "           \"predicted_E0_5_kPa\": pd.DataFrame([pred_val], columns=[TARGET_KEY]).reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you get \"Not enough rows\", check coverage:\n",
        "# print(props_df[[TARGET_KEY]].notna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64NhDCGf_Hlz",
        "outputId": "496b5902-c456-4d09-84fa-7c2cd35b9008"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1708404795.py:105: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Holdout] E0_5_kPa  R2=-0.655  MAE=  40.612\n",
            "\n",
            "================ RESULTS (Initial Modulus 0–5%) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe (composition or knobs):\n",
            "mat_type  mat_level\n",
            "     Cel       10.0\n",
            "\n",
            "Predicted E0_5_kPa:\n",
            "  E0_5_kPa\n",
            "384.896667\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_E0_5.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes\n",
        "\n",
        "If you have real composition fields (e.g., AAm_wt, BIS_wt, APS_wt, …), list them in COMPOSITION_COLS and keep COMPOSITION_TOTAL=100.0 if they are wt%. Then the output becomes a true chemical composition satisfying the sum constraint.\n",
        "\n",
        "Without composition, the code optimizes label-derived knobs (mat_type, mat_level) instead—useful for exploration but not a full chemistry prescription.\n",
        "\n",
        "For best performance, choose a target E0_5_kPa within the observed range first; then explore beyond it."
      ],
      "metadata": {
        "id": "hnT3JKV3BVyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Stress at 10% strain only"
      ],
      "metadata": {
        "id": "w2qXt4RhAAtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a single, copy-paste Colab cell to design a recipe for a target stress at 10% strain (Stress@10%_kPa). It:\n",
        "\n",
        "reads your file from Google Drive (/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx),\n",
        "\n",
        "derives properties (including Stress@10%_kPa),\n",
        "\n",
        "trains a model (recipe → Stress@10%_kPa), and\n",
        "\n",
        "finds a recipe/composition that hits your target.\n",
        "\n",
        "It supports either a true composition table (recommended) or a fallback using mat_type + mat_level parsed from the stress–strain labels.\n",
        "\n",
        "Set your target here: TARGET_STRESS_10 = 60.0 (example, in kPa).\n",
        "If you have a composition table, fill COMPOSITION_PATH or COMPOSITION_SHEET and list numeric columns in COMPOSITION_COLS."
      ],
      "metadata": {
        "id": "HQ0nMWn7BX-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# One-cell: Stress at 10% strain only (Stress@10%_kPa)\n",
        "# Recipe/composition design to hit a target Stress@10%\n",
        "# =========================================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive for your provided path ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE: separate file OR sheet within DATA_PATH)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level').\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum target; else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) Target Stress at 10% strain (kPa):\n",
        "TARGET_STRESS_10 = 60.0\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Pattern: 0, 500, 1000 → 0.0%, 5.0%, 10.0% → 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, 0.05) or (np.nan, np.nan))[0]\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "\n",
        "        s5  = stress_at(f, 5, lo, hi)\n",
        "        s10 = stress_at(f, 10, lo, hi)    # <-- Stress at 10% strain\n",
        "        s15 = stress_at(f, 15, lo, hi)\n",
        "        s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5, \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and y (Stress@10%_kPa) ----------\n",
        "TARGET_KEY = \"Stress@10%_kPa\"  # single target\n",
        "\n",
        "if TARGET_KEY not in props_df.columns:\n",
        "    raise RuntimeError(\"Stress@10%_kPa not computed — your curves might not reach 10% strain.\")\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for Stress@10%_kPa. Need at least 2 valid samples with this property.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "reg = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, y_tr = X, y\n",
        "    X_te, y_te = X.iloc[:0], np.array([])\n",
        "\n",
        "reg.fit(X_tr, y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    y_hat = reg.predict(X_te)\n",
        "    print(f\"[Holdout] Stress@10%_kPa  R2={r2_score(y_te, y_hat):6.3f}  MAE={mean_absolute_error(y_te, y_hat):8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for Stress@10%_kPa ----------\n",
        "tgt = float(TARGET_STRESS_10)\n",
        "mins, maxs = X.min().values, X.max().values\n",
        "\n",
        "def _loss_from_row(dfrow):\n",
        "    pred = float(reg.predict(dfrow).ravel()[0])\n",
        "    # Weighted MSE + MAPE (robust scaling)\n",
        "    var = float(np.var(y_tr)) if len(y_tr) else float(np.var(y))\n",
        "    w = 1.0 if var <= 1e-12 else 1.0/var\n",
        "    eps = 1e-8\n",
        "    mse = w * (pred - tgt)**2\n",
        "    mape = abs(pred - tgt) / (abs(tgt) + eps)\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    return base, pred\n",
        "\n",
        "# ===== Case A: composition mode — continuous optimization over composition columns =====\n",
        "if MODE == \"composition\":\n",
        "    # Bounds with ±5% padding of historical range\n",
        "    bounds = []\n",
        "    for c in X.columns:\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "    # Sum-to-constant constraint if set (e.g., wt% totals to 100)\n",
        "    constraints = []\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        def sum_eq(x, total=COMPOSITION_TOTAL):\n",
        "            return float(np.sum(x) - total)\n",
        "        constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "    def objective(x):\n",
        "        row = pd.DataFrame([x], columns=X.columns)\n",
        "        base, _ = _loss_from_row(row)\n",
        "        # Soft penalty outside observed domain\n",
        "        over_low = np.maximum(0.0, mins - x)\n",
        "        over_high = np.maximum(0.0, x - maxs)\n",
        "        return base + 1e-3*np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    x0 = X.median().values\n",
        "    res = minimize(objective, x0, bounds=bounds, constraints=constraints, method=\"SLSQP\",\n",
        "                   options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False})\n",
        "    x_opt = res.x\n",
        "    opt_row = pd.DataFrame([x_opt], columns=X.columns)\n",
        "    _, pred_val = _loss_from_row(opt_row)\n",
        "    mode_label = \"COMPOSITION → PROPERTIES\"\n",
        "\n",
        "# ===== Case B: label_fallback — enumerate mat_type (discrete) + optimize mat_level (continuous) =====\n",
        "else:\n",
        "    mat_types = sorted(X[\"mat_type\"].unique().tolist())\n",
        "    level_min, level_max = float(X[\"mat_level\"].min()), float(X[\"mat_level\"].max())\n",
        "    span = max(1e-9, level_max - level_min)\n",
        "    bounds = [(level_min - 0.05*span, level_max + 0.05*span)]\n",
        "\n",
        "    best = {\"loss\": np.inf, \"row\": None, \"pred\": None}\n",
        "\n",
        "    for mt in mat_types:\n",
        "        def objective(x):\n",
        "            row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(x[0])}])\n",
        "            base, _ = _loss_from_row(row)\n",
        "            # Soft penalty outside observed level range\n",
        "            over_low = max(0.0, level_min - x[0])\n",
        "            over_high = max(0.0, x[0] - level_max)\n",
        "            return base + 1e-3*(over_low**2 + over_high**2)\n",
        "\n",
        "        x0 = np.array([(level_min + level_max)/2.0])\n",
        "        res = minimize(objective, x0, bounds=bounds, method=\"SLSQP\",\n",
        "                       options={\"maxiter\": 500, \"ftol\": 1e-9, \"disp\": False})\n",
        "        row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(res.x[0])}])\n",
        "        loss, pred = _loss_from_row(row)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": row, \"pred\": pred})\n",
        "\n",
        "    opt_row = best[\"row\"]\n",
        "    pred_val = best[\"pred\"]\n",
        "    mode_label = \"LABEL FALLBACK (no composition provided)\"\n",
        "\n",
        "# ---------- Report & save ----------\n",
        "print(\"\\n================ RESULTS (Stress at 10% strain) ================\")\n",
        "print(f\"Mode: {mode_label}\")\n",
        "print(\"\\nRecommended recipe (composition or knobs):\")\n",
        "print(opt_row.to_string(index=False))\n",
        "print(\"\\nPredicted Stress@10%_kPa:\")\n",
        "print(pd.DataFrame([pred_val], columns=[TARGET_KEY]).to_string(index=False))\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_Stress10.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_row.reset_index(drop=True),\n",
        "           \"predicted_Stress10\": pd.DataFrame([pred_val], columns=[TARGET_KEY]).reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you get \"Not enough rows\", your curves might not reach 10% strain.\n",
        "# You can check coverage with:\n",
        "# print(props_df[[TARGET_KEY]].notna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMRno22H_Ho1",
        "outputId": "3390fc7b-b170-4567-ccb3-4b342800d60d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1753915685.py:98: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Holdout] Stress@10%_kPa  R2=-2.135  MAE=   4.558\n",
            "\n",
            "================ RESULTS (Stress at 10% strain) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe (composition or knobs):\n",
            "mat_type  mat_level\n",
            "     Cel       10.0\n",
            "\n",
            "Predicted Stress@10%_kPa:\n",
            " Stress@10%_kPa\n",
            "        35.2375\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_Stress10.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes\n",
        "\n",
        "For true chemical output, provide your composition table and list its numeric columns in COMPOSITION_COLS. Keep COMPOSITION_TOTAL=100.0 if those are wt%.\n",
        "\n",
        "Without composition, the optimizer proposes label-derived knobs (mat_type, mat_level) instead—useful for exploration but not full chemistry.\n",
        "\n",
        "If you see “Not enough rows,” it likely means many curves don’t reach 10% strain; try targeting Stress@5%_kPa first to confirm the pipeline, or expand your data that covers 10%."
      ],
      "metadata": {
        "id": "nOyQDvqcByLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EpqIRJonBXNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Yield stress (0.2% offset) only\n",
        "Here’s a single, copy-paste Colab cell to design a recipe for a target Yield stress (0.2% offset) using your Drive file:\n",
        "\n",
        "reads /content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\n",
        "\n",
        "derives Yield_stress_kPa (and Yield_strain_frac) via 0.2% offset using an initial 0–5% modulus fit\n",
        "\n",
        "trains a model (recipe → Yield_stress_kPa)\n",
        "\n",
        "optimizes a composition (if you provide a composition table) or label-fallback knobs (mat_type, mat_level) to hit your target\n",
        "\n",
        "Set your target here: TARGET_YIELD_STRESS = 25.0 (kPa).\n",
        "If you have a composition table, fill COMPOSITION_PATH or COMPOSITION_SHEET and list numeric columns in COMPOSITION_COLS (e.g., wt%). Keep COMPOSITION_TOTAL=100.0 if those are percentages."
      ],
      "metadata": {
        "id": "A93PzoIDABmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# One-cell: Yield stress (0.2% offset) only — design recipe\n",
        "# ============================================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive for your provided path ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE: separate file OR sheet within DATA_PATH)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level').\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum target; else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) Target Yield stress (kPa):\n",
        "TARGET_YIELD_STRESS = 25.0\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Pattern: 0, 500, 1000 → 0.0%, 5.0%, 10.0% → 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    \"\"\"\n",
        "    Linear fit of σ(ε) in [a,b] using dense interpolation.\n",
        "    If [0,0.05] isn't fully available, shrink to what's available, with a minimum 0.01 span.\n",
        "    \"\"\"\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a_eff, b_eff = max(lo, a), min(hi, b)\n",
        "    if b_eff - a_eff < 0.01:\n",
        "        return None\n",
        "    xs = np.linspace(a_eff, b_eff, 20)\n",
        "    ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    \"\"\"\n",
        "    0.2% offset yield: intersection of σ(ε) with line E_init*(ε - offset).\n",
        "    Returns (yield_strain, yield_stress) or (None, None) if no intersection.\n",
        "    \"\"\"\n",
        "    if E_init is None or not np.isfinite(E_init):\n",
        "        return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 600)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g)\n",
        "    idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0:\n",
        "        return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a tidy table with per-label properties.\n",
        "    Crucially, E0_5_kPa is computed over [0, min(0.05, ε_max)], with a minimum 0.01 span.\n",
        "    Yield is computed via 0.2% offset using that E0_5_kPa.\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3:\n",
        "            continue\n",
        "\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        # Initial modulus over [0, min(0.05, hi)] with min span 0.01\n",
        "        fit = linear_fit_window(eps, sig, 0.00, min(0.05, hi))\n",
        "        E0_5 = fit[0] if fit is not None else np.nan\n",
        "\n",
        "        # Yield via 0.2% offset\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "\n",
        "        # Extra properties (optional context)\n",
        "        E5_10 = (lambda f=f: (float((f(0.10)-f(0.05))/0.05) if hi>=0.10 else np.nan))()\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "        uts = float(sig.max()); strain_uts = float(eps[int(sig.argmax())])\n",
        "        frac_strain = float(eps.max()); frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        sec_0_15 = (lambda f=f: (float((f(0.15)-f(0.00))/0.15) if hi>=0.15 else np.nan))()\n",
        "\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            # Optional context fields (not used for training unless you want to)\n",
        "            \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10,\n",
        "            \"Resilience_kJ_m3\":  resilience,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain, \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"Secant_0_15_kPa\": sec_0_15\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and y (Yield_stress_kPa) ----------\n",
        "TARGET_KEY = \"Yield_stress_kPa\"  # single target\n",
        "\n",
        "if TARGET_KEY not in props_df.columns:\n",
        "    raise RuntimeError(\"Yield_stress_kPa not computed — curves may not permit 0.2% offset yield (or initial modulus window too small).\")\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for Yield_stress_kPa. Need at least 2 valid samples with this property.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "reg = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, y_tr = X, y\n",
        "    X_te, y_te = X.iloc[:0], np.array([])\n",
        "\n",
        "reg.fit(X_tr, y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    y_hat = reg.predict(X_te)\n",
        "    print(f\"[Holdout] Yield_stress_kPa  R2={r2_score(y_te, y_hat):6.3f}  MAE={mean_absolute_error(y_te, y_hat):8.3f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for Yield_stress_kPa ----------\n",
        "tgt = float(TARGET_YIELD_STRESS)\n",
        "# Ensure mins and maxs are correctly sliced for continuous columns\n",
        "if cont_cols: # Check if cont_cols is not empty\n",
        "  mins, maxs = X[cont_cols].min().values, X[cont_cols].max().values\n",
        "else:\n",
        "  mins, maxs = np.array([]), np.array([])\n",
        "\n",
        "\n",
        "def _loss_from_row(dfrow):\n",
        "    pred = float(reg.predict(dfrow).ravel()[0])\n",
        "    # Weighted MSE + MAPE (robust scaling)\n",
        "    var = float(np.var(y_tr)) if len(y_tr) else float(np.var(y))\n",
        "    w = 1.0 if var <= 1e-12 else 1.0/var\n",
        "    eps = 1e-8\n",
        "    mse = w * (pred - tgt)**2\n",
        "    mape = abs(pred - tgt) / (abs(tgt) + eps)\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    return base, pred\n",
        "\n",
        "# ===== Case A: composition mode — continuous optimization over composition columns =====\n",
        "if MODE == \"composition\":\n",
        "    # Bounds with ±5% padding of historical range\n",
        "    bounds = []\n",
        "    for c in X.columns:\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "    # Sum-to-constant constraint if set (e.g., wt% totals to 100)\n",
        "    constraints = []\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        def sum_eq(x, total=COMPOSITION_TOTAL):\n",
        "            return float(np.sum(x) - total)\n",
        "        constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "    def objective(x):\n",
        "        row = pd.DataFrame([x], columns=X.columns)\n",
        "        base, _ = _loss_from_row(row)\n",
        "        # Soft penalty outside observed domain\n",
        "        over_low = np.maximum(0.0, mins - x)\n",
        "        over_high = np.maximum(0.0, x - maxs)\n",
        "        return base + 1e-3*np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    x0 = X.median().values\n",
        "    res = minimize(objective, x0, bounds=bounds, constraints=constraints, method=\"SLSQP\",\n",
        "                   options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False})\n",
        "    x_opt = res.x\n",
        "    opt_row = pd.DataFrame([x_opt], columns=X.columns)\n",
        "    _, pred_val = _loss_from_row(opt_row)\n",
        "    mode_label = \"COMPOSITION → PROPERTIES\"\n",
        "\n",
        "# ===== Case B: label_fallback — enumerate mat_type (discrete) + optimize mat_level (continuous) =====\n",
        "else:\n",
        "    mat_types = sorted(X[\"mat_type\"].unique().tolist())\n",
        "    level_min, level_max = float(X[\"mat_level\"].min()), float(X[\"mat_level\"].max())\n",
        "    span = max(1e-9, level_max - level_min)\n",
        "    bounds = [(level_min - 0.05*span, level_max + 0.05*span)]\n",
        "\n",
        "    best = {\"loss\": np.inf, \"row\": None, \"pred\": None}\n",
        "\n",
        "    for mt in mat_types:\n",
        "        def objective(x):\n",
        "            row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(x[0])}])\n",
        "            base, _ = _loss_from_row(row)\n",
        "            # Soft penalty outside observed level range\n",
        "            over_low = max(0.0, level_min - x[0])\n",
        "            over_high = max(0.0, x[0] - level_max)\n",
        "            return base + 1e-3*(over_low**2 + over_high**2)\n",
        "\n",
        "        x0 = np.array([(level_min + level_max)/2.0])\n",
        "        res = minimize(objective, x0, bounds=bounds, method=\"SLSQP\",\n",
        "                       options={\"maxiter\": 500, \"ftol\": 1e-9, \"disp\": False})\n",
        "        row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(res.x[0])}])\n",
        "        loss, pred = _loss_from_row(row)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": row, \"pred\": pred})\n",
        "\n",
        "    opt_row = best[\"row\"]\n",
        "    pred_val = best[\"pred\"]\n",
        "    mode_label = \"LABEL FALLBACK (no composition provided)\"\n",
        "\n",
        "# ---------- Report & save ----------\n",
        "print(\"\\n================ RESULTS (Yield stress 0.2% offset) ================\")\n",
        "print(f\"Mode: {mode_label}\")\n",
        "print(\"\\nRecommended recipe (composition or knobs):\")\n",
        "print(opt_row.to_string(index=False))\n",
        "print(\"\\nPredicted Yield_stress_kPa:\")\n",
        "print(pd.DataFrame([pred_val], columns=[TARGET_KEY]).to_string(index=False))\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_YieldStress.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_row.reset_index(drop=True),\n",
        "           \"predicted_Yield_stress\": pd.DataFrame([pred_val], columns=[TARGET_KEY]).reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you get \"Not enough rows\", it means few curves have a resolvable 0.2% yield.\n",
        "# Try: print(props_df[['E0_5_kPa','Yield_stress_kPa','Yield_strain_frac']].notna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra2Gf3pC_Hr8",
        "outputId": "d5c3b6b7-9e18-4879-b270-2109efd2c0b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3797196544.py:105: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Holdout] Yield_stress_kPa  R2=-0.017  MAE=   4.404\n",
            "\n",
            "================ RESULTS (Yield stress 0.2% offset) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe (composition or knobs):\n",
            "mat_type  mat_level\n",
            "     Cel       10.0\n",
            "\n",
            "Predicted Yield_stress_kPa:\n",
            " Yield_stress_kPa\n",
            "        25.124281\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_YieldStress.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want the output to be a true chemical composition, provide COMPOSITION_COLS and keep COMPOSITION_TOTAL=100.0 for wt%.\n",
        "\n",
        "If many curves don’t resolve a 0.2% offset yield, consider tightening your data near 0–2% strain (denser sampling), or temporarily target a proxy like Stress@5%_kPa to validate the pipeline."
      ],
      "metadata": {
        "id": "w6YrHKH1CFrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Fracture strain only\n",
        "Here’s a single, copy-paste Colab cell to design a recipe for a target Fracture strain (Fracture_strain_frac). It:\n",
        "\n",
        "reads your Drive file,\n",
        "\n",
        "derives properties (including Fracture_strain_frac),\n",
        "\n",
        "trains a model (recipe → Fracture_strain_frac),\n",
        "\n",
        "optimizes either a true composition (if you provide a composition table) or the label-fallback knobs (mat_type, mat_level) to hit your target.\n",
        "\n",
        "Set your target here: TARGET_FRACTURE_STRAIN = 0.20 (example = 20% strain).\n",
        "If you have a composition table, fill COMPOSITION_PATH or COMPOSITION_SHEET and list numeric columns in COMPOSITION_COLS. Keep COMPOSITION_TOTAL=100.0 if those are wt%."
      ],
      "metadata": {
        "id": "sxak1PDQAD2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# One-cell: Fracture strain only (design recipe/composition)\n",
        "# Target property: Fracture_strain_frac  (unitless fraction)\n",
        "# =======================================================\n",
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "# --- Mount Google Drive for your provided path ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "# 1) Stress–strain file (paired columns: Strain(%)_<label>, Stress(kPa)_<label>)\n",
        "DATA_PATH = \"/content/drive/MyDrive/AI Training/paam_hydrogel_stress_strain_data_v2.xlsx\"\n",
        "\n",
        "# 2) Composition source (choose ONE: separate file OR sheet within DATA_PATH)\n",
        "COMPOSITION_PATH = \"\"                 # e.g., \"/content/drive/MyDrive/AI Training/composition_table.xlsx\" or \".csv\"\n",
        "COMPOSITION_SHEET = \"\"                # e.g., \"composition\" if stored as a sheet in DATA_PATH\n",
        "\n",
        "# 3) Composition columns (true chemistry knobs). Leave empty to fallback to ('mat_type','mat_level').\n",
        "COMPOSITION_COLS: List[str] = [\n",
        "    # Example: \"AAm_wt\", \"BIS_wt\", \"APS_wt\", \"TEMED_wt\", \"Water_wt\"\n",
        "]\n",
        "\n",
        "# 4) If composition columns are wt% (or any total constraint), set sum target; else set None.\n",
        "COMPOSITION_TOTAL = 100.0   # or None if not applicable\n",
        "\n",
        "# 5) Target Fracture strain (fraction). Example: 0.20 = 20% strain.\n",
        "TARGET_FRACTURE_STRAIN = 0.20\n",
        "# --------------------------------------------\n",
        "\n",
        "# ======== Stress–strain utilities ========\n",
        "def find_pairs(df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], list]:\n",
        "    strain_cols = [c for c in df.columns if c.lower().startswith(\"strain\")]\n",
        "    stress_cols = [c for c in df.columns if c.lower().startswith(\"stress\")]\n",
        "    def lab(c): return c.split(\"_\", 1)[1] if \"_\" in c else None\n",
        "    labels_strain = {lab(c): c for c in strain_cols if lab(c) is not None}\n",
        "    labels_stress = {lab(c): c for c in stress_cols if lab(c) is not None}\n",
        "    labels = sorted(set(labels_strain).intersection(labels_stress))\n",
        "    return labels_strain, labels_stress, labels\n",
        "\n",
        "def to_fraction(eps_raw: np.ndarray) -> np.ndarray:\n",
        "    # Pattern: 0, 500, 1000 → 0.0%, 5.0%, 10.0% → 0.0, 0.05, 0.10 (fraction)\n",
        "    return (eps_raw.astype(float)/100.0)/100.0\n",
        "\n",
        "def ensure_sorted(eps: np.ndarray, sig: np.ndarray):\n",
        "    idx = np.argsort(eps)\n",
        "    return eps[idx], sig[idx]\n",
        "\n",
        "def interp_curve(eps: np.ndarray, sig: np.ndarray):\n",
        "    x = np.asarray(eps, float); y = np.asarray(sig, float)\n",
        "    lo, hi = float(x.min()), float(x.max())\n",
        "    def f(xq):\n",
        "        xq = np.asarray(xq, float)\n",
        "        return np.interp(np.clip(xq, lo, hi), x, y)\n",
        "    return f, (lo, hi)\n",
        "\n",
        "def linear_fit_window(eps: np.ndarray, sig: np.ndarray, a: float, b: float):\n",
        "    if b <= a: return None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    a, b = max(lo, a), min(hi, b)\n",
        "    if b <= a: return None\n",
        "    xs = np.linspace(a, b, 20); ys = f(xs)\n",
        "    X = np.vstack([xs, np.ones_like(xs)]).T\n",
        "    slope, intercept = np.linalg.lstsq(X, ys, rcond=None)[0]\n",
        "    return float(slope), float(intercept)\n",
        "\n",
        "def secant_modulus(f, a: float, b: float):\n",
        "    if b <= a: return np.nan\n",
        "    return float((f(b) - f(a)) / (b - a))\n",
        "\n",
        "def yield_offset(eps: np.ndarray, sig: np.ndarray, E_init: Optional[float], offset: float = 0.002):\n",
        "    if E_init is None or not np.isfinite(E_init): return None, None\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    xs = np.linspace(lo, hi, 400)\n",
        "    g = f(xs) - E_init*(xs - offset)\n",
        "    s = np.sign(g); idx = np.where(np.diff(s) != 0)[0]\n",
        "    if len(idx) == 0: return None, None\n",
        "    i = idx[0]; x0, x1 = xs[i], xs[i+1]; y0, y1 = g[i], g[i+1]\n",
        "    eps_y = x0 if (y1 - y0) == 0 else x0 - y0*(x1 - x0)/(y1 - y0)\n",
        "    return float(eps_y), float(f(eps_y))\n",
        "\n",
        "def integrate_toughness(eps: np.ndarray, sig: np.ndarray, up_to: Optional[float] = None) -> float:\n",
        "    f, (lo, hi) = interp_curve(eps, sig)\n",
        "    b = hi if up_to is None else max(lo, min(hi, up_to))\n",
        "    xs = np.linspace(lo, b, 400)\n",
        "    return float(np.trapz(f(xs), xs))\n",
        "\n",
        "def stress_at(f, p: float, lo: float, hi: float):\n",
        "    x = p/100.0\n",
        "    return float(f(x)) if lo <= x <= hi else np.nan\n",
        "\n",
        "def parse_label_recipe(label: str):\n",
        "    import re\n",
        "    m = re.match(r\"([A-Za-z]+)(\\d+)?\", label)\n",
        "    mat_type = m.group(1) if m else None\n",
        "    mat_level = float(m.group(2)) if (m and m.group(2)) else 0.0\n",
        "    return mat_type, mat_level\n",
        "\n",
        "def derive_properties_table(stress_strain_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(stress_strain_path)\n",
        "    labels_strain, labels_stress, labels = find_pairs(df)\n",
        "    rows = []\n",
        "    for lab in labels:\n",
        "        eps_raw = df[labels_strain[lab]].to_numpy(dtype=float)\n",
        "        sig_raw = df[labels_stress[lab]].to_numpy(dtype=float)\n",
        "        m = np.isfinite(eps_raw) & np.isfinite(sig_raw)\n",
        "        eps_raw, sig_raw = eps_raw[m], sig_raw[m]\n",
        "        if len(eps_raw) < 3: continue\n",
        "        eps = to_fraction(eps_raw)\n",
        "        eps, sig = ensure_sorted(eps, sig_raw)\n",
        "        f, (lo, hi) = interp_curve(eps, sig)\n",
        "\n",
        "        # Initial modulus (for context; not used for training)\n",
        "        E0_5  = (linear_fit_window(eps, sig, 0.00, min(0.05, hi)) or (np.nan, np.nan))[0]\n",
        "        eps_y, sig_y = yield_offset(eps, sig, E0_5, offset=0.002)\n",
        "\n",
        "        # Key quantities\n",
        "        uts = float(sig.max()); i_uts = int(sig.argmax()); strain_uts = float(eps[i_uts])\n",
        "        frac_strain = float(eps.max())           # <-- Fracture strain (max recorded strain)\n",
        "        frac_stress = float(sig[-1])\n",
        "        toughness = integrate_toughness(eps, sig, None)\n",
        "\n",
        "        # Convenience probes\n",
        "        s5  = stress_at(f, 5, lo, hi); s10 = stress_at(f, 10, lo, hi)\n",
        "        s15 = stress_at(f, 15, lo, hi); s20 = stress_at(f, 20, lo, hi)\n",
        "        E5_10 = secant_modulus(f, 0.05, 0.10) if hi >= 0.10 else np.nan\n",
        "        TanE10 = (linear_fit_window(eps, sig, 0.08, 0.12) or (np.nan, np.nan))[0] if hi >= 0.12 else np.nan\n",
        "        sec_0_15 = secant_modulus(f, 0.00, 0.15) if hi >= 0.15 else np.nan\n",
        "        resilience = integrate_toughness(eps, sig, up_to=eps_y) if eps_y is not None else (integrate_toughness(eps, sig, up_to=0.02) if hi >= 0.02 else np.nan)\n",
        "\n",
        "        mat_type, mat_level = parse_label_recipe(lab)\n",
        "        rows.append({\n",
        "            \"label\": lab, \"mat_type\": mat_type, \"mat_level\": mat_level,\n",
        "            \"E0_5_kPa\": E0_5,\n",
        "            \"Yield_strain_frac\": eps_y if eps_y is not None else np.nan,\n",
        "            \"Yield_stress_kPa\":  sig_y if sig_y is not None else np.nan,\n",
        "            \"UTS_kPa\": uts, \"Strain_UTS_frac\": strain_uts,\n",
        "            \"Fracture_strain_frac\": frac_strain,   # <-- target column\n",
        "            \"Fracture_stress_kPa\": frac_stress,\n",
        "            \"Toughness_kJ_m3\": toughness,\n",
        "            \"Stress@5%_kPa\": s5, \"Stress@10%_kPa\": s10, \"Stress@15%_kPa\": s15, \"Stress@20%_kPa\": s20,\n",
        "            \"E5_10_kPa\": E5_10, \"TanE10_kPa\": TanE10, \"Secant_0_15_kPa\": sec_0_15,\n",
        "            \"Resilience_kJ_m3\": resilience\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---------- Derive properties ----------\n",
        "props_df = derive_properties_table(DATA_PATH)\n",
        "if props_df.empty:\n",
        "    raise RuntimeError(\"No valid stress–strain pairs found. Check column names like Strain(%)_<label> and Stress(kPa)_<label>.\")\n",
        "\n",
        "# ---------- Load composition (optional) ----------\n",
        "def load_composition() -> Optional[pd.DataFrame]:\n",
        "    comp_df = None\n",
        "    if COMPOSITION_PATH and Path(COMPOSITION_PATH).exists():\n",
        "        if COMPOSITION_PATH.lower().endswith(\".csv\"):\n",
        "            comp_df = pd.read_csv(COMPOSITION_PATH)\n",
        "        else:\n",
        "            comp_df = pd.read_excel(COMPOSITION_PATH)\n",
        "    elif COMPOSITION_SHEET:\n",
        "        comp_df = pd.read_excel(DATA_PATH, sheet_name=COMPOSITION_SHEET)\n",
        "    return comp_df\n",
        "\n",
        "comp_df = load_composition()\n",
        "\n",
        "# ---------- Build X (recipe) and y (Fracture_strain_frac) ----------\n",
        "TARGET_KEY = \"Fracture_strain_frac\"  # single target\n",
        "\n",
        "if TARGET_KEY not in props_df.columns:\n",
        "    raise RuntimeError(\"Fracture_strain_frac not computed.\")\n",
        "\n",
        "if comp_df is not None and len(COMPOSITION_COLS) > 0 and all(c in comp_df.columns for c in COMPOSITION_COLS) and \"label\" in comp_df.columns:\n",
        "    MODE = \"composition\"\n",
        "    df_model = props_df.merge(comp_df[[\"label\"] + COMPOSITION_COLS], on=\"label\", how=\"inner\")\n",
        "    XY = df_model[[\"label\"] + COMPOSITION_COLS + [TARGET_KEY]].dropna(subset=COMPOSITION_COLS + [TARGET_KEY])\n",
        "    X = XY[COMPOSITION_COLS].astype(float)\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "else:\n",
        "    MODE = \"label_fallback\"\n",
        "    print(\"[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\")\n",
        "    XY = props_df[[\"label\",\"mat_type\",\"mat_level\", TARGET_KEY]].dropna(subset=[TARGET_KEY])\n",
        "    X = XY[[\"mat_type\",\"mat_level\"]].copy()\n",
        "    y = XY[TARGET_KEY].astype(float).values\n",
        "\n",
        "if len(X) < 2:\n",
        "    raise RuntimeError(\"Not enough rows to train for Fracture_strain_frac. Need at least 2 valid samples.\")\n",
        "\n",
        "# ---------- Model pipeline ----------\n",
        "if MODE == \"composition\":\n",
        "    preprocess = ColumnTransformer([(\"num\", StandardScaler(), X.columns.tolist())])\n",
        "else:\n",
        "    preprocess = ColumnTransformer([\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"mat_type\"]),\n",
        "        (\"num\", StandardScaler(), [\"mat_level\"]),\n",
        "    ])\n",
        "\n",
        "reg = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"rf\", RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "ts = 0.33 if len(X) >= 6 else 0.0\n",
        "if ts > 0:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=ts, random_state=42)\n",
        "else:\n",
        "    X_tr, y_tr = X, y\n",
        "    X_te, y_te = X.iloc[:0], np.array([])\n",
        "\n",
        "reg.fit(X_tr, y_tr)\n",
        "\n",
        "if len(X_te) > 0:\n",
        "    y_hat = reg.predict(X_te)\n",
        "    print(f\"[Holdout] Fracture_strain_frac  R2={r2_score(y_te, y_hat):6.3f}  MAE={mean_absolute_error(y_te, y_hat):8.4f}\")\n",
        "else:\n",
        "    print(\"Trained on all rows (small dataset).\")\n",
        "\n",
        "# ---------- Forward design for Fracture_strain_frac ----------\n",
        "tgt = float(TARGET_FRACTURE_STRAIN)\n",
        "mins, maxs = X.min().values, X.max().values\n",
        "\n",
        "def _loss_from_row(dfrow):\n",
        "    pred = float(reg.predict(dfrow).ravel()[0])\n",
        "    # Weighted MSE + MAPE (robust scaling)\n",
        "    var = float(np.var(y_tr)) if len(y_tr) else float(np.var(y))\n",
        "    w = 1.0 if var <= 1e-12 else 1.0/var\n",
        "    eps = 1e-8\n",
        "    mse = w * (pred - tgt)**2\n",
        "    mape = abs(pred - tgt) / (abs(tgt) + eps)\n",
        "    base = 0.5*mse + 0.5*mape\n",
        "    return base, pred\n",
        "\n",
        "# ===== Case A: composition mode — continuous optimization over composition columns =====\n",
        "if MODE == \"composition\":\n",
        "    # Bounds with ±5% padding of historical range\n",
        "    bounds = []\n",
        "    for c in X.columns:\n",
        "        lo, hi = float(X[c].min()), float(X[c].max())\n",
        "        span = hi - lo if hi > lo else 1.0\n",
        "        bounds.append((lo - 0.05*span, hi + 0.05*span))\n",
        "\n",
        "    # Sum-to-constant constraint if set (e.g., wt% totals to 100)\n",
        "    constraints = []\n",
        "    if COMPOSITION_TOTAL is not None:\n",
        "        def sum_eq(x, total=COMPOSITION_TOTAL):\n",
        "            return float(np.sum(x) - total)\n",
        "        constraints.append({\"type\": \"eq\", \"fun\": sum_eq})\n",
        "\n",
        "    def objective(x):\n",
        "        row = pd.DataFrame([x], columns=X.columns)\n",
        "        base, _ = _loss_from_row(row)\n",
        "        # Soft penalty outside observed domain\n",
        "        over_low = np.maximum(0.0, mins - x)\n",
        "        over_high = np.maximum(0.0, x - maxs)\n",
        "        return base + 1e-3*np.sum(over_low**2 + over_high**2)\n",
        "\n",
        "    x0 = X.median().values\n",
        "    res = minimize(objective, x0, bounds=bounds, constraints=constraints, method=\"SLSQP\",\n",
        "                   options={\"maxiter\": 1000, \"ftol\": 1e-9, \"disp\": False})\n",
        "    x_opt = res.x\n",
        "    opt_row = pd.DataFrame([x_opt], columns=X.columns)\n",
        "    _, pred_val = _loss_from_row(opt_row)\n",
        "    mode_label = \"COMPOSITION → PROPERTIES\"\n",
        "\n",
        "# ===== Case B: label_fallback — enumerate mat_type (discrete) + optimize mat_level (continuous) =====\n",
        "else:\n",
        "    mat_types = sorted(X[\"mat_type\"].unique().tolist())\n",
        "    level_min, level_max = float(X[\"mat_level\"].min()), float(X[\"mat_level\"].max())\n",
        "    span = max(1e-9, level_max - level_min)\n",
        "    bounds = [(level_min - 0.05*span, level_max + 0.05*span)]\n",
        "\n",
        "    best = {\"loss\": np.inf, \"row\": None, \"pred\": None}\n",
        "\n",
        "    for mt in mat_types:\n",
        "        def objective(x):\n",
        "            row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(x[0])}])\n",
        "            base, _ = _loss_from_row(row)\n",
        "            # Soft penalty outside observed level range\n",
        "            over_low = max(0.0, level_min - x[0])\n",
        "            over_high = max(0.0, x[0] - level_max)\n",
        "            return base + 1e-3*(over_low**2 + over_high**2)\n",
        "\n",
        "        x0 = np.array([(level_min + level_max)/2.0])\n",
        "        res = minimize(objective, x0, bounds=bounds, method=\"SLSQP\",\n",
        "                       options={\"maxiter\": 500, \"ftol\": 1e-9, \"disp\": False})\n",
        "        row = pd.DataFrame([{\"mat_type\": mt, \"mat_level\": float(res.x[0])}])\n",
        "        loss, pred = _loss_from_row(row)\n",
        "        if loss < best[\"loss\"]:\n",
        "            best.update({\"loss\": loss, \"row\": row, \"pred\": pred})\n",
        "\n",
        "    opt_row = best[\"row\"]\n",
        "    pred_val = best[\"pred\"]\n",
        "    mode_label = \"LABEL FALLBACK (no composition provided)\"\n",
        "\n",
        "# ---------- Report & save ----------\n",
        "print(\"\\n================ RESULTS (Fracture strain) ================\")\n",
        "print(f\"Mode: {mode_label}\")\n",
        "print(\"\\nRecommended recipe (composition or knobs):\")\n",
        "print(opt_row.to_string(index=False))\n",
        "print(\"\\nPredicted Fracture_strain_frac:\")\n",
        "print(pd.DataFrame([pred_val], columns=[TARGET_KEY]).to_string(index=False))\n",
        "\n",
        "# Save artifacts next to your Drive file\n",
        "out_dir = Path(DATA_PATH).parent\n",
        "props_out = out_dir / \"_derived_properties_table.csv\"\n",
        "opt_out = out_dir / \"_optimal_recipe_FractureStrain.csv\"\n",
        "props_df.to_csv(props_out, index=False)\n",
        "pd.concat({\"recipe_opt\": opt_row.reset_index(drop=True),\n",
        "           \"predicted_Fracture_strain\": pd.DataFrame([pred_val], columns=[TARGET_KEY]).reset_index(drop=True)}, axis=1).to_csv(opt_out, index=False)\n",
        "print(f\"\\nSaved:\\n  {props_out}\\n  {opt_out}\")\n",
        "\n",
        "# If you get \"Not enough rows\", check coverage:\n",
        "# print(props_df[[TARGET_KEY]].notna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHJ3zOqJ_Hu7",
        "outputId": "eda0b989-3162-4af7-c71c-dab1bcc5f373"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Warn] No composition table recognized. Falling back to ('mat_type','mat_level') from labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1000350996.py:98: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return float(np.trapz(f(xs), xs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Holdout] Fracture_strain_frac  R2= 0.065  MAE=  0.0071\n",
            "\n",
            "================ RESULTS (Fracture strain) ================\n",
            "Mode: LABEL FALLBACK (no composition provided)\n",
            "\n",
            "Recommended recipe (composition or knobs):\n",
            "mat_type  mat_level\n",
            "     Cel       10.0\n",
            "\n",
            "Predicted Fracture_strain_frac:\n",
            " Fracture_strain_frac\n",
            "             0.156046\n",
            "\n",
            "Saved:\n",
            "  /content/drive/MyDrive/AI Training/_derived_properties_table.csv\n",
            "  /content/drive/MyDrive/AI Training/_optimal_recipe_FractureStrain.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FQbHRJMjDDWR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH72-1J0_Hxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YyPH0KX9Nr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tc2DPjoY9Nvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}